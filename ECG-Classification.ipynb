{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "import itertools\n",
    "import collections\n",
    "import pandas as pd\n",
    "import random\n",
    "import plotly.offline as offline\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>Label</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21-MAR-17</td>\n",
       "      <td>24-MAR-17</td>\n",
       "      <td>E06_Baseline_Time</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12-APR-17</td>\n",
       "      <td>15-APR-17</td>\n",
       "      <td>E06_Peak_Time</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27-JUL-16</td>\n",
       "      <td>30-JUL-16</td>\n",
       "      <td>E30_Baseline_Time</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19-AUG-16</td>\n",
       "      <td>22-AUG-16</td>\n",
       "      <td>E30_Peak_Time</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-JAN-17</td>\n",
       "      <td>13-JAN-17</td>\n",
       "      <td>E07B_Baseline_Time</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27-JAN-17</td>\n",
       "      <td>30-JAN-17</td>\n",
       "      <td>E07B_Peak_Time</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Start        End               Label  Class\n",
       "0  21-MAR-17  24-MAR-17   E06_Baseline_Time      1\n",
       "1  12-APR-17  15-APR-17       E06_Peak_Time      0\n",
       "2  27-JUL-16  30-JUL-16   E30_Baseline_Time      1\n",
       "3  19-AUG-16  22-AUG-16       E30_Peak_Time      0\n",
       "4  10-JAN-17  13-JAN-17  E07B_Baseline_Time      1\n",
       "5  27-JAN-17  30-JAN-17      E07B_Peak_Time      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub</th>\n",
       "      <th>desc1</th>\n",
       "      <th>tel</th>\n",
       "      <th>dtype</th>\n",
       "      <th>desc2</th>\n",
       "      <th>id</th>\n",
       "      <th>Exp</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>Telemetry Collection from ECG Sensor for RIh16...</td>\n",
       "      <td>Telemetry</td>\n",
       "      <td>TEL_ECG</td>\n",
       "      <td>Telemetry Collection from ECG Sensor</td>\n",
       "      <td>1912</td>\n",
       "      <td>E06</td>\n",
       "      <td>RIh16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201</td>\n",
       "      <td>Telemetry Collection from ECG Sensor for RTe16...</td>\n",
       "      <td>Telemetry</td>\n",
       "      <td>TEL_ECG</td>\n",
       "      <td>Telemetry Collection from ECG Sensor</td>\n",
       "      <td>1915</td>\n",
       "      <td>E06</td>\n",
       "      <td>RTe16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>206</td>\n",
       "      <td>Telemetry Collection from ECG Sensor for RCl15...</td>\n",
       "      <td>Telemetry</td>\n",
       "      <td>TEL_ECG</td>\n",
       "      <td>Telemetry Collection from ECG Sensor</td>\n",
       "      <td>1916</td>\n",
       "      <td>E06</td>\n",
       "      <td>RCl15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>211</td>\n",
       "      <td>Telemetry Collection from ECG Sensor for RUf16...</td>\n",
       "      <td>Telemetry</td>\n",
       "      <td>TEL_ECG</td>\n",
       "      <td>Telemetry Collection from ECG Sensor</td>\n",
       "      <td>1918</td>\n",
       "      <td>E06</td>\n",
       "      <td>RUf16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>220</td>\n",
       "      <td>Telemetry Collection from ECG Sensor for 12C13...</td>\n",
       "      <td>Telemetry</td>\n",
       "      <td>TEL_ECG</td>\n",
       "      <td>Telemetry Collection from ECG Sensor</td>\n",
       "      <td>1401</td>\n",
       "      <td>E07B</td>\n",
       "      <td>12C136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sub                                              desc1        tel    dtype  \\\n",
       "0  196  Telemetry Collection from ECG Sensor for RIh16...  Telemetry  TEL_ECG   \n",
       "1  201  Telemetry Collection from ECG Sensor for RTe16...  Telemetry  TEL_ECG   \n",
       "2  206  Telemetry Collection from ECG Sensor for RCl15...  Telemetry  TEL_ECG   \n",
       "3  211  Telemetry Collection from ECG Sensor for RUf16...  Telemetry  TEL_ECG   \n",
       "4  220  Telemetry Collection from ECG Sensor for 12C13...  Telemetry  TEL_ECG   \n",
       "\n",
       "                                  desc2    id   Exp Species  \n",
       "0  Telemetry Collection from ECG Sensor  1912   E06   RIh16  \n",
       "1  Telemetry Collection from ECG Sensor  1915   E06   RTe16  \n",
       "2  Telemetry Collection from ECG Sensor  1916   E06   RCl15  \n",
       "3  Telemetry Collection from ECG Sensor  1918   E06   RUf16  \n",
       "4  Telemetry Collection from ECG Sensor  1401  E07B  12C136  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Exp_Timeline = pd.read_csv(\"/home/tao/jupyter3/Tel_Data/Exp_TimeLine.csv\")\n",
    "ECG_meta = pd.read_csv(\"/home/tao/jupyter3/Tel_Data/ECG_MetaInfor.csv\")\n",
    "display(Exp_Timeline)\n",
    "display(ECG_meta.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9345, 10001)\n",
      "(9345,)\n"
     ]
    }
   ],
   "source": [
    "X_H0 = list()\n",
    "Y_H0 = list()\n",
    "### Baseline\n",
    "for i in range(0, 13):\n",
    "    dd = \"/mnt/data0/tao/ECG/\" + ECG_meta.Exp[i] + \"/Baseline/\" + ECG_meta.Species[i]\n",
    "    file_hour = [x for x in os.listdir(dd) if '_H0_' in x]\n",
    "    for item in file_hour:\n",
    "        ff = dd + \"/\" + item\n",
    "        data = pd.read_csv(ff)\n",
    "        X_H0.append(list(data.VALUE.values))\n",
    "        Y_H0.append(0)\n",
    "### Peak\n",
    "for i in range(0, 13):\n",
    "    dd = \"/mnt/data0/tao/ECG/\" + ECG_meta.Exp[i] + \"/Peak/\" + ECG_meta.Species[i]\n",
    "    file_hour = [x for x in os.listdir(dd) if '_H0_' in x]\n",
    "    for item in file_hour:\n",
    "        ff = dd + \"/\" + item\n",
    "        data = pd.read_csv(ff)\n",
    "        X_H0.append(list(data.VALUE.values))\n",
    "        Y_H0.append(1)\n",
    "\n",
    "X_H0 = np.asarray(X_H0)\n",
    "Y_H0 = np.asarray(Y_H0)\n",
    "print(np.shape(X_H0))\n",
    "print(np.shape(Y_H0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline # : 3692\n",
      "Peak # : 5653\n"
     ]
    }
   ],
   "source": [
    "print('Baseline # :', len([x for x in Y_H0 if x == 0]))\n",
    "print('Peak # :', len([x for x in Y_H0 if x == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train :  8410\n",
      "x_test  :  935\n",
      "y_train :  Counter({1: 5063, 0: 3347})\n",
      "y_test  :  Counter({1: 590, 0: 345})\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_H0, Y_H0, test_size=0.1)\n",
    "\n",
    "print(\"x_train : \", len(x_train))\n",
    "print(\"x_test  : \", len(x_test))\n",
    "print(\"y_train : \", collections.Counter(y_train))\n",
    "print(\"y_test  : \", collections.Counter(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10001,)\n",
      "(8410,)\n",
      "(935, 10001)\n",
      "(935,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(x_train[0]))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(x_test))\n",
    "print(np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpjk7j_5e6\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpjk7j_5e6', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f260e2e6898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpjk7j_5e6/model.ckpt.\n",
      "INFO:tensorflow:loss = 69.31474, step = 1\n",
      "INFO:tensorflow:global_step/sec: 129.652\n",
      "INFO:tensorflow:loss = 70.90298, step = 101 (0.773 sec)\n",
      "INFO:tensorflow:global_step/sec: 160.211\n",
      "INFO:tensorflow:loss = 86.93245, step = 201 (0.625 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.893\n",
      "INFO:tensorflow:loss = 76.2851, step = 301 (0.629 sec)\n",
      "INFO:tensorflow:global_step/sec: 122.311\n",
      "INFO:tensorflow:loss = 66.88684, step = 401 (0.817 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.771\n",
      "INFO:tensorflow:loss = 65.98689, step = 501 (0.993 sec)\n",
      "INFO:tensorflow:global_step/sec: 159.518\n",
      "INFO:tensorflow:loss = 60.78144, step = 601 (0.626 sec)\n",
      "INFO:tensorflow:global_step/sec: 161.393\n",
      "INFO:tensorflow:loss = 66.28484, step = 701 (0.619 sec)\n",
      "INFO:tensorflow:global_step/sec: 163.772\n",
      "INFO:tensorflow:loss = 68.609146, step = 801 (0.610 sec)\n",
      "INFO:tensorflow:global_step/sec: 159.374\n",
      "INFO:tensorflow:loss = 59.774292, step = 901 (0.628 sec)\n",
      "INFO:tensorflow:global_step/sec: 160.563\n",
      "INFO:tensorflow:loss = 56.906307, step = 1001 (0.623 sec)\n",
      "INFO:tensorflow:global_step/sec: 163.439\n",
      "INFO:tensorflow:loss = 57.03872, step = 1101 (0.612 sec)\n",
      "INFO:tensorflow:global_step/sec: 159.603\n",
      "INFO:tensorflow:loss = 65.11074, step = 1201 (0.626 sec)\n",
      "INFO:tensorflow:global_step/sec: 160.996\n",
      "INFO:tensorflow:loss = 63.266525, step = 1301 (0.622 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.91\n",
      "INFO:tensorflow:loss = 62.200817, step = 1401 (0.591 sec)\n",
      "INFO:tensorflow:global_step/sec: 159.628\n",
      "INFO:tensorflow:loss = 56.215004, step = 1501 (0.627 sec)\n",
      "INFO:tensorflow:global_step/sec: 151.81\n",
      "INFO:tensorflow:loss = 48.55052, step = 1601 (0.660 sec)\n",
      "INFO:tensorflow:global_step/sec: 160.23\n",
      "INFO:tensorflow:loss = 55.435406, step = 1701 (0.624 sec)\n",
      "INFO:tensorflow:global_step/sec: 161.685\n",
      "INFO:tensorflow:loss = 59.058365, step = 1801 (0.618 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.066\n",
      "INFO:tensorflow:loss = 66.13094, step = 1901 (0.775 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.166\n",
      "INFO:tensorflow:loss = 59.457855, step = 2001 (0.652 sec)\n",
      "INFO:tensorflow:global_step/sec: 161.585\n",
      "INFO:tensorflow:loss = 62.483673, step = 2101 (0.620 sec)\n",
      "INFO:tensorflow:global_step/sec: 161.187\n",
      "INFO:tensorflow:loss = 56.52081, step = 2201 (0.620 sec)\n",
      "INFO:tensorflow:global_step/sec: 161.975\n",
      "INFO:tensorflow:loss = 66.56522, step = 2301 (0.617 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.025\n",
      "INFO:tensorflow:loss = 58.145298, step = 2401 (0.633 sec)\n",
      "INFO:tensorflow:global_step/sec: 161.767\n",
      "INFO:tensorflow:loss = 51.95647, step = 2501 (0.618 sec)\n",
      "INFO:tensorflow:global_step/sec: 159.867\n",
      "INFO:tensorflow:loss = 61.592056, step = 2601 (0.626 sec)\n",
      "INFO:tensorflow:global_step/sec: 161.765\n",
      "INFO:tensorflow:loss = 60.28153, step = 2701 (0.617 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.297\n",
      "INFO:tensorflow:loss = 58.18685, step = 2801 (0.977 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.221\n",
      "INFO:tensorflow:loss = 64.63698, step = 2901 (0.637 sec)\n",
      "INFO:tensorflow:global_step/sec: 162.795\n",
      "INFO:tensorflow:loss = 60.982082, step = 3001 (0.614 sec)\n",
      "INFO:tensorflow:global_step/sec: 149.651\n",
      "INFO:tensorflow:loss = 52.87242, step = 3101 (0.668 sec)\n",
      "INFO:tensorflow:global_step/sec: 161.525\n",
      "INFO:tensorflow:loss = 55.559578, step = 3201 (0.619 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.905\n",
      "INFO:tensorflow:loss = 58.96898, step = 3301 (0.633 sec)\n",
      "INFO:tensorflow:global_step/sec: 160.645\n",
      "INFO:tensorflow:loss = 54.27404, step = 3401 (0.622 sec)\n",
      "INFO:tensorflow:global_step/sec: 159.564\n",
      "INFO:tensorflow:loss = 58.72736, step = 3501 (0.627 sec)\n",
      "INFO:tensorflow:global_step/sec: 133.37\n",
      "INFO:tensorflow:loss = 61.283154, step = 3601 (0.750 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.425\n",
      "INFO:tensorflow:loss = 67.17578, step = 3701 (0.698 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.687\n",
      "INFO:tensorflow:loss = 58.821747, step = 3801 (0.650 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.695\n",
      "INFO:tensorflow:loss = 61.930714, step = 3901 (0.630 sec)\n",
      "INFO:tensorflow:global_step/sec: 152.815\n",
      "INFO:tensorflow:loss = 64.358696, step = 4001 (0.654 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.046\n",
      "INFO:tensorflow:loss = 58.46506, step = 4101 (0.653 sec)\n",
      "INFO:tensorflow:global_step/sec: 152.071\n",
      "INFO:tensorflow:loss = 53.67335, step = 4201 (0.658 sec)\n",
      "INFO:tensorflow:global_step/sec: 155.839\n",
      "INFO:tensorflow:loss = 64.321686, step = 4301 (0.642 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.606\n",
      "INFO:tensorflow:loss = 58.217056, step = 4401 (0.651 sec)\n",
      "INFO:tensorflow:global_step/sec: 152.803\n",
      "INFO:tensorflow:loss = 60.08767, step = 4501 (0.654 sec)\n",
      "INFO:tensorflow:global_step/sec: 151.714\n",
      "INFO:tensorflow:loss = 60.64534, step = 4601 (0.659 sec)\n",
      "INFO:tensorflow:global_step/sec: 147.553\n",
      "INFO:tensorflow:loss = 51.791294, step = 4701 (0.678 sec)\n",
      "INFO:tensorflow:global_step/sec: 147.119\n",
      "INFO:tensorflow:loss = 60.503323, step = 4801 (0.680 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.19\n",
      "INFO:tensorflow:loss = 57.12288, step = 4901 (0.718 sec)\n",
      "INFO:tensorflow:global_step/sec: 148.988\n",
      "INFO:tensorflow:loss = 68.06578, step = 5001 (0.671 sec)\n",
      "INFO:tensorflow:global_step/sec: 150.707\n",
      "INFO:tensorflow:loss = 56.453224, step = 5101 (0.664 sec)\n",
      "INFO:tensorflow:global_step/sec: 155.852\n",
      "INFO:tensorflow:loss = 54.830814, step = 5201 (0.641 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.53\n",
      "INFO:tensorflow:loss = 54.586086, step = 5301 (0.639 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.193\n",
      "INFO:tensorflow:loss = 60.03604, step = 5401 (0.652 sec)\n",
      "INFO:tensorflow:global_step/sec: 155.496\n",
      "INFO:tensorflow:loss = 60.253548, step = 5501 (0.643 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.779\n",
      "INFO:tensorflow:loss = 61.883926, step = 5601 (0.634 sec)\n",
      "INFO:tensorflow:global_step/sec: 148.363\n",
      "INFO:tensorflow:loss = 51.62812, step = 5701 (0.674 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.293\n",
      "INFO:tensorflow:loss = 60.89128, step = 5801 (0.780 sec)\n",
      "INFO:tensorflow:global_step/sec: 154.654\n",
      "INFO:tensorflow:loss = 54.20547, step = 5901 (0.647 sec)\n",
      "INFO:tensorflow:global_step/sec: 149.571\n",
      "INFO:tensorflow:loss = 52.769188, step = 6001 (0.668 sec)\n",
      "INFO:tensorflow:global_step/sec: 146.753\n",
      "INFO:tensorflow:loss = 57.404305, step = 6101 (0.681 sec)\n",
      "INFO:tensorflow:global_step/sec: 151.24\n",
      "INFO:tensorflow:loss = 51.375473, step = 6201 (0.661 sec)\n",
      "INFO:tensorflow:global_step/sec: 155.792\n",
      "INFO:tensorflow:loss = 54.8841, step = 6301 (0.642 sec)\n",
      "INFO:tensorflow:global_step/sec: 155.574\n",
      "INFO:tensorflow:loss = 57.26486, step = 6401 (0.643 sec)\n",
      "INFO:tensorflow:global_step/sec: 151.637\n",
      "INFO:tensorflow:loss = 63.87171, step = 6501 (0.659 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.969\n",
      "INFO:tensorflow:loss = 55.613594, step = 6601 (0.633 sec)\n",
      "INFO:tensorflow:global_step/sec: 152.564\n",
      "INFO:tensorflow:loss = 62.798668, step = 6701 (0.656 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.174\n",
      "INFO:tensorflow:loss = 55.817894, step = 6801 (0.632 sec)\n",
      "INFO:tensorflow:global_step/sec: 155.428\n",
      "INFO:tensorflow:loss = 53.913887, step = 6901 (0.643 sec)\n",
      "INFO:tensorflow:global_step/sec: 155.456\n",
      "INFO:tensorflow:loss = 61.53572, step = 7001 (0.643 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.732\n",
      "INFO:tensorflow:loss = 55.423233, step = 7101 (0.650 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.811\n",
      "INFO:tensorflow:loss = 56.75744, step = 7201 (0.650 sec)\n",
      "INFO:tensorflow:global_step/sec: 154.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 52.27883, step = 7301 (0.647 sec)\n",
      "INFO:tensorflow:global_step/sec: 154.494\n",
      "INFO:tensorflow:loss = 59.902428, step = 7401 (0.647 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.044\n",
      "INFO:tensorflow:loss = 55.02206, step = 7501 (0.653 sec)\n",
      "INFO:tensorflow:global_step/sec: 151.858\n",
      "INFO:tensorflow:loss = 57.968773, step = 7601 (0.659 sec)\n",
      "INFO:tensorflow:global_step/sec: 149.516\n",
      "INFO:tensorflow:loss = 59.08509, step = 7701 (0.669 sec)\n",
      "INFO:tensorflow:global_step/sec: 152.668\n",
      "INFO:tensorflow:loss = 50.12422, step = 7801 (0.655 sec)\n",
      "INFO:tensorflow:global_step/sec: 130.002\n",
      "INFO:tensorflow:loss = 61.96545, step = 7901 (0.770 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.659\n",
      "INFO:tensorflow:loss = 60.32515, step = 8001 (0.697 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.747\n",
      "INFO:tensorflow:loss = 57.267853, step = 8101 (0.650 sec)\n",
      "INFO:tensorflow:global_step/sec: 155.56\n",
      "INFO:tensorflow:loss = 60.599228, step = 8201 (0.644 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.143\n",
      "INFO:tensorflow:loss = 58.08236, step = 8301 (0.639 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.184\n",
      "INFO:tensorflow:loss = 65.431564, step = 8401 (0.653 sec)\n",
      "INFO:tensorflow:global_step/sec: 152.069\n",
      "INFO:tensorflow:loss = 60.750557, step = 8501 (0.658 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.385\n",
      "INFO:tensorflow:loss = 55.41663, step = 8601 (0.652 sec)\n",
      "INFO:tensorflow:global_step/sec: 155.017\n",
      "INFO:tensorflow:loss = 54.30147, step = 8701 (0.645 sec)\n",
      "INFO:tensorflow:global_step/sec: 151.746\n",
      "INFO:tensorflow:loss = 52.23993, step = 8801 (0.659 sec)\n",
      "INFO:tensorflow:global_step/sec: 151.312\n",
      "INFO:tensorflow:loss = 57.17261, step = 8901 (0.661 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.708\n",
      "INFO:tensorflow:loss = 59.07757, step = 9001 (0.651 sec)\n",
      "INFO:tensorflow:global_step/sec: 151.579\n",
      "INFO:tensorflow:loss = 49.628048, step = 9101 (0.659 sec)\n",
      "INFO:tensorflow:global_step/sec: 152.265\n",
      "INFO:tensorflow:loss = 56.207508, step = 9201 (0.657 sec)\n",
      "INFO:tensorflow:global_step/sec: 144.762\n",
      "INFO:tensorflow:loss = 56.954422, step = 9301 (0.690 sec)\n",
      "INFO:tensorflow:global_step/sec: 152.585\n",
      "INFO:tensorflow:loss = 50.821472, step = 9401 (0.656 sec)\n",
      "INFO:tensorflow:global_step/sec: 150.959\n",
      "INFO:tensorflow:loss = 56.386383, step = 9501 (0.662 sec)\n",
      "INFO:tensorflow:global_step/sec: 151.999\n",
      "INFO:tensorflow:loss = 59.285275, step = 9601 (0.658 sec)\n",
      "INFO:tensorflow:global_step/sec: 150.829\n",
      "INFO:tensorflow:loss = 58.305767, step = 9701 (0.663 sec)\n",
      "INFO:tensorflow:global_step/sec: 151.425\n",
      "INFO:tensorflow:loss = 53.516594, step = 9801 (0.660 sec)\n",
      "INFO:tensorflow:global_step/sec: 152.927\n",
      "INFO:tensorflow:loss = 53.086613, step = 9901 (0.654 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/tmpjk7j_5e6/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 49.49803.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-02-12-16:10:57\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpjk7j_5e6/model.ckpt-10000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-02-12-16:10:58\n",
      "INFO:tensorflow:Saving dict for global step 10000: accuracy = 0.5176471, accuracy_baseline = 0.61497325, auc = 0.49566668, auc_precision_recall = 0.6295595, average_loss = 0.9713864, global_step = 10000, label/mean = 0.61497325, loss = 9.662194, precision = 0.6068966, prediction/mean = 0.57907796, recall = 0.6121739\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 10000: /tmp/tmpjk7j_5e6/model.ckpt-10000\n"
     ]
    }
   ],
   "source": [
    "# Normalize\n",
    "x_train = normalize(x_train, axis=0, norm='max')\n",
    "x_test = normalize(x_test, axis=0, norm='max')\n",
    "\n",
    "# Specify that all features have real-value data\n",
    "feature_columns = [tf.feature_column.numeric_column(\"ecg\", shape=np.shape(x_train[0]))]\n",
    "\n",
    "# Build Classifier\n",
    "estimator = tf.estimator.LinearClassifier(\n",
    "    feature_columns=feature_columns,\n",
    "    n_classes=2,\n",
    "    optimizer=tf.train.FtrlOptimizer(\n",
    "      learning_rate=0.1,\n",
    "      l1_regularization_strength=0.001\n",
    "    )\n",
    ")\n",
    "\n",
    "# Train model\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"ecg\": np.array(x_train)},\n",
    "    y=np.array(y_train),\n",
    "    batch_size=100,\n",
    "    num_epochs=1000,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "estimator.train(input_fn=train_input_fn, steps=10000)\n",
    "\n",
    "# Evaluate accuracy\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"ecg\": np.array(x_test)},\n",
    "    y=np.array(y_test),\n",
    "    batch_size=10,\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "accuracy_score = estimator.evaluate(input_fn=test_input_fn)[\"accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "pred_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"ecg\": np.array(x_test)},\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "predictions = estimator.predict(input_fn=pred_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot matrix\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        #cm[i, j] = 0 if np.isnan(cm[i, j]) else cm[i, j]\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpjk7j_5e6/model.ckpt-10000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "\n",
      "Test Accuracy: 51.764709%\n",
      "\n",
      "Normalized confusion matrix\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = list()\n",
    "for p in predictions:\n",
    "    pred.append(p[\"class_ids\"][0])\n",
    "    \n",
    "cm = confusion_matrix(y_test, pred)\n",
    "\n",
    "print(\"\\nTest Accuracy: {0:f}%\\n\".format(accuracy_score*100))\n",
    "plot_confusion_matrix(cm, classes, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define the sequential model that is used to train.\n",
    "\n",
    "2. Set of neural network layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train :  8410\n",
      "x_test  :  935\n",
      "y_train :  Counter({1: 5093, 0: 3317})\n",
      "y_test  :  Counter({1: 560, 0: 375})\n",
      "(10001,)\n",
      "(8410,)\n",
      "(935, 10001)\n",
      "(935,)\n"
     ]
    }
   ],
   "source": [
    "### Separate the dataset into training and testing.\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_H0, Y_H0, test_size=0.1)\n",
    "print(\"x_train : \", len(x_train))\n",
    "print(\"x_test  : \", len(x_test))\n",
    "print(\"y_train : \", collections.Counter(y_train))\n",
    "print(\"y_test  : \", collections.Counter(y_test))\n",
    "## Print train & test\n",
    "print(np.shape(x_train[0]))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(x_test))\n",
    "print(np.shape(y_test))\n",
    "# Normalize\n",
    "# x_train = normalize(x_train, axis=0, norm='max')\n",
    "# x_test = normalize(x_test, axis=0, norm='max')\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=2)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "### Separate the dataset into training and testing.\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define F1 metric\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "        Only computes a batch-wise average of recall.\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "        Only computes a batch-wise average of precision.\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_156 (Dense)            (None, 256)               2560512   \n",
      "_________________________________________________________________\n",
      "dropout_126 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_157 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_127 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_158 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_128 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_159 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_129 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_160 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_130 (Dropout)        (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_161 (Dense)            (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 2,604,306\n",
      "Trainable params: 2,604,306\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Keras Sequential Models\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=10001, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "### Compilation\n",
    "# For a binary classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the model\n",
    "#history = model.fit(x_train, y_train, epochs=20, batch_size=128, validation_data=(x_test, y_test))\n",
    "# score = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve\n",
    "#plt.figure(figsize=(15,15))\n",
    "#plt.plot(history.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "#plt.legend(['loss','val_loss'], loc='upper right',prop={'size': 15});\n",
    "#plt.title('Learning curve for the training of Dense Layers', fontsize=15)\n",
    "#plt.show()\n",
    "#print('Best test F1 score: ' + max(history.history['f1']).astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(10001,)\n",
      "(8410,)\n",
      "(935, 10001)\n",
      "(935,)\n",
      "Train on 8410 samples, validate on 935 samples\n",
      "Epoch 1/20\n",
      "8410/8410 [==============================] - 22s 3ms/step - loss: 0.7410 - acc: 0.5387 - f1: 0.5570 - val_loss: 0.6790 - val_acc: 0.5995 - val_f1: 0.6001\n",
      "Epoch 2/20\n",
      "8410/8410 [==============================] - 1s 87us/step - loss: 0.7158 - acc: 0.5644 - f1: 0.5723 - val_loss: 0.6812 - val_acc: 0.5968 - val_f1: 0.5968\n",
      "Epoch 3/20\n",
      "8410/8410 [==============================] - 1s 87us/step - loss: 0.6984 - acc: 0.5810 - f1: 0.5840 - val_loss: 0.6786 - val_acc: 0.5979 - val_f1: 0.5979\n",
      "Epoch 4/20\n",
      "8410/8410 [==============================] - 1s 86us/step - loss: 0.6879 - acc: 0.5927 - f1: 0.5917 - val_loss: 0.6762 - val_acc: 0.5979 - val_f1: 0.5979\n",
      "Epoch 5/20\n",
      "8410/8410 [==============================] - 1s 89us/step - loss: 0.6855 - acc: 0.5944 - f1: 0.5953 - val_loss: 0.6760 - val_acc: 0.5979 - val_f1: 0.5979\n",
      "Epoch 6/20\n",
      "8410/8410 [==============================] - 1s 100us/step - loss: 0.6786 - acc: 0.5993 - f1: 0.5991 - val_loss: 0.6754 - val_acc: 0.5979 - val_f1: 0.5979\n",
      "Epoch 7/20\n",
      "8410/8410 [==============================] - 1s 113us/step - loss: 0.6775 - acc: 0.6003 - f1: 0.6006 - val_loss: 0.6738 - val_acc: 0.5979 - val_f1: 0.5979\n",
      "Epoch 8/20\n",
      "8410/8410 [==============================] - 1s 89us/step - loss: 0.6753 - acc: 0.6023 - f1: 0.6024 - val_loss: 0.6742 - val_acc: 0.5979 - val_f1: 0.5979\n",
      "Epoch 9/20\n",
      "8410/8410 [==============================] - 1s 92us/step - loss: 0.6752 - acc: 0.6026 - f1: 0.6022 - val_loss: 0.6735 - val_acc: 0.5979 - val_f1: 0.5979\n",
      "Epoch 10/20\n",
      "8410/8410 [==============================] - 1s 90us/step - loss: 0.6724 - acc: 0.6027 - f1: 0.6020 - val_loss: 0.6717 - val_acc: 0.5979 - val_f1: 0.5979\n",
      "Epoch 11/20\n",
      "8410/8410 [==============================] - 1s 88us/step - loss: 0.6702 - acc: 0.6041 - f1: 0.6040 - val_loss: 0.6706 - val_acc: 0.5979 - val_f1: 0.5979\n",
      "Epoch 12/20\n",
      "8410/8410 [==============================] - 1s 104us/step - loss: 0.6677 - acc: 0.6045 - f1: 0.6045 - val_loss: 0.6672 - val_acc: 0.5979 - val_f1: 0.5979\n",
      "Epoch 13/20\n",
      "8410/8410 [==============================] - 1s 106us/step - loss: 0.6625 - acc: 0.6057 - f1: 0.6056 - val_loss: 0.6656 - val_acc: 0.5979 - val_f1: 0.5979\n",
      "Epoch 14/20\n",
      "8410/8410 [==============================] - 1s 87us/step - loss: 0.6578 - acc: 0.6064 - f1: 0.6063 - val_loss: 0.6655 - val_acc: 0.5979 - val_f1: 0.5979\n",
      "Epoch 15/20\n",
      "8410/8410 [==============================] - 1s 87us/step - loss: 0.6531 - acc: 0.6056 - f1: 0.6056 - val_loss: 0.6637 - val_acc: 0.5979 - val_f1: 0.5979\n",
      "Epoch 16/20\n",
      "8410/8410 [==============================] - 1s 87us/step - loss: 0.6426 - acc: 0.6074 - f1: 0.6076 - val_loss: 0.6617 - val_acc: 0.5979 - val_f1: 0.5979\n",
      "Epoch 17/20\n",
      "8410/8410 [==============================] - 1s 87us/step - loss: 0.6372 - acc: 0.6105 - f1: 0.6111 - val_loss: 0.6595 - val_acc: 0.5979 - val_f1: 0.5979\n",
      "Epoch 18/20\n",
      "8410/8410 [==============================] - 1s 87us/step - loss: 0.6265 - acc: 0.6154 - f1: 0.6166 - val_loss: 0.6569 - val_acc: 0.6128 - val_f1: 0.6116\n",
      "Epoch 19/20\n",
      "8410/8410 [==============================] - 1s 87us/step - loss: 0.6190 - acc: 0.6206 - f1: 0.6223 - val_loss: 0.6551 - val_acc: 0.6166 - val_f1: 0.6147\n",
      "Epoch 20/20\n",
      "8410/8410 [==============================] - 1s 87us/step - loss: 0.6050 - acc: 0.6238 - f1: 0.6262 - val_loss: 0.6564 - val_acc: 0.6144 - val_f1: 0.6114\n",
      "935/935 [==============================] - 8s 9ms/step\n",
      "1\n",
      "(10001,)\n",
      "(8977,)\n",
      "(998, 10001)\n",
      "(998,)\n",
      "Train on 8977 samples, validate on 998 samples\n",
      "Epoch 1/20\n",
      "8977/8977 [==============================] - 22s 2ms/step - loss: 0.7554 - acc: 0.5439 - f1: 0.5440 - val_loss: 0.6765 - val_acc: 0.6468 - val_f1: 0.6446\n",
      "Epoch 2/20\n",
      "8977/8977 [==============================] - 1s 80us/step - loss: 0.7142 - acc: 0.5837 - f1: 0.5829 - val_loss: 0.6712 - val_acc: 0.6483 - val_f1: 0.6483\n",
      "Epoch 3/20\n",
      "8977/8977 [==============================] - 1s 78us/step - loss: 0.6941 - acc: 0.6103 - f1: 0.6119 - val_loss: 0.6652 - val_acc: 0.6488 - val_f1: 0.6483\n",
      "Epoch 4/20\n",
      "8977/8977 [==============================] - 1s 78us/step - loss: 0.6837 - acc: 0.6162 - f1: 0.6171 - val_loss: 0.6614 - val_acc: 0.6513 - val_f1: 0.6513\n",
      "Epoch 5/20\n",
      "8977/8977 [==============================] - 1s 78us/step - loss: 0.6734 - acc: 0.6232 - f1: 0.6240 - val_loss: 0.6585 - val_acc: 0.6508 - val_f1: 0.6506\n",
      "Epoch 6/20\n",
      "8977/8977 [==============================] - 1s 78us/step - loss: 0.6751 - acc: 0.6218 - f1: 0.6229 - val_loss: 0.6548 - val_acc: 0.6513 - val_f1: 0.6513\n",
      "Epoch 7/20\n",
      "8977/8977 [==============================] - 1s 80us/step - loss: 0.6698 - acc: 0.6247 - f1: 0.6253 - val_loss: 0.6539 - val_acc: 0.6513 - val_f1: 0.6513\n",
      "Epoch 8/20\n",
      "8977/8977 [==============================] - 1s 84us/step - loss: 0.6664 - acc: 0.6257 - f1: 0.6271 - val_loss: 0.6525 - val_acc: 0.6508 - val_f1: 0.6506\n",
      "Epoch 9/20\n",
      "8977/8977 [==============================] - 1s 106us/step - loss: 0.6627 - acc: 0.6259 - f1: 0.6269 - val_loss: 0.6502 - val_acc: 0.6513 - val_f1: 0.6513\n",
      "Epoch 10/20\n",
      "8977/8977 [==============================] - 1s 79us/step - loss: 0.6612 - acc: 0.6265 - f1: 0.6272 - val_loss: 0.6478 - val_acc: 0.6508 - val_f1: 0.6506\n",
      "Epoch 11/20\n",
      "8977/8977 [==============================] - 1s 78us/step - loss: 0.6575 - acc: 0.6272 - f1: 0.6283 - val_loss: 0.6460 - val_acc: 0.6483 - val_f1: 0.6483\n",
      "Epoch 12/20\n",
      "8977/8977 [==============================] - 1s 78us/step - loss: 0.6506 - acc: 0.6270 - f1: 0.6274 - val_loss: 0.6417 - val_acc: 0.6488 - val_f1: 0.6486\n",
      "Epoch 13/20\n",
      "8977/8977 [==============================] - 1s 79us/step - loss: 0.6409 - acc: 0.6320 - f1: 0.6322 - val_loss: 0.6423 - val_acc: 0.6493 - val_f1: 0.6489\n",
      "Epoch 14/20\n",
      "8977/8977 [==============================] - 1s 79us/step - loss: 0.6335 - acc: 0.6372 - f1: 0.6379 - val_loss: 0.6395 - val_acc: 0.6468 - val_f1: 0.6466\n",
      "Epoch 15/20\n",
      "8977/8977 [==============================] - 1s 78us/step - loss: 0.6205 - acc: 0.6426 - f1: 0.6430 - val_loss: 0.6362 - val_acc: 0.6493 - val_f1: 0.6500\n",
      "Epoch 16/20\n",
      "8977/8977 [==============================] - 1s 78us/step - loss: 0.6080 - acc: 0.6541 - f1: 0.6554 - val_loss: 0.6296 - val_acc: 0.6518 - val_f1: 0.6533\n",
      "Epoch 17/20\n",
      "8977/8977 [==============================] - 1s 78us/step - loss: 0.6036 - acc: 0.6654 - f1: 0.6666 - val_loss: 0.6362 - val_acc: 0.6618 - val_f1: 0.6658\n",
      "Epoch 18/20\n",
      "8977/8977 [==============================] - 1s 78us/step - loss: 0.5790 - acc: 0.6775 - f1: 0.6783 - val_loss: 0.6268 - val_acc: 0.6759 - val_f1: 0.6792\n",
      "Epoch 19/20\n",
      "8977/8977 [==============================] - 1s 78us/step - loss: 0.5578 - acc: 0.7013 - f1: 0.7019 - val_loss: 0.6243 - val_acc: 0.6904 - val_f1: 0.6940\n",
      "Epoch 20/20\n",
      "8977/8977 [==============================] - 1s 79us/step - loss: 0.5440 - acc: 0.7150 - f1: 0.7160 - val_loss: 0.6281 - val_acc: 0.6904 - val_f1: 0.6941\n",
      "998/998 [==============================] - 8s 8ms/step\n",
      "2\n",
      "(10001,)\n",
      "(10305,)\n",
      "(1145, 10001)\n",
      "(1145,)\n",
      "Train on 10305 samples, validate on 1145 samples\n",
      "Epoch 1/20\n",
      "10305/10305 [==============================] - 22s 2ms/step - loss: 0.7357 - acc: 0.5076 - f1: 0.4943 - val_loss: 0.6930 - val_acc: 0.4991 - val_f1: 0.3329\n",
      "Epoch 2/20\n",
      "10305/10305 [==============================] - 1s 78us/step - loss: 0.7118 - acc: 0.5096 - f1: 0.4823 - val_loss: 0.6933 - val_acc: 0.5205 - val_f1: 0.3978\n",
      "Epoch 3/20\n",
      "10305/10305 [==============================] - 1s 77us/step - loss: 0.7047 - acc: 0.5127 - f1: 0.4877 - val_loss: 0.6929 - val_acc: 0.5148 - val_f1: 0.4933\n",
      "Epoch 4/20\n",
      "10305/10305 [==============================] - 1s 77us/step - loss: 0.7016 - acc: 0.5191 - f1: 0.4911 - val_loss: 0.6928 - val_acc: 0.5197 - val_f1: 0.4808\n",
      "Epoch 5/20\n",
      "10305/10305 [==============================] - 1s 80us/step - loss: 0.7029 - acc: 0.5126 - f1: 0.4830 - val_loss: 0.6931 - val_acc: 0.5131 - val_f1: 0.4964\n",
      "Epoch 6/20\n",
      "10305/10305 [==============================] - 1s 78us/step - loss: 0.6975 - acc: 0.5184 - f1: 0.4849 - val_loss: 0.6927 - val_acc: 0.5127 - val_f1: 0.5017\n",
      "Epoch 7/20\n",
      "10305/10305 [==============================] - 1s 80us/step - loss: 0.6970 - acc: 0.5248 - f1: 0.4909 - val_loss: 0.6931 - val_acc: 0.5074 - val_f1: 0.5053\n",
      "Epoch 8/20\n",
      "10305/10305 [==============================] - 1s 91us/step - loss: 0.6951 - acc: 0.5280 - f1: 0.5050 - val_loss: 0.6930 - val_acc: 0.5127 - val_f1: 0.5094\n",
      "Epoch 9/20\n",
      "10305/10305 [==============================] - 1s 98us/step - loss: 0.6935 - acc: 0.5345 - f1: 0.5166 - val_loss: 0.6920 - val_acc: 0.5275 - val_f1: 0.5276\n",
      "Epoch 10/20\n",
      "10305/10305 [==============================] - 1s 78us/step - loss: 0.6929 - acc: 0.5399 - f1: 0.5324 - val_loss: 0.6903 - val_acc: 0.5415 - val_f1: 0.5392\n",
      "Epoch 11/20\n",
      "10305/10305 [==============================] - 1s 79us/step - loss: 0.6937 - acc: 0.5522 - f1: 0.5417 - val_loss: 0.6903 - val_acc: 0.5354 - val_f1: 0.5330\n",
      "Epoch 12/20\n",
      "10305/10305 [==============================] - 1s 78us/step - loss: 0.6876 - acc: 0.5560 - f1: 0.5488 - val_loss: 0.6887 - val_acc: 0.5498 - val_f1: 0.5469\n",
      "Epoch 13/20\n",
      "10305/10305 [==============================] - 1s 78us/step - loss: 0.6838 - acc: 0.5660 - f1: 0.5578 - val_loss: 0.6864 - val_acc: 0.5489 - val_f1: 0.5473\n",
      "Epoch 14/20\n",
      "10305/10305 [==============================] - 1s 78us/step - loss: 0.6816 - acc: 0.5733 - f1: 0.5664 - val_loss: 0.6838 - val_acc: 0.5651 - val_f1: 0.5638\n",
      "Epoch 15/20\n",
      "10305/10305 [==============================] - 1s 78us/step - loss: 0.6753 - acc: 0.5918 - f1: 0.5863 - val_loss: 0.6793 - val_acc: 0.5721 - val_f1: 0.5698\n",
      "Epoch 16/20\n",
      "10305/10305 [==============================] - 1s 78us/step - loss: 0.6687 - acc: 0.6063 - f1: 0.6029 - val_loss: 0.6782 - val_acc: 0.5782 - val_f1: 0.5771\n",
      "Epoch 17/20\n",
      "10305/10305 [==============================] - 1s 78us/step - loss: 0.6616 - acc: 0.6077 - f1: 0.6054 - val_loss: 0.6767 - val_acc: 0.5677 - val_f1: 0.5655\n",
      "Epoch 18/20\n",
      "10305/10305 [==============================] - 1s 78us/step - loss: 0.6519 - acc: 0.6260 - f1: 0.6244 - val_loss: 0.6724 - val_acc: 0.5773 - val_f1: 0.5758\n",
      "Epoch 19/20\n",
      "10305/10305 [==============================] - 1s 78us/step - loss: 0.6401 - acc: 0.6327 - f1: 0.6317 - val_loss: 0.6677 - val_acc: 0.5921 - val_f1: 0.5918\n",
      "Epoch 20/20\n",
      "10305/10305 [==============================] - 1s 79us/step - loss: 0.6299 - acc: 0.6580 - f1: 0.6574 - val_loss: 0.6659 - val_acc: 0.5913 - val_f1: 0.5916\n",
      "1145/1145 [==============================] - 8s 7ms/step\n",
      "3\n",
      "(10001,)\n",
      "(10604,)\n",
      "(1179, 10001)\n",
      "(1179,)\n",
      "Train on 10604 samples, validate on 1179 samples\n",
      "Epoch 1/20\n",
      "10604/10604 [==============================] - 23s 2ms/step - loss: 0.7264 - acc: 0.5457 - f1: 0.5321 - val_loss: 0.6741 - val_acc: 0.6234 - val_f1: 0.6234\n",
      "Epoch 2/20\n",
      "10604/10604 [==============================] - 1s 81us/step - loss: 0.6906 - acc: 0.6035 - f1: 0.6133 - val_loss: 0.6707 - val_acc: 0.6234 - val_f1: 0.6234\n",
      "Epoch 3/20\n",
      "10604/10604 [==============================] - 1s 80us/step - loss: 0.6786 - acc: 0.6199 - f1: 0.6246 - val_loss: 0.6654 - val_acc: 0.6234 - val_f1: 0.6234\n",
      "Epoch 4/20\n",
      "10604/10604 [==============================] - 1s 83us/step - loss: 0.6686 - acc: 0.6268 - f1: 0.6302 - val_loss: 0.6645 - val_acc: 0.6234 - val_f1: 0.6234\n",
      "Epoch 5/20\n",
      "10604/10604 [==============================] - 1s 87us/step - loss: 0.6632 - acc: 0.6298 - f1: 0.6317 - val_loss: 0.6616 - val_acc: 0.6234 - val_f1: 0.6234\n",
      "Epoch 6/20\n",
      "10604/10604 [==============================] - 1s 107us/step - loss: 0.6586 - acc: 0.6296 - f1: 0.6308 - val_loss: 0.6611 - val_acc: 0.6234 - val_f1: 0.6234\n",
      "Epoch 7/20\n",
      "10604/10604 [==============================] - 1s 81us/step - loss: 0.6543 - acc: 0.6307 - f1: 0.6314 - val_loss: 0.6555 - val_acc: 0.6234 - val_f1: 0.6234\n",
      "Epoch 8/20\n",
      "10604/10604 [==============================] - 1s 83us/step - loss: 0.6491 - acc: 0.6309 - f1: 0.6315 - val_loss: 0.6516 - val_acc: 0.6234 - val_f1: 0.6234\n",
      "Epoch 9/20\n",
      "10604/10604 [==============================] - 1s 82us/step - loss: 0.6450 - acc: 0.6318 - f1: 0.6321 - val_loss: 0.6477 - val_acc: 0.6234 - val_f1: 0.6234\n",
      "Epoch 10/20\n",
      "10604/10604 [==============================] - 1s 84us/step - loss: 0.6328 - acc: 0.6324 - f1: 0.6325 - val_loss: 0.6449 - val_acc: 0.6234 - val_f1: 0.6234\n",
      "Epoch 11/20\n",
      "10604/10604 [==============================] - 1s 82us/step - loss: 0.6304 - acc: 0.6320 - f1: 0.6322 - val_loss: 0.6405 - val_acc: 0.6234 - val_f1: 0.6234\n",
      "Epoch 12/20\n",
      "10604/10604 [==============================] - 1s 80us/step - loss: 0.6151 - acc: 0.6333 - f1: 0.6335 - val_loss: 0.6369 - val_acc: 0.6234 - val_f1: 0.6234\n",
      "Epoch 13/20\n",
      "10604/10604 [==============================] - 1s 80us/step - loss: 0.6078 - acc: 0.6373 - f1: 0.6380 - val_loss: 0.6377 - val_acc: 0.6238 - val_f1: 0.6240\n",
      "Epoch 14/20\n",
      "10604/10604 [==============================] - 1s 80us/step - loss: 0.5939 - acc: 0.6413 - f1: 0.6425 - val_loss: 0.6295 - val_acc: 0.6268 - val_f1: 0.6274\n",
      "Epoch 15/20\n",
      "10604/10604 [==============================] - 1s 80us/step - loss: 0.5788 - acc: 0.6561 - f1: 0.6567 - val_loss: 0.6406 - val_acc: 0.6510 - val_f1: 0.6535\n",
      "Epoch 16/20\n",
      "10604/10604 [==============================] - 1s 80us/step - loss: 0.5665 - acc: 0.6708 - f1: 0.6712 - val_loss: 0.6288 - val_acc: 0.6645 - val_f1: 0.6667\n",
      "Epoch 17/20\n",
      "10604/10604 [==============================] - 1s 80us/step - loss: 0.5495 - acc: 0.6895 - f1: 0.6898 - val_loss: 0.6546 - val_acc: 0.6726 - val_f1: 0.6717\n",
      "Epoch 18/20\n",
      "10604/10604 [==============================] - 1s 80us/step - loss: 0.5415 - acc: 0.7011 - f1: 0.7003 - val_loss: 0.6514 - val_acc: 0.6739 - val_f1: 0.6731\n",
      "Epoch 19/20\n",
      "10604/10604 [==============================] - 1s 81us/step - loss: 0.5295 - acc: 0.7286 - f1: 0.7258 - val_loss: 0.6459 - val_acc: 0.6459 - val_f1: 0.6431\n",
      "Epoch 20/20\n",
      "10604/10604 [==============================] - 1s 81us/step - loss: 0.5210 - acc: 0.7538 - f1: 0.7526 - val_loss: 0.6552 - val_acc: 0.6671 - val_f1: 0.6651\n",
      "1179/1179 [==============================] - 9s 7ms/step\n",
      "4\n",
      "(10001,)\n",
      "(10557,)\n",
      "(1174, 10001)\n",
      "(1174,)\n",
      "Train on 10557 samples, validate on 1174 samples\n",
      "Epoch 1/20\n",
      "10557/10557 [==============================] - 23s 2ms/step - loss: 0.7530 - acc: 0.5164 - f1: 0.5287 - val_loss: 0.6927 - val_acc: 0.5213 - val_f1: 0.5102\n",
      "Epoch 2/20\n",
      "10557/10557 [==============================] - 1s 86us/step - loss: 0.7208 - acc: 0.5335 - f1: 0.5319 - val_loss: 0.6928 - val_acc: 0.5162 - val_f1: 0.5162\n",
      "Epoch 3/20\n",
      "10557/10557 [==============================] - 1s 85us/step - loss: 0.7041 - acc: 0.5544 - f1: 0.5560 - val_loss: 0.6918 - val_acc: 0.5192 - val_f1: 0.5169\n",
      "Epoch 4/20\n",
      "10557/10557 [==============================] - 1s 91us/step - loss: 0.7010 - acc: 0.5557 - f1: 0.5599 - val_loss: 0.6916 - val_acc: 0.5196 - val_f1: 0.5196\n",
      "Epoch 5/20\n",
      "10557/10557 [==============================] - 1s 100us/step - loss: 0.6954 - acc: 0.5584 - f1: 0.5606 - val_loss: 0.6908 - val_acc: 0.5200 - val_f1: 0.5202\n",
      "Epoch 6/20\n",
      "10557/10557 [==============================] - 1s 103us/step - loss: 0.6913 - acc: 0.5529 - f1: 0.5549 - val_loss: 0.6883 - val_acc: 0.5204 - val_f1: 0.5200\n",
      "Epoch 7/20\n",
      "10557/10557 [==============================] - 1s 85us/step - loss: 0.6882 - acc: 0.5566 - f1: 0.5572 - val_loss: 0.6846 - val_acc: 0.5200 - val_f1: 0.5198\n",
      "Epoch 8/20\n",
      "10557/10557 [==============================] - 1s 88us/step - loss: 0.6793 - acc: 0.5592 - f1: 0.5593 - val_loss: 0.6745 - val_acc: 0.5196 - val_f1: 0.5183\n",
      "Epoch 9/20\n",
      "10557/10557 [==============================] - 1s 86us/step - loss: 0.6746 - acc: 0.5613 - f1: 0.5620 - val_loss: 0.6679 - val_acc: 0.5230 - val_f1: 0.5225\n",
      "Epoch 10/20\n",
      "10557/10557 [==============================] - 1s 85us/step - loss: 0.6614 - acc: 0.5627 - f1: 0.5634 - val_loss: 0.6573 - val_acc: 0.5209 - val_f1: 0.5211\n",
      "Epoch 11/20\n",
      "10557/10557 [==============================] - 1s 85us/step - loss: 0.6478 - acc: 0.5767 - f1: 0.5794 - val_loss: 0.6440 - val_acc: 0.5217 - val_f1: 0.5280\n",
      "Epoch 12/20\n",
      "10557/10557 [==============================] - 1s 85us/step - loss: 0.6351 - acc: 0.6011 - f1: 0.6350 - val_loss: 0.6337 - val_acc: 0.6307 - val_f1: 0.6996\n",
      "Epoch 13/20\n",
      "10557/10557 [==============================] - 1s 85us/step - loss: 0.6211 - acc: 0.6476 - f1: 0.6669 - val_loss: 0.6197 - val_acc: 0.6917 - val_f1: 0.6974\n",
      "Epoch 14/20\n",
      "10557/10557 [==============================] - 1s 86us/step - loss: 0.6087 - acc: 0.6808 - f1: 0.6849 - val_loss: 0.6143 - val_acc: 0.6972 - val_f1: 0.6984\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10557/10557 [==============================] - 1s 84us/step - loss: 0.5852 - acc: 0.7069 - f1: 0.7090 - val_loss: 0.6075 - val_acc: 0.6895 - val_f1: 0.6923\n",
      "Epoch 16/20\n",
      "10557/10557 [==============================] - 1s 85us/step - loss: 0.5746 - acc: 0.7126 - f1: 0.7139 - val_loss: 0.6059 - val_acc: 0.6921 - val_f1: 0.6936\n",
      "Epoch 17/20\n",
      "10557/10557 [==============================] - 1s 89us/step - loss: 0.5554 - acc: 0.7298 - f1: 0.7306 - val_loss: 0.5943 - val_acc: 0.6985 - val_f1: 0.7000\n",
      "Epoch 18/20\n",
      "10557/10557 [==============================] - 1s 86us/step - loss: 0.5432 - acc: 0.7405 - f1: 0.7411 - val_loss: 0.5907 - val_acc: 0.6925 - val_f1: 0.6941\n",
      "Epoch 19/20\n",
      "10557/10557 [==============================] - 1s 105us/step - loss: 0.5323 - acc: 0.7500 - f1: 0.7503 - val_loss: 0.5874 - val_acc: 0.7061 - val_f1: 0.7072\n",
      "Epoch 20/20\n",
      "10557/10557 [==============================] - 1s 93us/step - loss: 0.5122 - acc: 0.7607 - f1: 0.7609 - val_loss: 0.5899 - val_acc: 0.7044 - val_f1: 0.7052\n",
      "1174/1174 [==============================] - 9s 7ms/step\n",
      "5\n",
      "(10001,)\n",
      "(11257,)\n",
      "(1251, 10001)\n",
      "(1251,)\n",
      "Train on 11257 samples, validate on 1251 samples\n",
      "Epoch 1/20\n",
      "11257/11257 [==============================] - 23s 2ms/step - loss: 0.7555 - acc: 0.5058 - f1: 0.4978 - val_loss: 0.6838 - val_acc: 0.5967 - val_f1: 0.5975\n",
      "Epoch 2/20\n",
      "11257/11257 [==============================] - 1s 87us/step - loss: 0.7062 - acc: 0.5670 - f1: 0.5555 - val_loss: 0.6798 - val_acc: 0.5975 - val_f1: 0.5977\n",
      "Epoch 3/20\n",
      "11257/11257 [==============================] - 1s 86us/step - loss: 0.6930 - acc: 0.5847 - f1: 0.5760 - val_loss: 0.6786 - val_acc: 0.5979 - val_f1: 0.5979\n",
      "Epoch 4/20\n",
      "11257/11257 [==============================] - 1s 87us/step - loss: 0.6870 - acc: 0.5878 - f1: 0.5829 - val_loss: 0.6764 - val_acc: 0.5979 - val_f1: 0.5979\n",
      "Epoch 5/20\n",
      "11257/11257 [==============================] - 1s 89us/step - loss: 0.6825 - acc: 0.5936 - f1: 0.5913 - val_loss: 0.6754 - val_acc: 0.5979 - val_f1: 0.5979\n",
      "Epoch 6/20\n",
      "11257/11257 [==============================] - 1s 86us/step - loss: 0.6794 - acc: 0.5947 - f1: 0.5928 - val_loss: 0.6747 - val_acc: 0.5979 - val_f1: 0.5979\n",
      "Epoch 7/20\n",
      "11257/11257 [==============================] - 1s 91us/step - loss: 0.6786 - acc: 0.5964 - f1: 0.5955 - val_loss: 0.6747 - val_acc: 0.5979 - val_f1: 0.5979\n",
      "Epoch 8/20\n",
      "11257/11257 [==============================] - 1s 111us/step - loss: 0.6790 - acc: 0.5979 - f1: 0.5972 - val_loss: 0.6746 - val_acc: 0.5979 - val_f1: 0.5979\n",
      "Epoch 9/20\n",
      "11257/11257 [==============================] - 1s 85us/step - loss: 0.6763 - acc: 0.5967 - f1: 0.5962 - val_loss: 0.6746 - val_acc: 0.5979 - val_f1: 0.5979\n",
      "Epoch 10/20\n",
      "11257/11257 [==============================] - 1s 86us/step - loss: 0.6790 - acc: 0.5963 - f1: 0.5958 - val_loss: 0.6742 - val_acc: 0.5979 - val_f1: 0.5979\n",
      "Epoch 11/20\n",
      "11257/11257 [==============================] - 1s 85us/step - loss: 0.6744 - acc: 0.5977 - f1: 0.5973 - val_loss: 0.6742 - val_acc: 0.5979 - val_f1: 0.5979\n",
      "Epoch 12/20\n",
      "11257/11257 [==============================] - 1s 86us/step - loss: 0.6741 - acc: 0.5984 - f1: 0.5979 - val_loss: 0.6734 - val_acc: 0.5979 - val_f1: 0.5979\n",
      "Epoch 13/20\n",
      "11257/11257 [==============================] - 1s 86us/step - loss: 0.6712 - acc: 0.5971 - f1: 0.5968 - val_loss: 0.6723 - val_acc: 0.5979 - val_f1: 0.5979\n",
      "Epoch 14/20\n",
      "11257/11257 [==============================] - 1s 86us/step - loss: 0.6691 - acc: 0.6005 - f1: 0.6002 - val_loss: 0.6714 - val_acc: 0.5979 - val_f1: 0.5979\n",
      "Epoch 15/20\n",
      "11257/11257 [==============================] - 1s 86us/step - loss: 0.6677 - acc: 0.5997 - f1: 0.5993 - val_loss: 0.6685 - val_acc: 0.5991 - val_f1: 0.5986\n",
      "Epoch 16/20\n",
      "11257/11257 [==============================] - 1s 85us/step - loss: 0.6620 - acc: 0.6047 - f1: 0.6040 - val_loss: 0.6645 - val_acc: 0.6035 - val_f1: 0.6028\n",
      "Epoch 17/20\n",
      "11257/11257 [==============================] - 1s 87us/step - loss: 0.6460 - acc: 0.6153 - f1: 0.6146 - val_loss: 0.6566 - val_acc: 0.6167 - val_f1: 0.6159\n",
      "Epoch 18/20\n",
      "11257/11257 [==============================] - 1s 87us/step - loss: 0.6374 - acc: 0.6224 - f1: 0.6217 - val_loss: 0.6527 - val_acc: 0.6255 - val_f1: 0.6251\n",
      "Epoch 19/20\n",
      "11257/11257 [==============================] - 1s 97us/step - loss: 0.6241 - acc: 0.6371 - f1: 0.6364 - val_loss: 0.6424 - val_acc: 0.6359 - val_f1: 0.6346\n",
      "Epoch 20/20\n",
      "11257/11257 [==============================] - 1s 104us/step - loss: 0.6104 - acc: 0.6551 - f1: 0.6547 - val_loss: 0.6350 - val_acc: 0.6559 - val_f1: 0.6547\n",
      "1251/1251 [==============================] - 8s 7ms/step\n",
      "6\n",
      "(10001,)\n",
      "(9074,)\n",
      "(1009, 10001)\n",
      "(1009,)\n",
      "Train on 9074 samples, validate on 1009 samples\n",
      "Epoch 1/20\n",
      "9074/9074 [==============================] - 23s 3ms/step - loss: 0.7686 - acc: 0.5029 - f1: 0.5198 - val_loss: 0.6939 - val_acc: 0.4990 - val_f1: 0.5200\n",
      "Epoch 2/20\n",
      "9074/9074 [==============================] - 1s 86us/step - loss: 0.7209 - acc: 0.5051 - f1: 0.5226 - val_loss: 0.6949 - val_acc: 0.4886 - val_f1: 0.4775\n",
      "Epoch 3/20\n",
      "9074/9074 [==============================] - 1s 86us/step - loss: 0.7050 - acc: 0.5184 - f1: 0.5285 - val_loss: 0.6940 - val_acc: 0.4931 - val_f1: 0.4963\n",
      "Epoch 4/20\n",
      "9074/9074 [==============================] - 1s 86us/step - loss: 0.7016 - acc: 0.5222 - f1: 0.5280 - val_loss: 0.6950 - val_acc: 0.4841 - val_f1: 0.4854\n",
      "Epoch 5/20\n",
      "9074/9074 [==============================] - 1s 85us/step - loss: 0.7000 - acc: 0.5282 - f1: 0.5309 - val_loss: 0.6948 - val_acc: 0.4886 - val_f1: 0.4892\n",
      "Epoch 6/20\n",
      "9074/9074 [==============================] - 1s 86us/step - loss: 0.6980 - acc: 0.5329 - f1: 0.5374 - val_loss: 0.6952 - val_acc: 0.4851 - val_f1: 0.4849\n",
      "Epoch 7/20\n",
      "9074/9074 [==============================] - 1s 90us/step - loss: 0.6939 - acc: 0.5299 - f1: 0.5304 - val_loss: 0.6956 - val_acc: 0.4817 - val_f1: 0.4807\n",
      "Epoch 8/20\n",
      "9074/9074 [==============================] - 1s 114us/step - loss: 0.6952 - acc: 0.5452 - f1: 0.5485 - val_loss: 0.6953 - val_acc: 0.4871 - val_f1: 0.4863\n",
      "Epoch 9/20\n",
      "9074/9074 [==============================] - 1s 88us/step - loss: 0.6891 - acc: 0.5633 - f1: 0.5654 - val_loss: 0.6941 - val_acc: 0.4950 - val_f1: 0.4943\n",
      "Epoch 10/20\n",
      "9074/9074 [==============================] - 1s 85us/step - loss: 0.6869 - acc: 0.5712 - f1: 0.5709 - val_loss: 0.6908 - val_acc: 0.5149 - val_f1: 0.5141\n",
      "Epoch 11/20\n",
      "9074/9074 [==============================] - 1s 85us/step - loss: 0.6755 - acc: 0.5877 - f1: 0.5868 - val_loss: 0.6892 - val_acc: 0.5258 - val_f1: 0.5231\n",
      "Epoch 12/20\n",
      "9074/9074 [==============================] - 1s 85us/step - loss: 0.6646 - acc: 0.6062 - f1: 0.6066 - val_loss: 0.6833 - val_acc: 0.5515 - val_f1: 0.5508\n",
      "Epoch 13/20\n",
      "9074/9074 [==============================] - 1s 86us/step - loss: 0.6506 - acc: 0.6292 - f1: 0.6287 - val_loss: 0.6750 - val_acc: 0.5783 - val_f1: 0.5781\n",
      "Epoch 14/20\n",
      "9074/9074 [==============================] - 1s 85us/step - loss: 0.6384 - acc: 0.6450 - f1: 0.6444 - val_loss: 0.6679 - val_acc: 0.5942 - val_f1: 0.5943\n",
      "Epoch 15/20\n",
      "9074/9074 [==============================] - 1s 85us/step - loss: 0.6277 - acc: 0.6644 - f1: 0.6640 - val_loss: 0.6572 - val_acc: 0.6125 - val_f1: 0.6129\n",
      "Epoch 16/20\n",
      "9074/9074 [==============================] - 1s 86us/step - loss: 0.6188 - acc: 0.6761 - f1: 0.6760 - val_loss: 0.6581 - val_acc: 0.6110 - val_f1: 0.6112\n",
      "Epoch 17/20\n",
      "9074/9074 [==============================] - 1s 84us/step - loss: 0.6041 - acc: 0.6902 - f1: 0.6898 - val_loss: 0.6572 - val_acc: 0.6105 - val_f1: 0.6109\n",
      "Epoch 18/20\n",
      "9074/9074 [==============================] - 1s 85us/step - loss: 0.5919 - acc: 0.6993 - f1: 0.6990 - val_loss: 0.6655 - val_acc: 0.6006 - val_f1: 0.6010\n",
      "Epoch 19/20\n",
      "9074/9074 [==============================] - 1s 84us/step - loss: 0.5849 - acc: 0.7093 - f1: 0.7091 - val_loss: 0.6686 - val_acc: 0.6056 - val_f1: 0.6063\n",
      "Epoch 20/20\n",
      "9074/9074 [==============================] - 1s 85us/step - loss: 0.5673 - acc: 0.7238 - f1: 0.7234 - val_loss: 0.6639 - val_acc: 0.6244 - val_f1: 0.6248\n",
      "1009/1009 [==============================] - 9s 9ms/step\n",
      "7\n",
      "(10001,)\n",
      "(8597,)\n",
      "(956, 10001)\n",
      "(956,)\n",
      "Train on 8597 samples, validate on 956 samples\n",
      "Epoch 1/20\n",
      "8597/8597 [==============================] - 23s 3ms/step - loss: 0.8391 - acc: 0.4948 - f1: 0.5374 - val_loss: 0.6927 - val_acc: 0.5021 - val_f1: 0.3814\n",
      "Epoch 2/20\n",
      "8597/8597 [==============================] - 1s 86us/step - loss: 0.7407 - acc: 0.5051 - f1: 0.4936 - val_loss: 0.6940 - val_acc: 0.5021 - val_f1: 0.1641\n",
      "Epoch 3/20\n",
      "8597/8597 [==============================] - 1s 87us/step - loss: 0.7203 - acc: 0.5085 - f1: 0.4484 - val_loss: 0.6940 - val_acc: 0.4953 - val_f1: 0.0300\n",
      "Epoch 4/20\n",
      "8597/8597 [==============================] - 1s 85us/step - loss: 0.7113 - acc: 0.5101 - f1: 0.4109 - val_loss: 0.6936 - val_acc: 0.5000 - val_f1: 0.0474\n",
      "Epoch 5/20\n",
      "8597/8597 [==============================] - 1s 87us/step - loss: 0.7065 - acc: 0.5067 - f1: 0.3832 - val_loss: 0.6930 - val_acc: 0.5047 - val_f1: 0.0441\n",
      "Epoch 6/20\n",
      "8597/8597 [==============================] - 1s 87us/step - loss: 0.7028 - acc: 0.5073 - f1: 0.3594 - val_loss: 0.6928 - val_acc: 0.5037 - val_f1: 0.0607\n",
      "Epoch 7/20\n",
      "8597/8597 [==============================] - 1s 86us/step - loss: 0.7009 - acc: 0.5087 - f1: 0.3658 - val_loss: 0.6925 - val_acc: 0.5026 - val_f1: 0.0930\n",
      "Epoch 8/20\n",
      "8597/8597 [==============================] - 1s 89us/step - loss: 0.7001 - acc: 0.5082 - f1: 0.4593 - val_loss: 0.6922 - val_acc: 0.5183 - val_f1: 0.2142\n",
      "Epoch 9/20\n",
      "8597/8597 [==============================] - 1s 109us/step - loss: 0.6969 - acc: 0.5175 - f1: 0.5095 - val_loss: 0.6920 - val_acc: 0.5418 - val_f1: 0.5409\n",
      "Epoch 10/20\n",
      "8597/8597 [==============================] - 1s 95us/step - loss: 0.6961 - acc: 0.5170 - f1: 0.5225 - val_loss: 0.6919 - val_acc: 0.5429 - val_f1: 0.5433\n",
      "Epoch 11/20\n",
      "8597/8597 [==============================] - 1s 85us/step - loss: 0.6973 - acc: 0.5209 - f1: 0.5266 - val_loss: 0.6919 - val_acc: 0.5392 - val_f1: 0.5404\n",
      "Epoch 12/20\n",
      "8597/8597 [==============================] - 1s 87us/step - loss: 0.6938 - acc: 0.5203 - f1: 0.5266 - val_loss: 0.6918 - val_acc: 0.5392 - val_f1: 0.5418\n",
      "Epoch 13/20\n",
      "8597/8597 [==============================] - 1s 86us/step - loss: 0.6945 - acc: 0.5145 - f1: 0.5201 - val_loss: 0.6911 - val_acc: 0.5408 - val_f1: 0.5418\n",
      "Epoch 14/20\n",
      "8597/8597 [==============================] - 1s 85us/step - loss: 0.6943 - acc: 0.5141 - f1: 0.5197 - val_loss: 0.6909 - val_acc: 0.5418 - val_f1: 0.5418\n",
      "Epoch 15/20\n",
      "8597/8597 [==============================] - 1s 86us/step - loss: 0.6930 - acc: 0.5183 - f1: 0.5234 - val_loss: 0.6902 - val_acc: 0.5397 - val_f1: 0.5402\n",
      "Epoch 16/20\n",
      "8597/8597 [==============================] - 1s 86us/step - loss: 0.6912 - acc: 0.5184 - f1: 0.5238 - val_loss: 0.6894 - val_acc: 0.5418 - val_f1: 0.5424\n",
      "Epoch 17/20\n",
      "8597/8597 [==============================] - 1s 85us/step - loss: 0.6875 - acc: 0.5179 - f1: 0.5227 - val_loss: 0.6898 - val_acc: 0.5361 - val_f1: 0.5372\n",
      "Epoch 18/20\n",
      "8597/8597 [==============================] - 1s 85us/step - loss: 0.6836 - acc: 0.5233 - f1: 0.5278 - val_loss: 0.6877 - val_acc: 0.5361 - val_f1: 0.5359\n",
      "Epoch 19/20\n",
      "8597/8597 [==============================] - 1s 85us/step - loss: 0.6816 - acc: 0.5354 - f1: 0.4517 - val_loss: 0.6882 - val_acc: 0.5052 - val_f1: 0.4967\n",
      "Epoch 20/20\n",
      "8597/8597 [==============================] - 1s 86us/step - loss: 0.6782 - acc: 0.5518 - f1: 0.5476 - val_loss: 0.6883 - val_acc: 0.5010 - val_f1: 0.4994\n",
      "956/956 [==============================] - 9s 9ms/step\n",
      "8\n",
      "(10001,)\n",
      "(10770,)\n",
      "(1197, 10001)\n",
      "(1197,)\n",
      "Train on 10770 samples, validate on 1197 samples\n",
      "Epoch 1/20\n",
      "10770/10770 [==============================] - 24s 2ms/step - loss: 0.8013 - acc: 0.4955 - f1: 0.5196 - val_loss: 0.6919 - val_acc: 0.5079 - val_f1: 0.3434\n",
      "Epoch 2/20\n",
      "10770/10770 [==============================] - 1s 106us/step - loss: 0.7231 - acc: 0.5035 - f1: 0.4844 - val_loss: 0.6935 - val_acc: 0.5013 - val_f1: 0.1829\n",
      "Epoch 3/20\n",
      "10770/10770 [==============================] - 1s 82us/step - loss: 0.7128 - acc: 0.4990 - f1: 0.4691 - val_loss: 0.6923 - val_acc: 0.4987 - val_f1: 0.1560\n",
      "Epoch 4/20\n",
      "10770/10770 [==============================] - 1s 82us/step - loss: 0.7013 - acc: 0.5020 - f1: 0.3630 - val_loss: 0.6922 - val_acc: 0.5205 - val_f1: 0.5205\n",
      "Epoch 5/20\n",
      "10770/10770 [==============================] - 1s 82us/step - loss: 0.7002 - acc: 0.5047 - f1: 0.4988 - val_loss: 0.6922 - val_acc: 0.5322 - val_f1: 0.3889\n",
      "Epoch 6/20\n",
      "10770/10770 [==============================] - 1s 83us/step - loss: 0.6972 - acc: 0.5006 - f1: 0.4141 - val_loss: 0.6925 - val_acc: 0.5155 - val_f1: 0.4932\n",
      "Epoch 7/20\n",
      "10770/10770 [==============================] - 1s 83us/step - loss: 0.6947 - acc: 0.5045 - f1: 0.4654 - val_loss: 0.6915 - val_acc: 0.5205 - val_f1: 0.5205\n",
      "Epoch 8/20\n",
      "10770/10770 [==============================] - 1s 82us/step - loss: 0.6948 - acc: 0.5106 - f1: 0.4322 - val_loss: 0.6925 - val_acc: 0.5255 - val_f1: 0.2234\n",
      "Epoch 9/20\n",
      "10770/10770 [==============================] - 1s 84us/step - loss: 0.6963 - acc: 0.5064 - f1: 0.3685 - val_loss: 0.6921 - val_acc: 0.5305 - val_f1: 0.3955\n",
      "Epoch 10/20\n",
      "10770/10770 [==============================] - 1s 83us/step - loss: 0.6949 - acc: 0.5023 - f1: 0.3233 - val_loss: 0.6922 - val_acc: 0.5317 - val_f1: 0.3279\n",
      "Epoch 11/20\n",
      "10770/10770 [==============================] - 1s 100us/step - loss: 0.6953 - acc: 0.4926 - f1: 0.3915 - val_loss: 0.6932 - val_acc: 0.5121 - val_f1: 0.3963\n",
      "Epoch 12/20\n",
      "10770/10770 [==============================] - 1s 95us/step - loss: 0.6924 - acc: 0.5108 - f1: 0.4894 - val_loss: 0.6922 - val_acc: 0.5305 - val_f1: 0.3080\n",
      "Epoch 13/20\n",
      "10770/10770 [==============================] - 1s 82us/step - loss: 0.6940 - acc: 0.5145 - f1: 0.4925 - val_loss: 0.6919 - val_acc: 0.5343 - val_f1: 0.4383\n",
      "Epoch 14/20\n",
      "10770/10770 [==============================] - 1s 82us/step - loss: 0.6937 - acc: 0.5109 - f1: 0.5010 - val_loss: 0.6917 - val_acc: 0.5380 - val_f1: 0.4677\n",
      "Epoch 15/20\n",
      "10770/10770 [==============================] - 1s 82us/step - loss: 0.6920 - acc: 0.5107 - f1: 0.5040 - val_loss: 0.6914 - val_acc: 0.5430 - val_f1: 0.4988\n",
      "Epoch 16/20\n",
      "10770/10770 [==============================] - 1s 81us/step - loss: 0.6906 - acc: 0.5160 - f1: 0.5106 - val_loss: 0.6904 - val_acc: 0.5372 - val_f1: 0.5146\n",
      "Epoch 17/20\n",
      "10770/10770 [==============================] - 1s 82us/step - loss: 0.6892 - acc: 0.5307 - f1: 0.5262 - val_loss: 0.6904 - val_acc: 0.5368 - val_f1: 0.5287\n",
      "Epoch 18/20\n",
      "10770/10770 [==============================] - 1s 82us/step - loss: 0.6884 - acc: 0.5332 - f1: 0.5294 - val_loss: 0.6904 - val_acc: 0.5251 - val_f1: 0.5213\n",
      "Epoch 19/20\n",
      "10770/10770 [==============================] - 1s 82us/step - loss: 0.6880 - acc: 0.5275 - f1: 0.5254 - val_loss: 0.6900 - val_acc: 0.5196 - val_f1: 0.5176\n",
      "Epoch 20/20\n",
      "10770/10770 [==============================] - 1s 82us/step - loss: 0.6812 - acc: 0.5476 - f1: 0.5464 - val_loss: 0.6863 - val_acc: 0.5384 - val_f1: 0.5371\n",
      "1197/1197 [==============================] - 9s 8ms/step\n",
      "9\n",
      "(10001,)\n",
      "(10529,)\n",
      "(1170, 10001)\n",
      "(1170,)\n",
      "Train on 10529 samples, validate on 1170 samples\n",
      "Epoch 1/20\n",
      "10529/10529 [==============================] - 23s 2ms/step - loss: 0.7836 - acc: 0.5323 - f1: 0.5363 - val_loss: 0.6791 - val_acc: 0.6359 - val_f1: 0.6375\n",
      "Epoch 2/20\n",
      "10529/10529 [==============================] - 1s 79us/step - loss: 0.7089 - acc: 0.5825 - f1: 0.5915 - val_loss: 0.6743 - val_acc: 0.6338 - val_f1: 0.6336\n",
      "Epoch 3/20\n",
      "10529/10529 [==============================] - 1s 79us/step - loss: 0.6906 - acc: 0.6037 - f1: 0.6080 - val_loss: 0.6690 - val_acc: 0.6333 - val_f1: 0.6333\n",
      "Epoch 4/20\n",
      "10529/10529 [==============================] - 1s 79us/step - loss: 0.6829 - acc: 0.6098 - f1: 0.6133 - val_loss: 0.6649 - val_acc: 0.6333 - val_f1: 0.6333\n",
      "Epoch 5/20\n",
      "10529/10529 [==============================] - 1s 80us/step - loss: 0.6769 - acc: 0.6125 - f1: 0.6145 - val_loss: 0.6626 - val_acc: 0.6333 - val_f1: 0.6333\n",
      "Epoch 6/20\n",
      "10529/10529 [==============================] - 1s 81us/step - loss: 0.6719 - acc: 0.6132 - f1: 0.6153 - val_loss: 0.6608 - val_acc: 0.6333 - val_f1: 0.6333\n",
      "Epoch 7/20\n",
      "10529/10529 [==============================] - 1s 83us/step - loss: 0.6705 - acc: 0.6131 - f1: 0.6147 - val_loss: 0.6586 - val_acc: 0.6338 - val_f1: 0.6339\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10529/10529 [==============================] - 1s 105us/step - loss: 0.6695 - acc: 0.6115 - f1: 0.6133 - val_loss: 0.6576 - val_acc: 0.6333 - val_f1: 0.6333\n",
      "Epoch 9/20\n",
      "10529/10529 [==============================] - 1s 86us/step - loss: 0.6686 - acc: 0.6130 - f1: 0.6140 - val_loss: 0.6570 - val_acc: 0.6333 - val_f1: 0.6333\n",
      "Epoch 10/20\n",
      "10529/10529 [==============================] - 1s 80us/step - loss: 0.6672 - acc: 0.6147 - f1: 0.6160 - val_loss: 0.6571 - val_acc: 0.6333 - val_f1: 0.6333\n",
      "Epoch 11/20\n",
      "10529/10529 [==============================] - 1s 81us/step - loss: 0.6668 - acc: 0.6134 - f1: 0.6152 - val_loss: 0.6562 - val_acc: 0.6333 - val_f1: 0.6333\n",
      "Epoch 12/20\n",
      "10529/10529 [==============================] - 1s 79us/step - loss: 0.6652 - acc: 0.6161 - f1: 0.6173 - val_loss: 0.6560 - val_acc: 0.6342 - val_f1: 0.6342\n",
      "Epoch 13/20\n",
      "10529/10529 [==============================] - 1s 80us/step - loss: 0.6644 - acc: 0.6146 - f1: 0.6157 - val_loss: 0.6553 - val_acc: 0.6342 - val_f1: 0.6342\n",
      "Epoch 14/20\n",
      "10529/10529 [==============================] - 1s 80us/step - loss: 0.6641 - acc: 0.6171 - f1: 0.6180 - val_loss: 0.6546 - val_acc: 0.6342 - val_f1: 0.6342\n",
      "Epoch 15/20\n",
      "10529/10529 [==============================] - 1s 80us/step - loss: 0.6615 - acc: 0.6195 - f1: 0.6204 - val_loss: 0.6537 - val_acc: 0.6342 - val_f1: 0.6342\n",
      "Epoch 16/20\n",
      "10529/10529 [==============================] - 1s 79us/step - loss: 0.6611 - acc: 0.6209 - f1: 0.6217 - val_loss: 0.6524 - val_acc: 0.6346 - val_f1: 0.6348\n",
      "Epoch 17/20\n",
      "10529/10529 [==============================] - 1s 80us/step - loss: 0.6569 - acc: 0.6271 - f1: 0.6274 - val_loss: 0.6509 - val_acc: 0.6359 - val_f1: 0.6359\n",
      "Epoch 18/20\n",
      "10529/10529 [==============================] - 1s 80us/step - loss: 0.6540 - acc: 0.6300 - f1: 0.6304 - val_loss: 0.6480 - val_acc: 0.6385 - val_f1: 0.6385\n",
      "Epoch 19/20\n",
      "10529/10529 [==============================] - 1s 81us/step - loss: 0.6481 - acc: 0.6373 - f1: 0.6375 - val_loss: 0.6468 - val_acc: 0.6376 - val_f1: 0.6376\n",
      "Epoch 20/20\n",
      "10529/10529 [==============================] - 1s 80us/step - loss: 0.6468 - acc: 0.6418 - f1: 0.6418 - val_loss: 0.6452 - val_acc: 0.6350 - val_f1: 0.6350\n",
      "1170/1170 [==============================] - 9s 8ms/step\n",
      "10\n",
      "(10001,)\n",
      "(8236,)\n",
      "(916, 10001)\n",
      "(916,)\n",
      "Train on 8236 samples, validate on 916 samples\n",
      "Epoch 1/20\n",
      "8236/8236 [==============================] - 23s 3ms/step - loss: 0.7637 - acc: 0.5140 - f1: 0.5053 - val_loss: 0.6846 - val_acc: 0.6070 - val_f1: 0.5817\n",
      "Epoch 2/20\n",
      "8236/8236 [==============================] - 1s 81us/step - loss: 0.7174 - acc: 0.5475 - f1: 0.5308 - val_loss: 0.6854 - val_acc: 0.5617 - val_f1: 0.5481\n",
      "Epoch 3/20\n",
      "8236/8236 [==============================] - 1s 82us/step - loss: 0.7001 - acc: 0.5593 - f1: 0.5542 - val_loss: 0.6830 - val_acc: 0.5699 - val_f1: 0.5717\n",
      "Epoch 4/20\n",
      "8236/8236 [==============================] - 1s 86us/step - loss: 0.6959 - acc: 0.5633 - f1: 0.5645 - val_loss: 0.6866 - val_acc: 0.5377 - val_f1: 0.5384\n",
      "Epoch 5/20\n",
      "8236/8236 [==============================] - 1s 108us/step - loss: 0.6818 - acc: 0.5869 - f1: 0.5871 - val_loss: 0.6852 - val_acc: 0.5377 - val_f1: 0.5384\n",
      "Epoch 6/20\n",
      "8236/8236 [==============================] - 1s 85us/step - loss: 0.6845 - acc: 0.5936 - f1: 0.5927 - val_loss: 0.6884 - val_acc: 0.5295 - val_f1: 0.5290\n",
      "Epoch 7/20\n",
      "8236/8236 [==============================] - 1s 81us/step - loss: 0.6798 - acc: 0.5948 - f1: 0.5939 - val_loss: 0.6848 - val_acc: 0.5398 - val_f1: 0.5396\n",
      "Epoch 8/20\n",
      "8236/8236 [==============================] - 1s 80us/step - loss: 0.6750 - acc: 0.6040 - f1: 0.6035 - val_loss: 0.6842 - val_acc: 0.5431 - val_f1: 0.5434\n",
      "Epoch 9/20\n",
      "8236/8236 [==============================] - 1s 80us/step - loss: 0.6702 - acc: 0.6041 - f1: 0.6043 - val_loss: 0.6839 - val_acc: 0.5415 - val_f1: 0.5410\n",
      "Epoch 10/20\n",
      "8236/8236 [==============================] - 1s 82us/step - loss: 0.6672 - acc: 0.6064 - f1: 0.6056 - val_loss: 0.6787 - val_acc: 0.5546 - val_f1: 0.5531\n",
      "Epoch 11/20\n",
      "8236/8236 [==============================] - 1s 80us/step - loss: 0.6621 - acc: 0.6138 - f1: 0.6136 - val_loss: 0.6793 - val_acc: 0.5459 - val_f1: 0.5459\n",
      "Epoch 12/20\n",
      "8236/8236 [==============================] - 1s 82us/step - loss: 0.6642 - acc: 0.6308 - f1: 0.6296 - val_loss: 0.6785 - val_acc: 0.5464 - val_f1: 0.5457\n",
      "Epoch 13/20\n",
      "8236/8236 [==============================] - 1s 82us/step - loss: 0.6545 - acc: 0.6328 - f1: 0.6329 - val_loss: 0.6739 - val_acc: 0.5611 - val_f1: 0.5612\n",
      "Epoch 14/20\n",
      "8236/8236 [==============================] - 1s 82us/step - loss: 0.6553 - acc: 0.6290 - f1: 0.6289 - val_loss: 0.6754 - val_acc: 0.5557 - val_f1: 0.5562\n",
      "Epoch 15/20\n",
      "8236/8236 [==============================] - 1s 81us/step - loss: 0.6479 - acc: 0.6421 - f1: 0.6423 - val_loss: 0.6639 - val_acc: 0.5748 - val_f1: 0.5755\n",
      "Epoch 16/20\n",
      "8236/8236 [==============================] - 1s 80us/step - loss: 0.6511 - acc: 0.6407 - f1: 0.6407 - val_loss: 0.6715 - val_acc: 0.5573 - val_f1: 0.5566\n",
      "Epoch 17/20\n",
      "8236/8236 [==============================] - 1s 82us/step - loss: 0.6376 - acc: 0.6546 - f1: 0.6553 - val_loss: 0.6596 - val_acc: 0.5884 - val_f1: 0.5880\n",
      "Epoch 18/20\n",
      "8236/8236 [==============================] - 1s 82us/step - loss: 0.6356 - acc: 0.6552 - f1: 0.6550 - val_loss: 0.6634 - val_acc: 0.5819 - val_f1: 0.5819\n",
      "Epoch 19/20\n",
      "8236/8236 [==============================] - 1s 84us/step - loss: 0.6295 - acc: 0.6648 - f1: 0.6648 - val_loss: 0.6602 - val_acc: 0.5846 - val_f1: 0.5848\n",
      "Epoch 20/20\n",
      "8236/8236 [==============================] - 1s 84us/step - loss: 0.6285 - acc: 0.6669 - f1: 0.6665 - val_loss: 0.6603 - val_acc: 0.5841 - val_f1: 0.5841\n",
      "916/916 [==============================] - 9s 10ms/step\n",
      "11\n",
      "(10001,)\n",
      "(8900,)\n",
      "(989, 10001)\n",
      "(989,)\n",
      "Train on 8900 samples, validate on 989 samples\n",
      "Epoch 1/20\n",
      "8900/8900 [==============================] - 25s 3ms/step - loss: 0.7951 - acc: 0.5279 - f1: 0.5403 - val_loss: 0.6818 - val_acc: 0.6239 - val_f1: 0.6276\n",
      "Epoch 2/20\n",
      "8900/8900 [==============================] - 1s 110us/step - loss: 0.7135 - acc: 0.5954 - f1: 0.6022 - val_loss: 0.6751 - val_acc: 0.6239 - val_f1: 0.6231\n",
      "Epoch 3/20\n",
      "8900/8900 [==============================] - 1s 86us/step - loss: 0.6881 - acc: 0.6221 - f1: 0.6267 - val_loss: 0.6700 - val_acc: 0.6249 - val_f1: 0.6256\n",
      "Epoch 4/20\n",
      "8900/8900 [==============================] - 1s 86us/step - loss: 0.6836 - acc: 0.6235 - f1: 0.6274 - val_loss: 0.6655 - val_acc: 0.6259 - val_f1: 0.6263\n",
      "Epoch 5/20\n",
      "8900/8900 [==============================] - 1s 86us/step - loss: 0.6707 - acc: 0.6365 - f1: 0.6402 - val_loss: 0.6618 - val_acc: 0.6244 - val_f1: 0.6246\n",
      "Epoch 6/20\n",
      "8900/8900 [==============================] - 1s 84us/step - loss: 0.6658 - acc: 0.6401 - f1: 0.6435 - val_loss: 0.6608 - val_acc: 0.6244 - val_f1: 0.6242\n",
      "Epoch 7/20\n",
      "8900/8900 [==============================] - 1s 86us/step - loss: 0.6634 - acc: 0.6385 - f1: 0.6402 - val_loss: 0.6591 - val_acc: 0.6244 - val_f1: 0.6242\n",
      "Epoch 8/20\n",
      "8900/8900 [==============================] - 1s 85us/step - loss: 0.6603 - acc: 0.6394 - f1: 0.6412 - val_loss: 0.6572 - val_acc: 0.6254 - val_f1: 0.6252\n",
      "Epoch 9/20\n",
      "8900/8900 [==============================] - 1s 86us/step - loss: 0.6541 - acc: 0.6417 - f1: 0.6432 - val_loss: 0.6560 - val_acc: 0.6254 - val_f1: 0.6255\n",
      "Epoch 10/20\n",
      "8900/8900 [==============================] - 1s 87us/step - loss: 0.6505 - acc: 0.6426 - f1: 0.6450 - val_loss: 0.6558 - val_acc: 0.6249 - val_f1: 0.6249\n",
      "Epoch 11/20\n",
      "8900/8900 [==============================] - 1s 87us/step - loss: 0.6518 - acc: 0.6371 - f1: 0.6383 - val_loss: 0.6548 - val_acc: 0.6249 - val_f1: 0.6249\n",
      "Epoch 12/20\n",
      "8900/8900 [==============================] - 1s 88us/step - loss: 0.6496 - acc: 0.6428 - f1: 0.6440 - val_loss: 0.6539 - val_acc: 0.6254 - val_f1: 0.6256\n",
      "Epoch 13/20\n",
      "8900/8900 [==============================] - 1s 86us/step - loss: 0.6435 - acc: 0.6494 - f1: 0.6502 - val_loss: 0.6507 - val_acc: 0.6239 - val_f1: 0.6242\n",
      "Epoch 14/20\n",
      "8900/8900 [==============================] - 1s 93us/step - loss: 0.6443 - acc: 0.6502 - f1: 0.6511 - val_loss: 0.6513 - val_acc: 0.6244 - val_f1: 0.6246\n",
      "Epoch 15/20\n",
      "8900/8900 [==============================] - 1s 108us/step - loss: 0.6393 - acc: 0.6502 - f1: 0.6509 - val_loss: 0.6498 - val_acc: 0.6259 - val_f1: 0.6259\n",
      "Epoch 16/20\n",
      "8900/8900 [==============================] - 1s 88us/step - loss: 0.6343 - acc: 0.6545 - f1: 0.6551 - val_loss: 0.6481 - val_acc: 0.6249 - val_f1: 0.6252\n",
      "Epoch 17/20\n",
      "8900/8900 [==============================] - 1s 85us/step - loss: 0.6303 - acc: 0.6602 - f1: 0.6609 - val_loss: 0.6453 - val_acc: 0.6284 - val_f1: 0.6286\n",
      "Epoch 18/20\n",
      "8900/8900 [==============================] - 1s 85us/step - loss: 0.6283 - acc: 0.6581 - f1: 0.6584 - val_loss: 0.6457 - val_acc: 0.6269 - val_f1: 0.6269\n",
      "Epoch 19/20\n",
      "8900/8900 [==============================] - 1s 85us/step - loss: 0.6183 - acc: 0.6674 - f1: 0.6675 - val_loss: 0.6441 - val_acc: 0.6249 - val_f1: 0.6249\n",
      "Epoch 20/20\n",
      "8900/8900 [==============================] - 1s 85us/step - loss: 0.6177 - acc: 0.6701 - f1: 0.6701 - val_loss: 0.6421 - val_acc: 0.6289 - val_f1: 0.6289\n",
      "989/989 [==============================] - 9s 9ms/step\n",
      "12\n",
      "(10001,)\n",
      "(6523,)\n",
      "(725, 10001)\n",
      "(725,)\n",
      "Train on 6523 samples, validate on 725 samples\n",
      "Epoch 1/20\n",
      "6523/6523 [==============================] - 24s 4ms/step - loss: 0.7529 - acc: 0.6069 - f1: 0.5955 - val_loss: 0.6630 - val_acc: 0.7014 - val_f1: 0.6996\n",
      "Epoch 2/20\n",
      "6523/6523 [==============================] - 1s 81us/step - loss: 0.6972 - acc: 0.6663 - f1: 0.6629 - val_loss: 0.6552 - val_acc: 0.7014 - val_f1: 0.7008\n",
      "Epoch 3/20\n",
      "6523/6523 [==============================] - 1s 80us/step - loss: 0.6798 - acc: 0.6954 - f1: 0.6934 - val_loss: 0.6469 - val_acc: 0.6993 - val_f1: 0.6997\n",
      "Epoch 4/20\n",
      "6523/6523 [==============================] - 1s 79us/step - loss: 0.6582 - acc: 0.7113 - f1: 0.7105 - val_loss: 0.6372 - val_acc: 0.7007 - val_f1: 0.7007\n",
      "Epoch 5/20\n",
      "6523/6523 [==============================] - 1s 80us/step - loss: 0.6508 - acc: 0.7134 - f1: 0.7116 - val_loss: 0.6287 - val_acc: 0.7007 - val_f1: 0.7007\n",
      "Epoch 6/20\n",
      "6523/6523 [==============================] - 1s 80us/step - loss: 0.6347 - acc: 0.7233 - f1: 0.7220 - val_loss: 0.6220 - val_acc: 0.7007 - val_f1: 0.7007\n",
      "Epoch 7/20\n",
      "6523/6523 [==============================] - 1s 81us/step - loss: 0.6214 - acc: 0.7231 - f1: 0.7221 - val_loss: 0.6148 - val_acc: 0.7007 - val_f1: 0.7007\n",
      "Epoch 8/20\n",
      "6523/6523 [==============================] - 1s 82us/step - loss: 0.6173 - acc: 0.7204 - f1: 0.7199 - val_loss: 0.6126 - val_acc: 0.7007 - val_f1: 0.7007\n",
      "Epoch 9/20\n",
      "6523/6523 [==============================] - 1s 80us/step - loss: 0.6100 - acc: 0.7246 - f1: 0.7234 - val_loss: 0.6114 - val_acc: 0.7007 - val_f1: 0.7007\n",
      "Epoch 10/20\n",
      "6523/6523 [==============================] - 1s 79us/step - loss: 0.6047 - acc: 0.7235 - f1: 0.7225 - val_loss: 0.6078 - val_acc: 0.7007 - val_f1: 0.7007\n",
      "Epoch 11/20\n",
      "6523/6523 [==============================] - 1s 79us/step - loss: 0.5997 - acc: 0.7241 - f1: 0.7235 - val_loss: 0.6052 - val_acc: 0.7007 - val_f1: 0.7007\n",
      "Epoch 12/20\n",
      "6523/6523 [==============================] - 1s 81us/step - loss: 0.5888 - acc: 0.7250 - f1: 0.7238 - val_loss: 0.6064 - val_acc: 0.7007 - val_f1: 0.7007\n",
      "Epoch 13/20\n",
      "6523/6523 [==============================] - 1s 90us/step - loss: 0.5842 - acc: 0.7251 - f1: 0.7242 - val_loss: 0.6069 - val_acc: 0.7007 - val_f1: 0.7007\n",
      "Epoch 14/20\n",
      "6523/6523 [==============================] - 1s 110us/step - loss: 0.5836 - acc: 0.7259 - f1: 0.7251 - val_loss: 0.6060 - val_acc: 0.7007 - val_f1: 0.7007\n",
      "Epoch 15/20\n",
      "6523/6523 [==============================] - 1s 80us/step - loss: 0.5750 - acc: 0.7264 - f1: 0.7261 - val_loss: 0.6101 - val_acc: 0.7007 - val_f1: 0.7007\n",
      "Epoch 16/20\n",
      "6523/6523 [==============================] - 1s 80us/step - loss: 0.5727 - acc: 0.7269 - f1: 0.7262 - val_loss: 0.6149 - val_acc: 0.7007 - val_f1: 0.7007\n",
      "Epoch 17/20\n",
      "6523/6523 [==============================] - 1s 80us/step - loss: 0.5641 - acc: 0.7254 - f1: 0.7244 - val_loss: 0.6246 - val_acc: 0.7007 - val_f1: 0.7007\n",
      "Epoch 18/20\n",
      "6523/6523 [==============================] - 1s 80us/step - loss: 0.5604 - acc: 0.7253 - f1: 0.7246 - val_loss: 0.6167 - val_acc: 0.7007 - val_f1: 0.7007\n",
      "Epoch 19/20\n",
      "6523/6523 [==============================] - 1s 80us/step - loss: 0.5519 - acc: 0.7240 - f1: 0.7230 - val_loss: 0.6290 - val_acc: 0.7007 - val_f1: 0.6998\n",
      "Epoch 20/20\n",
      "6523/6523 [==============================] - 1s 80us/step - loss: 0.5433 - acc: 0.7283 - f1: 0.7269 - val_loss: 0.6452 - val_acc: 0.7000 - val_f1: 0.6989\n",
      "725/725 [==============================] - 9s 13ms/step\n",
      "13\n",
      "(10001,)\n",
      "(7601,)\n",
      "(845, 10001)\n",
      "(845,)\n",
      "Train on 7601 samples, validate on 845 samples\n",
      "Epoch 1/20\n",
      "7601/7601 [==============================] - 24s 3ms/step - loss: 0.8086 - acc: 0.5469 - f1: 0.5769 - val_loss: 0.6697 - val_acc: 0.6882 - val_f1: 0.6894\n",
      "Epoch 2/20\n",
      "7601/7601 [==============================] - 1s 81us/step - loss: 0.7280 - acc: 0.6151 - f1: 0.6318 - val_loss: 0.6665 - val_acc: 0.6852 - val_f1: 0.6863\n",
      "Epoch 3/20\n",
      "7601/7601 [==============================] - 1s 80us/step - loss: 0.6953 - acc: 0.6430 - f1: 0.6496 - val_loss: 0.6581 - val_acc: 0.6899 - val_f1: 0.6899\n",
      "Epoch 4/20\n",
      "7601/7601 [==============================] - 1s 80us/step - loss: 0.6752 - acc: 0.6607 - f1: 0.6650 - val_loss: 0.6493 - val_acc: 0.6899 - val_f1: 0.6899\n",
      "Epoch 5/20\n",
      "7601/7601 [==============================] - 1s 80us/step - loss: 0.6664 - acc: 0.6664 - f1: 0.6670 - val_loss: 0.6419 - val_acc: 0.6899 - val_f1: 0.6899\n",
      "Epoch 6/20\n",
      "7601/7601 [==============================] - 1s 80us/step - loss: 0.6579 - acc: 0.6700 - f1: 0.6712 - val_loss: 0.6374 - val_acc: 0.6899 - val_f1: 0.6899\n",
      "Epoch 7/20\n",
      "7601/7601 [==============================] - 1s 82us/step - loss: 0.6511 - acc: 0.6698 - f1: 0.6700 - val_loss: 0.6330 - val_acc: 0.6899 - val_f1: 0.6899\n",
      "Epoch 8/20\n",
      "7601/7601 [==============================] - 1s 83us/step - loss: 0.6476 - acc: 0.6724 - f1: 0.6715 - val_loss: 0.6284 - val_acc: 0.6899 - val_f1: 0.6899\n",
      "Epoch 9/20\n",
      "7601/7601 [==============================] - 1s 100us/step - loss: 0.6439 - acc: 0.6719 - f1: 0.6717 - val_loss: 0.6246 - val_acc: 0.6899 - val_f1: 0.6899\n",
      "Epoch 10/20\n",
      "7601/7601 [==============================] - 1s 95us/step - loss: 0.6405 - acc: 0.6708 - f1: 0.6706 - val_loss: 0.6217 - val_acc: 0.6899 - val_f1: 0.6899\n",
      "Epoch 11/20\n",
      "7601/7601 [==============================] - 1s 80us/step - loss: 0.6385 - acc: 0.6710 - f1: 0.6708 - val_loss: 0.6203 - val_acc: 0.6899 - val_f1: 0.6899\n",
      "Epoch 12/20\n",
      "7601/7601 [==============================] - 1s 80us/step - loss: 0.6323 - acc: 0.6741 - f1: 0.6743 - val_loss: 0.6176 - val_acc: 0.6899 - val_f1: 0.6899\n",
      "Epoch 13/20\n",
      "7601/7601 [==============================] - 1s 81us/step - loss: 0.6357 - acc: 0.6724 - f1: 0.6730 - val_loss: 0.6173 - val_acc: 0.6899 - val_f1: 0.6899\n",
      "Epoch 14/20\n",
      "7601/7601 [==============================] - 1s 82us/step - loss: 0.6318 - acc: 0.6736 - f1: 0.6737 - val_loss: 0.6173 - val_acc: 0.6899 - val_f1: 0.6899\n",
      "Epoch 15/20\n",
      "7601/7601 [==============================] - 1s 80us/step - loss: 0.6323 - acc: 0.6738 - f1: 0.6737 - val_loss: 0.6158 - val_acc: 0.6899 - val_f1: 0.6899\n",
      "Epoch 16/20\n",
      "7601/7601 [==============================] - 1s 80us/step - loss: 0.6297 - acc: 0.6739 - f1: 0.6742 - val_loss: 0.6149 - val_acc: 0.6899 - val_f1: 0.6899\n",
      "Epoch 17/20\n",
      "7601/7601 [==============================] - 1s 80us/step - loss: 0.6280 - acc: 0.6746 - f1: 0.6755 - val_loss: 0.6132 - val_acc: 0.6899 - val_f1: 0.6899\n",
      "Epoch 18/20\n",
      "7601/7601 [==============================] - 1s 80us/step - loss: 0.6258 - acc: 0.6762 - f1: 0.6767 - val_loss: 0.6119 - val_acc: 0.6911 - val_f1: 0.6911\n",
      "Epoch 19/20\n",
      "7601/7601 [==============================] - 1s 80us/step - loss: 0.6276 - acc: 0.6741 - f1: 0.6747 - val_loss: 0.6120 - val_acc: 0.6899 - val_f1: 0.6899\n",
      "Epoch 20/20\n",
      "7601/7601 [==============================] - 1s 80us/step - loss: 0.6210 - acc: 0.6725 - f1: 0.6725 - val_loss: 0.6132 - val_acc: 0.6899 - val_f1: 0.6899\n",
      "845/845 [==============================] - 9s 11ms/step\n",
      "14\n",
      "(10001,)\n",
      "(8289,)\n",
      "(921, 10001)\n",
      "(921,)\n",
      "Train on 8289 samples, validate on 921 samples\n",
      "Epoch 1/20\n",
      "8289/8289 [==============================] - 25s 3ms/step - loss: 0.8045 - acc: 0.5466 - f1: 0.5072 - val_loss: 0.7634 - val_acc: 0.6308 - val_f1: 0.6280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20\n",
      "8289/8289 [==============================] - 1s 82us/step - loss: 0.7344 - acc: 0.6115 - f1: 0.6021 - val_loss: 0.7402 - val_acc: 0.6336 - val_f1: 0.6334\n",
      "Epoch 3/20\n",
      "8289/8289 [==============================] - 1s 81us/step - loss: 0.7138 - acc: 0.6358 - f1: 0.6310 - val_loss: 0.7296 - val_acc: 0.6341 - val_f1: 0.6341\n",
      "Epoch 4/20\n",
      "8289/8289 [==============================] - 1s 81us/step - loss: 0.6967 - acc: 0.6451 - f1: 0.6413 - val_loss: 0.7289 - val_acc: 0.6363 - val_f1: 0.6363\n",
      "Epoch 5/20\n",
      "8289/8289 [==============================] - 1s 82us/step - loss: 0.6998 - acc: 0.6494 - f1: 0.6467 - val_loss: 0.7102 - val_acc: 0.6341 - val_f1: 0.6341\n",
      "Epoch 6/20\n",
      "8289/8289 [==============================] - 1s 80us/step - loss: 0.6852 - acc: 0.6549 - f1: 0.6540 - val_loss: 0.7076 - val_acc: 0.6330 - val_f1: 0.6330\n",
      "Epoch 7/20\n",
      "8289/8289 [==============================] - 1s 80us/step - loss: 0.6755 - acc: 0.6530 - f1: 0.6515 - val_loss: 0.7046 - val_acc: 0.6352 - val_f1: 0.6352\n",
      "Epoch 8/20\n",
      "8289/8289 [==============================] - 1s 82us/step - loss: 0.6718 - acc: 0.6559 - f1: 0.6541 - val_loss: 0.6852 - val_acc: 0.6363 - val_f1: 0.6363\n",
      "Epoch 9/20\n",
      "8289/8289 [==============================] - 1s 84us/step - loss: 0.6728 - acc: 0.6579 - f1: 0.6564 - val_loss: 0.6751 - val_acc: 0.6368 - val_f1: 0.6370\n",
      "Epoch 10/20\n",
      "8289/8289 [==============================] - 1s 92us/step - loss: 0.6578 - acc: 0.6606 - f1: 0.6586 - val_loss: 0.6820 - val_acc: 0.6363 - val_f1: 0.6363\n",
      "Epoch 11/20\n",
      "8289/8289 [==============================] - 1s 104us/step - loss: 0.6677 - acc: 0.6616 - f1: 0.6602 - val_loss: 0.6626 - val_acc: 0.6374 - val_f1: 0.6374\n",
      "Epoch 12/20\n",
      "8289/8289 [==============================] - 1s 81us/step - loss: 0.6500 - acc: 0.6644 - f1: 0.6636 - val_loss: 0.6440 - val_acc: 0.6401 - val_f1: 0.6399\n",
      "Epoch 13/20\n",
      "8289/8289 [==============================] - 1s 81us/step - loss: 0.6495 - acc: 0.6700 - f1: 0.6691 - val_loss: 0.6616 - val_acc: 0.6374 - val_f1: 0.6374\n",
      "Epoch 14/20\n",
      "8289/8289 [==============================] - 1s 80us/step - loss: 0.6501 - acc: 0.6690 - f1: 0.6686 - val_loss: 0.6797 - val_acc: 0.6357 - val_f1: 0.6355\n",
      "Epoch 15/20\n",
      "8289/8289 [==============================] - 1s 80us/step - loss: 0.6353 - acc: 0.6726 - f1: 0.6725 - val_loss: 0.6949 - val_acc: 0.6357 - val_f1: 0.6355\n",
      "Epoch 16/20\n",
      "8289/8289 [==============================] - 1s 81us/step - loss: 0.6392 - acc: 0.6730 - f1: 0.6730 - val_loss: 0.6929 - val_acc: 0.6330 - val_f1: 0.6330\n",
      "Epoch 17/20\n",
      "8289/8289 [==============================] - 1s 82us/step - loss: 0.6239 - acc: 0.6831 - f1: 0.6830 - val_loss: 0.6900 - val_acc: 0.6368 - val_f1: 0.6366\n",
      "Epoch 18/20\n",
      "8289/8289 [==============================] - 1s 80us/step - loss: 0.6281 - acc: 0.6872 - f1: 0.6871 - val_loss: 0.6904 - val_acc: 0.6341 - val_f1: 0.6337\n",
      "Epoch 19/20\n",
      "8289/8289 [==============================] - 1s 82us/step - loss: 0.6221 - acc: 0.6915 - f1: 0.6915 - val_loss: 0.6894 - val_acc: 0.6346 - val_f1: 0.6344\n",
      "Epoch 20/20\n",
      "8289/8289 [==============================] - 1s 80us/step - loss: 0.6124 - acc: 0.6914 - f1: 0.6913 - val_loss: 0.6944 - val_acc: 0.6336 - val_f1: 0.6334\n",
      "921/921 [==============================] - 9s 10ms/step\n",
      "15\n",
      "(10001,)\n",
      "(9199,)\n",
      "(1023, 10001)\n",
      "(1023,)\n",
      "Train on 9199 samples, validate on 1023 samples\n",
      "Epoch 1/20\n",
      "9199/9199 [==============================] - 25s 3ms/step - loss: 0.8604 - acc: 0.5147 - f1: 0.5404 - val_loss: 0.7276 - val_acc: 0.5738 - val_f1: 0.5800\n",
      "Epoch 2/20\n",
      "9199/9199 [==============================] - 1s 80us/step - loss: 0.7651 - acc: 0.5475 - f1: 0.5684 - val_loss: 0.7168 - val_acc: 0.5777 - val_f1: 0.5935\n",
      "Epoch 3/20\n",
      "9199/9199 [==============================] - 1s 80us/step - loss: 0.7462 - acc: 0.5594 - f1: 0.5758 - val_loss: 0.7124 - val_acc: 0.5821 - val_f1: 0.5854\n",
      "Epoch 4/20\n",
      "9199/9199 [==============================] - 1s 79us/step - loss: 0.7344 - acc: 0.5688 - f1: 0.5838 - val_loss: 0.6958 - val_acc: 0.5802 - val_f1: 0.5828\n",
      "Epoch 5/20\n",
      "9199/9199 [==============================] - 1s 81us/step - loss: 0.7228 - acc: 0.5717 - f1: 0.5842 - val_loss: 0.7011 - val_acc: 0.5826 - val_f1: 0.5842\n",
      "Epoch 6/20\n",
      "9199/9199 [==============================] - 1s 83us/step - loss: 0.7186 - acc: 0.5739 - f1: 0.5841 - val_loss: 0.7084 - val_acc: 0.5811 - val_f1: 0.5825\n",
      "Epoch 7/20\n",
      "9199/9199 [==============================] - 1s 84us/step - loss: 0.7140 - acc: 0.5769 - f1: 0.5843 - val_loss: 0.6768 - val_acc: 0.5836 - val_f1: 0.5839\n",
      "Epoch 8/20\n",
      "9199/9199 [==============================] - 1s 109us/step - loss: 0.7008 - acc: 0.5756 - f1: 0.5815 - val_loss: 0.6765 - val_acc: 0.5836 - val_f1: 0.5840\n",
      "Epoch 9/20\n",
      "9199/9199 [==============================] - 1s 82us/step - loss: 0.7013 - acc: 0.5786 - f1: 0.5832 - val_loss: 0.6765 - val_acc: 0.5831 - val_f1: 0.5829\n",
      "Epoch 10/20\n",
      "9199/9199 [==============================] - 1s 80us/step - loss: 0.6950 - acc: 0.5822 - f1: 0.5869 - val_loss: 0.6761 - val_acc: 0.5836 - val_f1: 0.5836\n",
      "Epoch 11/20\n",
      "9199/9199 [==============================] - 1s 80us/step - loss: 0.6924 - acc: 0.5813 - f1: 0.5847 - val_loss: 0.6751 - val_acc: 0.5816 - val_f1: 0.5820\n",
      "Epoch 12/20\n",
      "9199/9199 [==============================] - 1s 80us/step - loss: 0.6880 - acc: 0.5847 - f1: 0.5876 - val_loss: 0.6748 - val_acc: 0.5821 - val_f1: 0.5823\n",
      "Epoch 13/20\n",
      "9199/9199 [==============================] - 1s 81us/step - loss: 0.6843 - acc: 0.5849 - f1: 0.5882 - val_loss: 0.6748 - val_acc: 0.5826 - val_f1: 0.5826\n",
      "Epoch 14/20\n",
      "9199/9199 [==============================] - 1s 80us/step - loss: 0.6830 - acc: 0.5859 - f1: 0.5895 - val_loss: 0.6736 - val_acc: 0.5846 - val_f1: 0.5846\n",
      "Epoch 15/20\n",
      "9199/9199 [==============================] - 1s 81us/step - loss: 0.6771 - acc: 0.5892 - f1: 0.5910 - val_loss: 0.6729 - val_acc: 0.5846 - val_f1: 0.5846\n",
      "Epoch 16/20\n",
      "9199/9199 [==============================] - 1s 81us/step - loss: 0.6808 - acc: 0.5940 - f1: 0.5957 - val_loss: 0.6717 - val_acc: 0.5846 - val_f1: 0.5846\n",
      "Epoch 17/20\n",
      "9199/9199 [==============================] - 1s 80us/step - loss: 0.6742 - acc: 0.6014 - f1: 0.6026 - val_loss: 0.6715 - val_acc: 0.5855 - val_f1: 0.5855\n",
      "Epoch 18/20\n",
      "9199/9199 [==============================] - 1s 81us/step - loss: 0.6732 - acc: 0.6077 - f1: 0.6083 - val_loss: 0.6714 - val_acc: 0.5865 - val_f1: 0.5865\n",
      "Epoch 19/20\n",
      "9199/9199 [==============================] - 1s 81us/step - loss: 0.6727 - acc: 0.6051 - f1: 0.6052 - val_loss: 0.6712 - val_acc: 0.5875 - val_f1: 0.5875\n",
      "Epoch 20/20\n",
      "9199/9199 [==============================] - 1s 82us/step - loss: 0.6722 - acc: 0.6057 - f1: 0.6058 - val_loss: 0.6707 - val_acc: 0.5855 - val_f1: 0.5855\n",
      "1023/1023 [==============================] - 10s 9ms/step\n",
      "16\n",
      "(10001,)\n",
      "(8345,)\n",
      "(928, 10001)\n",
      "(928,)\n",
      "Train on 8345 samples, validate on 928 samples\n",
      "Epoch 1/20\n",
      "8345/8345 [==============================] - 25s 3ms/step - loss: 0.7645 - acc: 0.5026 - f1: 0.5134 - val_loss: 0.6878 - val_acc: 0.5431 - val_f1: 0.4914\n",
      "Epoch 2/20\n",
      "8345/8345 [==============================] - 1s 83us/step - loss: 0.7181 - acc: 0.5138 - f1: 0.5100 - val_loss: 0.6889 - val_acc: 0.5480 - val_f1: 0.5171\n",
      "Epoch 3/20\n",
      "8345/8345 [==============================] - 1s 81us/step - loss: 0.7002 - acc: 0.5252 - f1: 0.5144 - val_loss: 0.6898 - val_acc: 0.5296 - val_f1: 0.5190\n",
      "Epoch 4/20\n",
      "8345/8345 [==============================] - 1s 81us/step - loss: 0.6932 - acc: 0.5332 - f1: 0.5253 - val_loss: 0.6877 - val_acc: 0.5426 - val_f1: 0.5310\n",
      "Epoch 5/20\n",
      "8345/8345 [==============================] - 1s 81us/step - loss: 0.6946 - acc: 0.5375 - f1: 0.5321 - val_loss: 0.6901 - val_acc: 0.5286 - val_f1: 0.5205\n",
      "Epoch 6/20\n",
      "8345/8345 [==============================] - 1s 83us/step - loss: 0.6894 - acc: 0.5412 - f1: 0.5349 - val_loss: 0.6907 - val_acc: 0.5248 - val_f1: 0.5154\n",
      "Epoch 7/20\n",
      "8345/8345 [==============================] - 1s 82us/step - loss: 0.6794 - acc: 0.5633 - f1: 0.5601 - val_loss: 0.6883 - val_acc: 0.5302 - val_f1: 0.5224\n",
      "Epoch 8/20\n",
      "8345/8345 [==============================] - 1s 83us/step - loss: 0.6800 - acc: 0.5533 - f1: 0.5505 - val_loss: 0.6893 - val_acc: 0.5323 - val_f1: 0.5255\n",
      "Epoch 9/20\n",
      "8345/8345 [==============================] - 1s 91us/step - loss: 0.6757 - acc: 0.5726 - f1: 0.5719 - val_loss: 0.6891 - val_acc: 0.5372 - val_f1: 0.5329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20\n",
      "8345/8345 [==============================] - 1s 106us/step - loss: 0.6725 - acc: 0.5769 - f1: 0.5770 - val_loss: 0.6888 - val_acc: 0.5296 - val_f1: 0.5243\n",
      "Epoch 11/20\n",
      "8345/8345 [==============================] - 1s 83us/step - loss: 0.6700 - acc: 0.5890 - f1: 0.5896 - val_loss: 0.6869 - val_acc: 0.5393 - val_f1: 0.5352\n",
      "Epoch 12/20\n",
      "8345/8345 [==============================] - 1s 82us/step - loss: 0.6639 - acc: 0.5892 - f1: 0.5902 - val_loss: 0.6781 - val_acc: 0.5550 - val_f1: 0.5514\n",
      "Epoch 13/20\n",
      "8345/8345 [==============================] - 1s 83us/step - loss: 0.6611 - acc: 0.6014 - f1: 0.6022 - val_loss: 0.6793 - val_acc: 0.5555 - val_f1: 0.5543\n",
      "Epoch 14/20\n",
      "8345/8345 [==============================] - 1s 82us/step - loss: 0.6521 - acc: 0.6143 - f1: 0.6151 - val_loss: 0.6742 - val_acc: 0.5636 - val_f1: 0.5623\n",
      "Epoch 15/20\n",
      "8345/8345 [==============================] - 1s 84us/step - loss: 0.6460 - acc: 0.6234 - f1: 0.6238 - val_loss: 0.6710 - val_acc: 0.5690 - val_f1: 0.5681\n",
      "Epoch 16/20\n",
      "8345/8345 [==============================] - 1s 81us/step - loss: 0.6385 - acc: 0.6354 - f1: 0.6360 - val_loss: 0.6698 - val_acc: 0.5652 - val_f1: 0.5636\n",
      "Epoch 17/20\n",
      "8345/8345 [==============================] - 1s 84us/step - loss: 0.6319 - acc: 0.6509 - f1: 0.6515 - val_loss: 0.6656 - val_acc: 0.5770 - val_f1: 0.5754\n",
      "Epoch 18/20\n",
      "8345/8345 [==============================] - 1s 83us/step - loss: 0.6234 - acc: 0.6579 - f1: 0.6583 - val_loss: 0.6673 - val_acc: 0.5668 - val_f1: 0.5664\n",
      "Epoch 19/20\n",
      "8345/8345 [==============================] - 1s 83us/step - loss: 0.6146 - acc: 0.6715 - f1: 0.6718 - val_loss: 0.6671 - val_acc: 0.5776 - val_f1: 0.5771\n",
      "Epoch 20/20\n",
      "8345/8345 [==============================] - 1s 83us/step - loss: 0.6109 - acc: 0.6718 - f1: 0.6717 - val_loss: 0.6656 - val_acc: 0.5679 - val_f1: 0.5674\n",
      "928/928 [==============================] - 10s 11ms/step\n",
      "17\n",
      "(10001,)\n",
      "(8127,)\n",
      "(904, 10001)\n",
      "(904,)\n",
      "Train on 8127 samples, validate on 904 samples\n",
      "Epoch 1/20\n",
      "8127/8127 [==============================] - 26s 3ms/step - loss: 0.7881 - acc: 0.5002 - f1: 0.4693 - val_loss: 0.6928 - val_acc: 0.4978 - val_f1: 0.6237\n",
      "Epoch 2/20\n",
      "8127/8127 [==============================] - 1s 91us/step - loss: 0.7284 - acc: 0.5055 - f1: 0.5364 - val_loss: 0.6922 - val_acc: 0.5315 - val_f1: 0.6042\n",
      "Epoch 3/20\n",
      "8127/8127 [==============================] - 1s 114us/step - loss: 0.7070 - acc: 0.5220 - f1: 0.5370 - val_loss: 0.6915 - val_acc: 0.5321 - val_f1: 0.5500\n",
      "Epoch 4/20\n",
      "8127/8127 [==============================] - 1s 84us/step - loss: 0.6988 - acc: 0.5249 - f1: 0.5360 - val_loss: 0.6907 - val_acc: 0.5304 - val_f1: 0.5342\n",
      "Epoch 5/20\n",
      "8127/8127 [==============================] - 1s 83us/step - loss: 0.6933 - acc: 0.5428 - f1: 0.5523 - val_loss: 0.6904 - val_acc: 0.5332 - val_f1: 0.5393\n",
      "Epoch 6/20\n",
      "8127/8127 [==============================] - 1s 82us/step - loss: 0.6937 - acc: 0.5409 - f1: 0.5504 - val_loss: 0.6910 - val_acc: 0.5254 - val_f1: 0.5270\n",
      "Epoch 7/20\n",
      "8127/8127 [==============================] - 1s 83us/step - loss: 0.6923 - acc: 0.5518 - f1: 0.5569 - val_loss: 0.6899 - val_acc: 0.5332 - val_f1: 0.5342\n",
      "Epoch 8/20\n",
      "8127/8127 [==============================] - 1s 82us/step - loss: 0.6890 - acc: 0.5520 - f1: 0.5558 - val_loss: 0.6896 - val_acc: 0.5326 - val_f1: 0.5340\n",
      "Epoch 9/20\n",
      "8127/8127 [==============================] - 1s 82us/step - loss: 0.6879 - acc: 0.5590 - f1: 0.5626 - val_loss: 0.6896 - val_acc: 0.5360 - val_f1: 0.5362\n",
      "Epoch 10/20\n",
      "8127/8127 [==============================] - 1s 83us/step - loss: 0.6873 - acc: 0.5560 - f1: 0.5590 - val_loss: 0.6889 - val_acc: 0.5393 - val_f1: 0.5395\n",
      "Epoch 11/20\n",
      "8127/8127 [==============================] - 1s 82us/step - loss: 0.6840 - acc: 0.5614 - f1: 0.5651 - val_loss: 0.6877 - val_acc: 0.5387 - val_f1: 0.5403\n",
      "Epoch 12/20\n",
      "8127/8127 [==============================] - 1s 83us/step - loss: 0.6850 - acc: 0.5634 - f1: 0.5664 - val_loss: 0.6884 - val_acc: 0.5343 - val_f1: 0.5348\n",
      "Epoch 13/20\n",
      "8127/8127 [==============================] - 1s 99us/step - loss: 0.6813 - acc: 0.5712 - f1: 0.5738 - val_loss: 0.6883 - val_acc: 0.5365 - val_f1: 0.5365\n",
      "Epoch 14/20\n",
      "8127/8127 [==============================] - 1s 96us/step - loss: 0.6795 - acc: 0.5830 - f1: 0.5852 - val_loss: 0.6874 - val_acc: 0.5371 - val_f1: 0.5378\n",
      "Epoch 15/20\n",
      "8127/8127 [==============================] - 1s 83us/step - loss: 0.6767 - acc: 0.5896 - f1: 0.5912 - val_loss: 0.6886 - val_acc: 0.5293 - val_f1: 0.5296\n",
      "Epoch 16/20\n",
      "8127/8127 [==============================] - 1s 82us/step - loss: 0.6693 - acc: 0.6050 - f1: 0.6062 - val_loss: 0.6854 - val_acc: 0.5371 - val_f1: 0.5378\n",
      "Epoch 17/20\n",
      "8127/8127 [==============================] - 1s 82us/step - loss: 0.6696 - acc: 0.6051 - f1: 0.6051 - val_loss: 0.6832 - val_acc: 0.5542 - val_f1: 0.5552\n",
      "Epoch 18/20\n",
      "8127/8127 [==============================] - 1s 81us/step - loss: 0.6637 - acc: 0.6105 - f1: 0.6109 - val_loss: 0.6797 - val_acc: 0.5642 - val_f1: 0.5652\n",
      "Epoch 19/20\n",
      "8127/8127 [==============================] - 1s 81us/step - loss: 0.6619 - acc: 0.6196 - f1: 0.6203 - val_loss: 0.6840 - val_acc: 0.5371 - val_f1: 0.5373\n",
      "Epoch 20/20\n",
      "8127/8127 [==============================] - 1s 81us/step - loss: 0.6504 - acc: 0.6336 - f1: 0.6336 - val_loss: 0.6750 - val_acc: 0.5719 - val_f1: 0.5724\n",
      "904/904 [==============================] - 10s 11ms/step\n",
      "18\n",
      "(10001,)\n",
      "(7490,)\n",
      "(833, 10001)\n",
      "(833,)\n",
      "Train on 7490 samples, validate on 833 samples\n",
      "Epoch 1/20\n",
      "7490/7490 [==============================] - 26s 3ms/step - loss: 0.8112 - acc: 0.5033 - f1: 0.4681 - val_loss: 0.6968 - val_acc: 0.5138 - val_f1: 0.5059\n",
      "Epoch 2/20\n",
      "7490/7490 [==============================] - 1s 82us/step - loss: 0.7410 - acc: 0.5089 - f1: 0.4885 - val_loss: 0.6924 - val_acc: 0.5204 - val_f1: 0.5189\n",
      "Epoch 3/20\n",
      "7490/7490 [==============================] - 1s 83us/step - loss: 0.7214 - acc: 0.5194 - f1: 0.5140 - val_loss: 0.6916 - val_acc: 0.5216 - val_f1: 0.5208\n",
      "Epoch 4/20\n",
      "7490/7490 [==============================] - 1s 83us/step - loss: 0.7129 - acc: 0.5114 - f1: 0.5149 - val_loss: 0.6928 - val_acc: 0.5126 - val_f1: 0.5127\n",
      "Epoch 5/20\n",
      "7490/7490 [==============================] - 1s 82us/step - loss: 0.7082 - acc: 0.5104 - f1: 0.5192 - val_loss: 0.6930 - val_acc: 0.5120 - val_f1: 0.5129\n",
      "Epoch 6/20\n",
      "7490/7490 [==============================] - 1s 81us/step - loss: 0.6998 - acc: 0.5111 - f1: 0.5176 - val_loss: 0.6932 - val_acc: 0.5102 - val_f1: 0.5120\n",
      "Epoch 7/20\n",
      "7490/7490 [==============================] - 1s 82us/step - loss: 0.6993 - acc: 0.5252 - f1: 0.5353 - val_loss: 0.6928 - val_acc: 0.5144 - val_f1: 0.5152\n",
      "Epoch 8/20\n",
      "7490/7490 [==============================] - 1s 83us/step - loss: 0.7003 - acc: 0.5206 - f1: 0.5328 - val_loss: 0.6928 - val_acc: 0.5126 - val_f1: 0.5120\n",
      "Epoch 9/20\n",
      "7490/7490 [==============================] - 1s 83us/step - loss: 0.6957 - acc: 0.5209 - f1: 0.5334 - val_loss: 0.6927 - val_acc: 0.5156 - val_f1: 0.5182\n",
      "Epoch 10/20\n",
      "7490/7490 [==============================] - 1s 85us/step - loss: 0.6957 - acc: 0.5276 - f1: 0.5388 - val_loss: 0.6930 - val_acc: 0.5108 - val_f1: 0.5128\n",
      "Epoch 11/20\n",
      "7490/7490 [==============================] - 1s 84us/step - loss: 0.6961 - acc: 0.5167 - f1: 0.5295 - val_loss: 0.6931 - val_acc: 0.5108 - val_f1: 0.5134\n",
      "Epoch 12/20\n",
      "7490/7490 [==============================] - 1s 89us/step - loss: 0.6935 - acc: 0.5338 - f1: 0.5438 - val_loss: 0.6921 - val_acc: 0.5204 - val_f1: 0.5212\n",
      "Epoch 13/20\n",
      "7490/7490 [==============================] - 1s 111us/step - loss: 0.6947 - acc: 0.5352 - f1: 0.5459 - val_loss: 0.6927 - val_acc: 0.5138 - val_f1: 0.5138\n",
      "Epoch 14/20\n",
      "7490/7490 [==============================] - 1s 82us/step - loss: 0.6944 - acc: 0.5276 - f1: 0.5378 - val_loss: 0.6922 - val_acc: 0.5180 - val_f1: 0.5183\n",
      "Epoch 15/20\n",
      "7490/7490 [==============================] - 1s 81us/step - loss: 0.6932 - acc: 0.5237 - f1: 0.5336 - val_loss: 0.6927 - val_acc: 0.5120 - val_f1: 0.5129\n",
      "Epoch 16/20\n",
      "7490/7490 [==============================] - 1s 81us/step - loss: 0.6938 - acc: 0.5234 - f1: 0.5329 - val_loss: 0.6929 - val_acc: 0.5114 - val_f1: 0.5119\n",
      "Epoch 17/20\n",
      "7490/7490 [==============================] - 1s 81us/step - loss: 0.6922 - acc: 0.5358 - f1: 0.5430 - val_loss: 0.6919 - val_acc: 0.5198 - val_f1: 0.5199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20\n",
      "7490/7490 [==============================] - 1s 81us/step - loss: 0.6904 - acc: 0.5466 - f1: 0.5521 - val_loss: 0.6912 - val_acc: 0.5252 - val_f1: 0.5256\n",
      "Epoch 19/20\n",
      "7490/7490 [==============================] - 1s 82us/step - loss: 0.6894 - acc: 0.5435 - f1: 0.5503 - val_loss: 0.6916 - val_acc: 0.5210 - val_f1: 0.5216\n",
      "Epoch 20/20\n",
      "7490/7490 [==============================] - 1s 81us/step - loss: 0.6911 - acc: 0.5415 - f1: 0.5477 - val_loss: 0.6922 - val_acc: 0.5204 - val_f1: 0.5219\n",
      "833/833 [==============================] - 10s 12ms/step\n",
      "19\n",
      "(10001,)\n",
      "(9271,)\n",
      "(1031, 10001)\n",
      "(1031,)\n",
      "Train on 9271 samples, validate on 1031 samples\n",
      "Epoch 1/20\n",
      "9271/9271 [==============================] - 26s 3ms/step - loss: 0.7652 - acc: 0.5160 - f1: 0.4890 - val_loss: 0.6824 - val_acc: 0.6086 - val_f1: 0.6144\n",
      "Epoch 2/20\n",
      "9271/9271 [==============================] - 1s 115us/step - loss: 0.7154 - acc: 0.5590 - f1: 0.5478 - val_loss: 0.6780 - val_acc: 0.6086 - val_f1: 0.6092\n",
      "Epoch 3/20\n",
      "9271/9271 [==============================] - 1s 88us/step - loss: 0.6918 - acc: 0.5809 - f1: 0.5724 - val_loss: 0.6732 - val_acc: 0.6120 - val_f1: 0.6119\n",
      "Epoch 4/20\n",
      "9271/9271 [==============================] - 1s 90us/step - loss: 0.6923 - acc: 0.5854 - f1: 0.5807 - val_loss: 0.6720 - val_acc: 0.6062 - val_f1: 0.6054\n",
      "Epoch 5/20\n",
      "9271/9271 [==============================] - 1s 90us/step - loss: 0.6850 - acc: 0.5891 - f1: 0.5852 - val_loss: 0.6702 - val_acc: 0.6062 - val_f1: 0.6058\n",
      "Epoch 6/20\n",
      "9271/9271 [==============================] - 1s 90us/step - loss: 0.6838 - acc: 0.5944 - f1: 0.5903 - val_loss: 0.6687 - val_acc: 0.6048 - val_f1: 0.6045\n",
      "Epoch 7/20\n",
      "9271/9271 [==============================] - 1s 90us/step - loss: 0.6782 - acc: 0.5921 - f1: 0.5888 - val_loss: 0.6673 - val_acc: 0.6048 - val_f1: 0.6050\n",
      "Epoch 8/20\n",
      "9271/9271 [==============================] - 1s 90us/step - loss: 0.6770 - acc: 0.5936 - f1: 0.5911 - val_loss: 0.6668 - val_acc: 0.6043 - val_f1: 0.6043\n",
      "Epoch 9/20\n",
      "9271/9271 [==============================] - 1s 90us/step - loss: 0.6742 - acc: 0.5987 - f1: 0.5976 - val_loss: 0.6644 - val_acc: 0.6043 - val_f1: 0.6038\n",
      "Epoch 10/20\n",
      "9271/9271 [==============================] - 1s 90us/step - loss: 0.6697 - acc: 0.6051 - f1: 0.6044 - val_loss: 0.6610 - val_acc: 0.6086 - val_f1: 0.6080\n",
      "Epoch 11/20\n",
      "9271/9271 [==============================] - 1s 90us/step - loss: 0.6651 - acc: 0.6144 - f1: 0.6125 - val_loss: 0.6576 - val_acc: 0.6106 - val_f1: 0.6104\n",
      "Epoch 12/20\n",
      "9271/9271 [==============================] - 1s 98us/step - loss: 0.6563 - acc: 0.6254 - f1: 0.6244 - val_loss: 0.6497 - val_acc: 0.6159 - val_f1: 0.6147\n",
      "Epoch 13/20\n",
      "9271/9271 [==============================] - 1s 97us/step - loss: 0.6486 - acc: 0.6397 - f1: 0.6387 - val_loss: 0.6424 - val_acc: 0.6217 - val_f1: 0.6217\n",
      "Epoch 14/20\n",
      "9271/9271 [==============================] - 1s 91us/step - loss: 0.6408 - acc: 0.6517 - f1: 0.6506 - val_loss: 0.6294 - val_acc: 0.6372 - val_f1: 0.6365\n",
      "Epoch 15/20\n",
      "9271/9271 [==============================] - 1s 100us/step - loss: 0.6322 - acc: 0.6583 - f1: 0.6574 - val_loss: 0.6199 - val_acc: 0.6542 - val_f1: 0.6526\n",
      "Epoch 16/20\n",
      "9271/9271 [==============================] - 1s 89us/step - loss: 0.6149 - acc: 0.6821 - f1: 0.6815 - val_loss: 0.6108 - val_acc: 0.6663 - val_f1: 0.6654\n",
      "Epoch 17/20\n",
      "9271/9271 [==============================] - 1s 92us/step - loss: 0.5966 - acc: 0.6989 - f1: 0.6979 - val_loss: 0.5949 - val_acc: 0.6833 - val_f1: 0.6812\n",
      "Epoch 18/20\n",
      "9271/9271 [==============================] - 1s 89us/step - loss: 0.5819 - acc: 0.7157 - f1: 0.7153 - val_loss: 0.5828 - val_acc: 0.7013 - val_f1: 0.7001\n",
      "Epoch 19/20\n",
      "9271/9271 [==============================] - 1s 90us/step - loss: 0.5643 - acc: 0.7306 - f1: 0.7302 - val_loss: 0.5721 - val_acc: 0.7114 - val_f1: 0.7113\n",
      "Epoch 20/20\n",
      "9271/9271 [==============================] - 1s 89us/step - loss: 0.5497 - acc: 0.7399 - f1: 0.7398 - val_loss: 0.5664 - val_acc: 0.7236 - val_f1: 0.7232\n",
      "1031/1031 [==============================] - 10s 10ms/step\n",
      "20\n",
      "(10001,)\n",
      "(8858,)\n",
      "(985, 10001)\n",
      "(985,)\n",
      "Train on 8858 samples, validate on 985 samples\n",
      "Epoch 1/20\n",
      "8858/8858 [==============================] - 26s 3ms/step - loss: 0.7509 - acc: 0.5297 - f1: 0.4789 - val_loss: 0.6933 - val_acc: 0.5239 - val_f1: 0.6398\n",
      "Epoch 2/20\n",
      "8858/8858 [==============================] - 1s 112us/step - loss: 0.7258 - acc: 0.5295 - f1: 0.5538 - val_loss: 0.6922 - val_acc: 0.4929 - val_f1: 0.6377\n",
      "Epoch 3/20\n",
      "8858/8858 [==============================] - 1s 90us/step - loss: 0.7077 - acc: 0.5406 - f1: 0.5749 - val_loss: 0.6895 - val_acc: 0.5614 - val_f1: 0.5823\n",
      "Epoch 4/20\n",
      "8858/8858 [==============================] - 1s 90us/step - loss: 0.7005 - acc: 0.5480 - f1: 0.5692 - val_loss: 0.6886 - val_acc: 0.5548 - val_f1: 0.5577\n",
      "Epoch 5/20\n",
      "8858/8858 [==============================] - 1s 91us/step - loss: 0.6936 - acc: 0.5573 - f1: 0.5742 - val_loss: 0.6870 - val_acc: 0.5563 - val_f1: 0.5572\n",
      "Epoch 6/20\n",
      "8858/8858 [==============================] - 1s 113us/step - loss: 0.6928 - acc: 0.5598 - f1: 0.5712 - val_loss: 0.6849 - val_acc: 0.5538 - val_f1: 0.5541\n",
      "Epoch 7/20\n",
      "8858/8858 [==============================] - 1s 98us/step - loss: 0.6904 - acc: 0.5639 - f1: 0.5735 - val_loss: 0.6821 - val_acc: 0.5543 - val_f1: 0.5579\n",
      "Epoch 8/20\n",
      "8858/8858 [==============================] - 1s 89us/step - loss: 0.6885 - acc: 0.5675 - f1: 0.5765 - val_loss: 0.6816 - val_acc: 0.5563 - val_f1: 0.5581\n",
      "Epoch 9/20\n",
      "8858/8858 [==============================] - 1s 90us/step - loss: 0.6829 - acc: 0.5713 - f1: 0.5778 - val_loss: 0.6775 - val_acc: 0.5589 - val_f1: 0.5631\n",
      "Epoch 10/20\n",
      "8858/8858 [==============================] - 1s 88us/step - loss: 0.6791 - acc: 0.5884 - f1: 0.5932 - val_loss: 0.6665 - val_acc: 0.5868 - val_f1: 0.5939\n",
      "Epoch 11/20\n",
      "8858/8858 [==============================] - 1s 88us/step - loss: 0.6687 - acc: 0.5977 - f1: 0.6010 - val_loss: 0.6529 - val_acc: 0.6183 - val_f1: 0.6217\n",
      "Epoch 12/20\n",
      "8858/8858 [==============================] - 1s 88us/step - loss: 0.6559 - acc: 0.6107 - f1: 0.6118 - val_loss: 0.6409 - val_acc: 0.6320 - val_f1: 0.6341\n",
      "Epoch 13/20\n",
      "8858/8858 [==============================] - 1s 87us/step - loss: 0.6418 - acc: 0.6293 - f1: 0.6290 - val_loss: 0.6242 - val_acc: 0.6609 - val_f1: 0.6630\n",
      "Epoch 14/20\n",
      "8858/8858 [==============================] - 1s 89us/step - loss: 0.6244 - acc: 0.6497 - f1: 0.6496 - val_loss: 0.6170 - val_acc: 0.6787 - val_f1: 0.6807\n",
      "Epoch 15/20\n",
      "8858/8858 [==============================] - 1s 88us/step - loss: 0.6042 - acc: 0.6664 - f1: 0.6661 - val_loss: 0.5999 - val_acc: 0.6904 - val_f1: 0.6919\n",
      "Epoch 16/20\n",
      "8858/8858 [==============================] - 1s 89us/step - loss: 0.6000 - acc: 0.6858 - f1: 0.6860 - val_loss: 0.5921 - val_acc: 0.7056 - val_f1: 0.7065\n",
      "Epoch 17/20\n",
      "8858/8858 [==============================] - 1s 89us/step - loss: 0.5871 - acc: 0.6976 - f1: 0.6981 - val_loss: 0.5849 - val_acc: 0.7107 - val_f1: 0.7112\n",
      "Epoch 18/20\n",
      "8858/8858 [==============================] - 1s 91us/step - loss: 0.5621 - acc: 0.7199 - f1: 0.7199 - val_loss: 0.5736 - val_acc: 0.7132 - val_f1: 0.7142\n",
      "Epoch 19/20\n",
      "8858/8858 [==============================] - 1s 110us/step - loss: 0.5478 - acc: 0.7300 - f1: 0.7301 - val_loss: 0.5610 - val_acc: 0.7223 - val_f1: 0.7237\n",
      "Epoch 20/20\n",
      "8858/8858 [==============================] - 1s 100us/step - loss: 0.5243 - acc: 0.7512 - f1: 0.7514 - val_loss: 0.5552 - val_acc: 0.7274 - val_f1: 0.7292\n",
      "985/985 [==============================] - 10s 10ms/step\n",
      "21\n",
      "(10001,)\n",
      "(7812,)\n",
      "(868, 10001)\n",
      "(868,)\n",
      "Train on 7812 samples, validate on 868 samples\n",
      "Epoch 1/20\n",
      "7812/7812 [==============================] - 27s 3ms/step - loss: 0.7262 - acc: 0.4991 - f1: 0.4475 - val_loss: 0.6909 - val_acc: 0.5363 - val_f1: 0.5883\n",
      "Epoch 2/20\n",
      "7812/7812 [==============================] - 1s 86us/step - loss: 0.7049 - acc: 0.5157 - f1: 0.5110 - val_loss: 0.6887 - val_acc: 0.5403 - val_f1: 0.5441\n",
      "Epoch 3/20\n",
      "7812/7812 [==============================] - 1s 102us/step - loss: 0.6976 - acc: 0.5306 - f1: 0.5283 - val_loss: 0.6872 - val_acc: 0.5501 - val_f1: 0.5453\n",
      "Epoch 4/20\n",
      "7812/7812 [==============================] - 1s 100us/step - loss: 0.6904 - acc: 0.5489 - f1: 0.5443 - val_loss: 0.6817 - val_acc: 0.5599 - val_f1: 0.5517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20\n",
      "7812/7812 [==============================] - 1s 85us/step - loss: 0.6867 - acc: 0.5605 - f1: 0.5553 - val_loss: 0.6720 - val_acc: 0.5829 - val_f1: 0.5784\n",
      "Epoch 6/20\n",
      "7812/7812 [==============================] - 1s 82us/step - loss: 0.6674 - acc: 0.5871 - f1: 0.5829 - val_loss: 0.6505 - val_acc: 0.6123 - val_f1: 0.6123\n",
      "Epoch 7/20\n",
      "7812/7812 [==============================] - 1s 82us/step - loss: 0.6482 - acc: 0.6237 - f1: 0.6247 - val_loss: 0.6261 - val_acc: 0.6429 - val_f1: 0.6432\n",
      "Epoch 8/20\n",
      "7812/7812 [==============================] - 1s 82us/step - loss: 0.6406 - acc: 0.6381 - f1: 0.6390 - val_loss: 0.6120 - val_acc: 0.6607 - val_f1: 0.6595\n",
      "Epoch 9/20\n",
      "7812/7812 [==============================] - 1s 82us/step - loss: 0.6133 - acc: 0.6739 - f1: 0.6738 - val_loss: 0.5875 - val_acc: 0.6901 - val_f1: 0.6905\n",
      "Epoch 10/20\n",
      "7812/7812 [==============================] - 1s 82us/step - loss: 0.5799 - acc: 0.7010 - f1: 0.7010 - val_loss: 0.5621 - val_acc: 0.6947 - val_f1: 0.6951\n",
      "Epoch 11/20\n",
      "7812/7812 [==============================] - 1s 82us/step - loss: 0.5642 - acc: 0.7235 - f1: 0.7239 - val_loss: 0.5524 - val_acc: 0.7200 - val_f1: 0.7201\n",
      "Epoch 12/20\n",
      "7812/7812 [==============================] - 1s 82us/step - loss: 0.5297 - acc: 0.7503 - f1: 0.7503 - val_loss: 0.5342 - val_acc: 0.7425 - val_f1: 0.7442\n",
      "Epoch 13/20\n",
      "7812/7812 [==============================] - 1s 84us/step - loss: 0.4930 - acc: 0.7728 - f1: 0.7729 - val_loss: 0.5108 - val_acc: 0.7483 - val_f1: 0.7489\n",
      "Epoch 14/20\n",
      "7812/7812 [==============================] - 1s 83us/step - loss: 0.4824 - acc: 0.7908 - f1: 0.7910 - val_loss: 0.4903 - val_acc: 0.7638 - val_f1: 0.7647\n",
      "Epoch 15/20\n",
      "7812/7812 [==============================] - 1s 83us/step - loss: 0.4513 - acc: 0.8081 - f1: 0.8084 - val_loss: 0.4924 - val_acc: 0.7563 - val_f1: 0.7571\n",
      "Epoch 16/20\n",
      "7812/7812 [==============================] - 1s 86us/step - loss: 0.4311 - acc: 0.8253 - f1: 0.8257 - val_loss: 0.4915 - val_acc: 0.7512 - val_f1: 0.7522\n",
      "Epoch 17/20\n",
      "7812/7812 [==============================] - 1s 101us/step - loss: 0.4121 - acc: 0.8300 - f1: 0.8304 - val_loss: 0.4789 - val_acc: 0.7632 - val_f1: 0.7642\n",
      "Epoch 18/20\n",
      "7812/7812 [==============================] - 1s 89us/step - loss: 0.3947 - acc: 0.8455 - f1: 0.8460 - val_loss: 0.4597 - val_acc: 0.7759 - val_f1: 0.7770\n",
      "Epoch 19/20\n",
      "7812/7812 [==============================] - 1s 82us/step - loss: 0.3847 - acc: 0.8535 - f1: 0.8540 - val_loss: 0.4561 - val_acc: 0.7702 - val_f1: 0.7719\n",
      "Epoch 20/20\n",
      "7812/7812 [==============================] - 1s 81us/step - loss: 0.3491 - acc: 0.8680 - f1: 0.8685 - val_loss: 0.4760 - val_acc: 0.7661 - val_f1: 0.7667\n",
      "868/868 [==============================] - 10s 11ms/step\n",
      "22\n",
      "(10001,)\n",
      "(8811,)\n",
      "(979, 10001)\n",
      "(979,)\n",
      "Train on 8811 samples, validate on 979 samples\n",
      "Epoch 1/20\n",
      "8811/8811 [==============================] - 26s 3ms/step - loss: 0.7352 - acc: 0.5338 - f1: 0.5618 - val_loss: 0.6887 - val_acc: 0.5909 - val_f1: 0.5781\n",
      "Epoch 2/20\n",
      "8811/8811 [==============================] - 1s 113us/step - loss: 0.7072 - acc: 0.5633 - f1: 0.5628 - val_loss: 0.6851 - val_acc: 0.5894 - val_f1: 0.5873\n",
      "Epoch 3/20\n",
      "8811/8811 [==============================] - 1s 100us/step - loss: 0.6963 - acc: 0.5843 - f1: 0.5793 - val_loss: 0.6824 - val_acc: 0.5899 - val_f1: 0.5897\n",
      "Epoch 4/20\n",
      "8811/8811 [==============================] - 1s 88us/step - loss: 0.6871 - acc: 0.5937 - f1: 0.5888 - val_loss: 0.6740 - val_acc: 0.6073 - val_f1: 0.6054\n",
      "Epoch 5/20\n",
      "8811/8811 [==============================] - 1s 88us/step - loss: 0.6778 - acc: 0.6036 - f1: 0.5998 - val_loss: 0.6663 - val_acc: 0.6261 - val_f1: 0.6262\n",
      "Epoch 6/20\n",
      "8811/8811 [==============================] - 1s 89us/step - loss: 0.6751 - acc: 0.6069 - f1: 0.6030 - val_loss: 0.6565 - val_acc: 0.6364 - val_f1: 0.6364\n",
      "Epoch 7/20\n",
      "8811/8811 [==============================] - 1s 90us/step - loss: 0.6586 - acc: 0.6299 - f1: 0.6265 - val_loss: 0.6385 - val_acc: 0.6537 - val_f1: 0.6535\n",
      "Epoch 8/20\n",
      "8811/8811 [==============================] - 1s 87us/step - loss: 0.6433 - acc: 0.6455 - f1: 0.6420 - val_loss: 0.6229 - val_acc: 0.6639 - val_f1: 0.6633\n",
      "Epoch 9/20\n",
      "8811/8811 [==============================] - 1s 90us/step - loss: 0.6316 - acc: 0.6608 - f1: 0.6564 - val_loss: 0.6175 - val_acc: 0.6742 - val_f1: 0.6728\n",
      "Epoch 10/20\n",
      "8811/8811 [==============================] - 1s 88us/step - loss: 0.6256 - acc: 0.6697 - f1: 0.6655 - val_loss: 0.6096 - val_acc: 0.6782 - val_f1: 0.6759\n",
      "Epoch 11/20\n",
      "8811/8811 [==============================] - 1s 87us/step - loss: 0.6014 - acc: 0.6834 - f1: 0.6803 - val_loss: 0.5937 - val_acc: 0.6788 - val_f1: 0.6769\n",
      "Epoch 12/20\n",
      "8811/8811 [==============================] - 1s 88us/step - loss: 0.5825 - acc: 0.7044 - f1: 0.7024 - val_loss: 0.5929 - val_acc: 0.6951 - val_f1: 0.6918\n",
      "Epoch 13/20\n",
      "8811/8811 [==============================] - 1s 89us/step - loss: 0.5617 - acc: 0.7200 - f1: 0.7185 - val_loss: 0.5823 - val_acc: 0.6900 - val_f1: 0.6877\n",
      "Epoch 14/20\n",
      "8811/8811 [==============================] - 1s 106us/step - loss: 0.5546 - acc: 0.7281 - f1: 0.7267 - val_loss: 0.5787 - val_acc: 0.7007 - val_f1: 0.6982\n",
      "Epoch 15/20\n",
      "8811/8811 [==============================] - 1s 105us/step - loss: 0.5328 - acc: 0.7438 - f1: 0.7433 - val_loss: 0.5846 - val_acc: 0.6977 - val_f1: 0.6960\n",
      "Epoch 16/20\n",
      "8811/8811 [==============================] - 1s 89us/step - loss: 0.5209 - acc: 0.7487 - f1: 0.7484 - val_loss: 0.5850 - val_acc: 0.7043 - val_f1: 0.7033\n",
      "Epoch 17/20\n",
      "8811/8811 [==============================] - 1s 90us/step - loss: 0.4946 - acc: 0.7627 - f1: 0.7624 - val_loss: 0.5946 - val_acc: 0.7048 - val_f1: 0.7032\n",
      "Epoch 18/20\n",
      "8811/8811 [==============================] - 1s 88us/step - loss: 0.4901 - acc: 0.7774 - f1: 0.7772 - val_loss: 0.6017 - val_acc: 0.7058 - val_f1: 0.7047\n",
      "Epoch 19/20\n",
      "8811/8811 [==============================] - 1s 87us/step - loss: 0.4742 - acc: 0.7850 - f1: 0.7848 - val_loss: 0.6063 - val_acc: 0.7079 - val_f1: 0.7069\n",
      "Epoch 20/20\n",
      "8811/8811 [==============================] - 1s 87us/step - loss: 0.4491 - acc: 0.7980 - f1: 0.7977 - val_loss: 0.6161 - val_acc: 0.7094 - val_f1: 0.7080\n",
      "979/979 [==============================] - 10s 10ms/step\n",
      "23\n",
      "(10001,)\n",
      "(10332,)\n",
      "(1149, 10001)\n",
      "(1149,)\n",
      "Train on 10332 samples, validate on 1149 samples\n",
      "Epoch 1/20\n",
      "10332/10332 [==============================] - 28s 3ms/step - loss: 0.7285 - acc: 0.5270 - f1: 0.5513 - val_loss: 0.6833 - val_acc: 0.5692 - val_f1: 0.5662\n",
      "Epoch 2/20\n",
      "10332/10332 [==============================] - 1s 105us/step - loss: 0.6974 - acc: 0.5515 - f1: 0.5592 - val_loss: 0.6762 - val_acc: 0.5683 - val_f1: 0.5683\n",
      "Epoch 3/20\n",
      "10332/10332 [==============================] - 1s 82us/step - loss: 0.6870 - acc: 0.5631 - f1: 0.5712 - val_loss: 0.6689 - val_acc: 0.5683 - val_f1: 0.5683\n",
      "Epoch 4/20\n",
      "10332/10332 [==============================] - 1s 83us/step - loss: 0.6775 - acc: 0.5712 - f1: 0.5801 - val_loss: 0.6623 - val_acc: 0.5683 - val_f1: 0.5679\n",
      "Epoch 5/20\n",
      "10332/10332 [==============================] - 1s 82us/step - loss: 0.6626 - acc: 0.5769 - f1: 0.5858 - val_loss: 0.6423 - val_acc: 0.5709 - val_f1: 0.5732\n",
      "Epoch 6/20\n",
      "10332/10332 [==============================] - 1s 81us/step - loss: 0.6521 - acc: 0.5883 - f1: 0.5973 - val_loss: 0.6260 - val_acc: 0.5770 - val_f1: 0.5817\n",
      "Epoch 7/20\n",
      "10332/10332 [==============================] - 1s 82us/step - loss: 0.6404 - acc: 0.5981 - f1: 0.6061 - val_loss: 0.6242 - val_acc: 0.5909 - val_f1: 0.5913\n",
      "Epoch 8/20\n",
      "10332/10332 [==============================] - 1s 87us/step - loss: 0.6246 - acc: 0.6333 - f1: 0.6332 - val_loss: 0.6087 - val_acc: 0.6728 - val_f1: 0.6492\n",
      "Epoch 9/20\n",
      "10332/10332 [==============================] - 1s 100us/step - loss: 0.6015 - acc: 0.6635 - f1: 0.6364 - val_loss: 0.6020 - val_acc: 0.7002 - val_f1: 0.6805\n",
      "Epoch 10/20\n",
      "10332/10332 [==============================] - 1s 90us/step - loss: 0.5870 - acc: 0.6932 - f1: 0.6724 - val_loss: 0.5864 - val_acc: 0.6941 - val_f1: 0.6843\n",
      "Epoch 11/20\n",
      "10332/10332 [==============================] - 1s 82us/step - loss: 0.5665 - acc: 0.7050 - f1: 0.6867 - val_loss: 0.5803 - val_acc: 0.7180 - val_f1: 0.7099\n",
      "Epoch 12/20\n",
      "10332/10332 [==============================] - 1s 82us/step - loss: 0.5574 - acc: 0.7198 - f1: 0.7053 - val_loss: 0.5685 - val_acc: 0.7124 - val_f1: 0.7039\n",
      "Epoch 13/20\n",
      "10332/10332 [==============================] - 1s 82us/step - loss: 0.5304 - acc: 0.7431 - f1: 0.7366 - val_loss: 0.5736 - val_acc: 0.7167 - val_f1: 0.7129\n",
      "Epoch 14/20\n",
      "10332/10332 [==============================] - 1s 81us/step - loss: 0.5092 - acc: 0.7609 - f1: 0.7575 - val_loss: 0.5660 - val_acc: 0.7128 - val_f1: 0.7091\n",
      "Epoch 15/20\n",
      "10332/10332 [==============================] - 1s 82us/step - loss: 0.5014 - acc: 0.7738 - f1: 0.7706 - val_loss: 0.5483 - val_acc: 0.7285 - val_f1: 0.7249\n",
      "Epoch 16/20\n",
      "10332/10332 [==============================] - 1s 82us/step - loss: 0.4830 - acc: 0.7876 - f1: 0.7850 - val_loss: 0.5765 - val_acc: 0.7215 - val_f1: 0.7170\n",
      "Epoch 17/20\n",
      "10332/10332 [==============================] - 1s 82us/step - loss: 0.4621 - acc: 0.7982 - f1: 0.7958 - val_loss: 0.5511 - val_acc: 0.7245 - val_f1: 0.7216\n",
      "Epoch 18/20\n",
      "10332/10332 [==============================] - 1s 82us/step - loss: 0.4534 - acc: 0.8081 - f1: 0.8064 - val_loss: 0.5503 - val_acc: 0.7276 - val_f1: 0.7257\n",
      "Epoch 19/20\n",
      "10332/10332 [==============================] - 1s 82us/step - loss: 0.4440 - acc: 0.8162 - f1: 0.8148 - val_loss: 0.5474 - val_acc: 0.7393 - val_f1: 0.7368\n",
      "Epoch 20/20\n",
      "10332/10332 [==============================] - 1s 82us/step - loss: 0.4316 - acc: 0.8266 - f1: 0.8253 - val_loss: 0.5585 - val_acc: 0.7450 - val_f1: 0.7432\n",
      "1149/1149 [==============================] - 10s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "## Run the model\n",
    "results = list()\n",
    "sample_size = list()\n",
    "fprs = list()\n",
    "tprs = list()\n",
    "aucs = list()\n",
    "for k in range(0, 24):\n",
    "    fx_n = '/mnt/data0/tao/ECG/hdf5/ECG_H' + str(k) + '_Dataset.h5'\n",
    "    fy_n = '/mnt/data0/tao/ECG/hdf5/ECG_H' + str(k) + '_Label.h5'\n",
    "    x_n = 'ECG_H' + str(k)    \n",
    "    with h5py.File(fx_n, 'r') as hf:\n",
    "        X_H = hf[\"x_n\"][:]\n",
    "    with h5py.File(fy_n, 'r') as hf:\n",
    "        Y_H = hf[\"x_n\"][:]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_H, Y_H, test_size=0.1)\n",
    "    ## Print train & test\n",
    "    print(k)\n",
    "    print(np.shape(x_train[0]))\n",
    "    print(np.shape(y_train))\n",
    "    print(np.shape(x_test))\n",
    "    print(np.shape(y_test))\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=10001, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "### Compilation\n",
    "# For a binary classification problem\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', f1])\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes=2)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes=2)\n",
    "    history = model.fit(x_train, y_train, epochs=20, batch_size=128, validation_data=(x_test, y_test))\n",
    "    preds = model.predict(x_test, verbose=1)\n",
    "    gt = np.argmax(y_test, axis=1)\n",
    "    binary_gt = gt == 0\n",
    "    binary_probs = preds[..., 0]\n",
    "    fpr, tpr, _ = skm.roc_curve(binary_gt.ravel(), binary_probs.ravel())\n",
    "    auc = skm.auc(fpr, tpr)\n",
    "    results.append([k, max(history.history['acc']).astype(str), max(history.history['f1']).astype(str), \n",
    "                    max(history.history['val_acc']).astype(str), max(history.history['val_f1']).astype(str)])\n",
    "    sample_size.append([len(x_train), len(x_test)])\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "    aucs.append(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hour</th>\n",
       "      <th>Training_Acc</th>\n",
       "      <th>Training_F1</th>\n",
       "      <th>Testing_Acc</th>\n",
       "      <th>Testing_F1</th>\n",
       "      <th>Training_Sample</th>\n",
       "      <th>Testing_Sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.6237812130544755</td>\n",
       "      <td>0.6262034862135026</td>\n",
       "      <td>0.616577541254421</td>\n",
       "      <td>0.6147016297687183</td>\n",
       "      <td>8410</td>\n",
       "      <td>935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.715049571112932</td>\n",
       "      <td>0.7159719670736485</td>\n",
       "      <td>0.6903807615230461</td>\n",
       "      <td>0.694121945716575</td>\n",
       "      <td>8977</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.657981562278966</td>\n",
       "      <td>0.6574413338140167</td>\n",
       "      <td>0.5921397397091295</td>\n",
       "      <td>0.5917629279424009</td>\n",
       "      <td>10305</td>\n",
       "      <td>1145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.7537721613360908</td>\n",
       "      <td>0.7526012328560512</td>\n",
       "      <td>0.6738761656864707</td>\n",
       "      <td>0.6731303084998741</td>\n",
       "      <td>10604</td>\n",
       "      <td>1179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.7606801175423008</td>\n",
       "      <td>0.760896008032972</td>\n",
       "      <td>0.7061328795537023</td>\n",
       "      <td>0.7071572448772709</td>\n",
       "      <td>10557</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.6550590745178773</td>\n",
       "      <td>0.6547077368480587</td>\n",
       "      <td>0.6558752980925958</td>\n",
       "      <td>0.654651168915484</td>\n",
       "      <td>11257</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.7238263171334505</td>\n",
       "      <td>0.7234123975189609</td>\n",
       "      <td>0.6243805760080207</td>\n",
       "      <td>0.6247544152767381</td>\n",
       "      <td>9074</td>\n",
       "      <td>1009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.5518204025353084</td>\n",
       "      <td>0.547626847158383</td>\n",
       "      <td>0.5428870285405275</td>\n",
       "      <td>0.5433048179458874</td>\n",
       "      <td>8597</td>\n",
       "      <td>956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.5476323120219905</td>\n",
       "      <td>0.5463962823567616</td>\n",
       "      <td>0.5430242272347535</td>\n",
       "      <td>0.5370567169105798</td>\n",
       "      <td>10770</td>\n",
       "      <td>1197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.6417988412615037</td>\n",
       "      <td>0.6418362098186713</td>\n",
       "      <td>0.6384615383596501</td>\n",
       "      <td>0.6384614787550054</td>\n",
       "      <td>10529</td>\n",
       "      <td>1170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.6668892665763918</td>\n",
       "      <td>0.6664926553305172</td>\n",
       "      <td>0.6069868995633187</td>\n",
       "      <td>0.5879612289141359</td>\n",
       "      <td>8236</td>\n",
       "      <td>916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.6700561797752809</td>\n",
       "      <td>0.6701477394211158</td>\n",
       "      <td>0.6289180980654409</td>\n",
       "      <td>0.6289180384607961</td>\n",
       "      <td>8900</td>\n",
       "      <td>989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.7282692017903197</td>\n",
       "      <td>0.7269153104189549</td>\n",
       "      <td>0.7013793127290133</td>\n",
       "      <td>0.7007852863443309</td>\n",
       "      <td>6523</td>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.6762268120811857</td>\n",
       "      <td>0.6766642140191004</td>\n",
       "      <td>0.691124259367497</td>\n",
       "      <td>0.6911241997628522</td>\n",
       "      <td>7601</td>\n",
       "      <td>845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.6914585593353927</td>\n",
       "      <td>0.6914877581688417</td>\n",
       "      <td>0.6400651469033911</td>\n",
       "      <td>0.639862831265349</td>\n",
       "      <td>8289</td>\n",
       "      <td>921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.6077291008920459</td>\n",
       "      <td>0.6083322320506422</td>\n",
       "      <td>0.5874877840076607</td>\n",
       "      <td>0.5934945286077721</td>\n",
       "      <td>9199</td>\n",
       "      <td>1023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.6717795086592668</td>\n",
       "      <td>0.6717915984740008</td>\n",
       "      <td>0.5775862068965517</td>\n",
       "      <td>0.5771213050546318</td>\n",
       "      <td>8345</td>\n",
       "      <td>928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.6336286451612083</td>\n",
       "      <td>0.6336396099369039</td>\n",
       "      <td>0.5719026548672567</td>\n",
       "      <td>0.623729364534395</td>\n",
       "      <td>8127</td>\n",
       "      <td>904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.5465954607414786</td>\n",
       "      <td>0.5521336324861117</td>\n",
       "      <td>0.525210084248276</td>\n",
       "      <td>0.525589579723033</td>\n",
       "      <td>7490</td>\n",
       "      <td>833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.7398878222221105</td>\n",
       "      <td>0.7398089759494607</td>\n",
       "      <td>0.7235693502611147</td>\n",
       "      <td>0.7231541801493336</td>\n",
       "      <td>9271</td>\n",
       "      <td>1031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.7512418153754851</td>\n",
       "      <td>0.7513941498689873</td>\n",
       "      <td>0.7274111693885725</td>\n",
       "      <td>0.7292030234022189</td>\n",
       "      <td>8858</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.8679595494111623</td>\n",
       "      <td>0.8684809782599036</td>\n",
       "      <td>0.7759216620076087</td>\n",
       "      <td>0.776976333785167</td>\n",
       "      <td>7812</td>\n",
       "      <td>868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.7980365451328493</td>\n",
       "      <td>0.7976882072613142</td>\n",
       "      <td>0.7093973456291183</td>\n",
       "      <td>0.707999492142125</td>\n",
       "      <td>8811</td>\n",
       "      <td>979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.8265582654672771</td>\n",
       "      <td>0.8252567464743203</td>\n",
       "      <td>0.744995645899893</td>\n",
       "      <td>0.7432273452545478</td>\n",
       "      <td>10332</td>\n",
       "      <td>1149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Hour        Training_Acc         Training_F1         Testing_Acc  \\\n",
       "0      0  0.6237812130544755  0.6262034862135026   0.616577541254421   \n",
       "1      1   0.715049571112932  0.7159719670736485  0.6903807615230461   \n",
       "2      2   0.657981562278966  0.6574413338140167  0.5921397397091295   \n",
       "3      3  0.7537721613360908  0.7526012328560512  0.6738761656864707   \n",
       "4      4  0.7606801175423008   0.760896008032972  0.7061328795537023   \n",
       "5      5  0.6550590745178773  0.6547077368480587  0.6558752980925958   \n",
       "6      6  0.7238263171334505  0.7234123975189609  0.6243805760080207   \n",
       "7      7  0.5518204025353084   0.547626847158383  0.5428870285405275   \n",
       "8      8  0.5476323120219905  0.5463962823567616  0.5430242272347535   \n",
       "9      9  0.6417988412615037  0.6418362098186713  0.6384615383596501   \n",
       "10    10  0.6668892665763918  0.6664926553305172  0.6069868995633187   \n",
       "11    11  0.6700561797752809  0.6701477394211158  0.6289180980654409   \n",
       "12    12  0.7282692017903197  0.7269153104189549  0.7013793127290133   \n",
       "13    13  0.6762268120811857  0.6766642140191004   0.691124259367497   \n",
       "14    14  0.6914585593353927  0.6914877581688417  0.6400651469033911   \n",
       "15    15  0.6077291008920459  0.6083322320506422  0.5874877840076607   \n",
       "16    16  0.6717795086592668  0.6717915984740008  0.5775862068965517   \n",
       "17    17  0.6336286451612083  0.6336396099369039  0.5719026548672567   \n",
       "18    18  0.5465954607414786  0.5521336324861117   0.525210084248276   \n",
       "19    19  0.7398878222221105  0.7398089759494607  0.7235693502611147   \n",
       "20    20  0.7512418153754851  0.7513941498689873  0.7274111693885725   \n",
       "21    21  0.8679595494111623  0.8684809782599036  0.7759216620076087   \n",
       "22    22  0.7980365451328493  0.7976882072613142  0.7093973456291183   \n",
       "23    23  0.8265582654672771  0.8252567464743203   0.744995645899893   \n",
       "\n",
       "            Testing_F1  Training_Sample  Testing_Sample  \n",
       "0   0.6147016297687183             8410             935  \n",
       "1    0.694121945716575             8977             998  \n",
       "2   0.5917629279424009            10305            1145  \n",
       "3   0.6731303084998741            10604            1179  \n",
       "4   0.7071572448772709            10557            1174  \n",
       "5    0.654651168915484            11257            1251  \n",
       "6   0.6247544152767381             9074            1009  \n",
       "7   0.5433048179458874             8597             956  \n",
       "8   0.5370567169105798            10770            1197  \n",
       "9   0.6384614787550054            10529            1170  \n",
       "10  0.5879612289141359             8236             916  \n",
       "11  0.6289180384607961             8900             989  \n",
       "12  0.7007852863443309             6523             725  \n",
       "13  0.6911241997628522             7601             845  \n",
       "14   0.639862831265349             8289             921  \n",
       "15  0.5934945286077721             9199            1023  \n",
       "16  0.5771213050546318             8345             928  \n",
       "17   0.623729364534395             8127             904  \n",
       "18   0.525589579723033             7490             833  \n",
       "19  0.7231541801493336             9271            1031  \n",
       "20  0.7292030234022189             8858             985  \n",
       "21   0.776976333785167             7812             868  \n",
       "22   0.707999492142125             8811             979  \n",
       "23  0.7432273452545478            10332            1149  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(results)\n",
    "results.columns = ['Hour', 'Training_Acc', 'Training_F1', 'Testing_Acc', 'Testing_F1']\n",
    "sample_size = pd.DataFrame(sample_size)\n",
    "sample_size.columns = ['Training_Sample', 'Testing_Sample']\n",
    "Summary_T2 = pd.concat([results, sample_size], axis=1)\n",
    "Summary_T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "fprs = np.asarray(fprs)\n",
    "tprs = np.asarray(tprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"/mnt/data0/tao/ECG/results/ECG_Classification_Hourly_fprs.txt\", fprs, fmt='%s')\n",
    "np.savetxt(\"/mnt/data0/tao/ECG/results/ECG_Classification_Hourly_tprs.txt\", tprs, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = h5py.special_dtype(vlen=np.dtype('float64'))\n",
    "with h5py.File(\"/mnt/data0/tao/ECG/results/ECG_Classification_Hourly_fprs.h5\", 'w') as hf:\n",
    "    hf.create_dataset(\"fprs\", data=fprs, dtype=dt)\n",
    "with h5py.File(\"/mnt/data0/tao/ECG/results/ECG_Classification_Hourly_tprs.h5\", 'w') as hf:\n",
    "    hf.create_dataset(\"tprs\", data=tprs, dtype=dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary_T2['AUC'] = aucs\n",
    "Summary_T2.to_csv('/mnt/data0/tao/ECG/results/Summarized_hourly_stats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hour</th>\n",
       "      <th>Training_Acc</th>\n",
       "      <th>Training_F1</th>\n",
       "      <th>Testing_Acc</th>\n",
       "      <th>Testing_F1</th>\n",
       "      <th>Training_Sample</th>\n",
       "      <th>Testing_Sample</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.6237812130544755</td>\n",
       "      <td>0.6262034862135026</td>\n",
       "      <td>0.616577541254421</td>\n",
       "      <td>0.6147016297687183</td>\n",
       "      <td>8410</td>\n",
       "      <td>935</td>\n",
       "      <td>0.619762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.715049571112932</td>\n",
       "      <td>0.7159719670736485</td>\n",
       "      <td>0.6903807615230461</td>\n",
       "      <td>0.694121945716575</td>\n",
       "      <td>8977</td>\n",
       "      <td>998</td>\n",
       "      <td>0.675822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.657981562278966</td>\n",
       "      <td>0.6574413338140167</td>\n",
       "      <td>0.5921397397091295</td>\n",
       "      <td>0.5917629279424009</td>\n",
       "      <td>10305</td>\n",
       "      <td>1145</td>\n",
       "      <td>0.610477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.7537721613360908</td>\n",
       "      <td>0.7526012328560512</td>\n",
       "      <td>0.6738761656864707</td>\n",
       "      <td>0.6731303084998741</td>\n",
       "      <td>10604</td>\n",
       "      <td>1179</td>\n",
       "      <td>0.691539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.7606801175423008</td>\n",
       "      <td>0.760896008032972</td>\n",
       "      <td>0.7061328795537023</td>\n",
       "      <td>0.7071572448772709</td>\n",
       "      <td>10557</td>\n",
       "      <td>1174</td>\n",
       "      <td>0.762644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.6550590745178773</td>\n",
       "      <td>0.6547077368480587</td>\n",
       "      <td>0.6558752980925958</td>\n",
       "      <td>0.654651168915484</td>\n",
       "      <td>11257</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.687353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.7238263171334505</td>\n",
       "      <td>0.7234123975189609</td>\n",
       "      <td>0.6243805760080207</td>\n",
       "      <td>0.6247544152767381</td>\n",
       "      <td>9074</td>\n",
       "      <td>1009</td>\n",
       "      <td>0.655783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.5518204025353084</td>\n",
       "      <td>0.547626847158383</td>\n",
       "      <td>0.5428870285405275</td>\n",
       "      <td>0.5433048179458874</td>\n",
       "      <td>8597</td>\n",
       "      <td>956</td>\n",
       "      <td>0.523834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.5476323120219905</td>\n",
       "      <td>0.5463962823567616</td>\n",
       "      <td>0.5430242272347535</td>\n",
       "      <td>0.5370567169105798</td>\n",
       "      <td>10770</td>\n",
       "      <td>1197</td>\n",
       "      <td>0.560332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.6417988412615037</td>\n",
       "      <td>0.6418362098186713</td>\n",
       "      <td>0.6384615383596501</td>\n",
       "      <td>0.6384614787550054</td>\n",
       "      <td>10529</td>\n",
       "      <td>1170</td>\n",
       "      <td>0.608563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.6668892665763918</td>\n",
       "      <td>0.6664926553305172</td>\n",
       "      <td>0.6069868995633187</td>\n",
       "      <td>0.5879612289141359</td>\n",
       "      <td>8236</td>\n",
       "      <td>916</td>\n",
       "      <td>0.735967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.6700561797752809</td>\n",
       "      <td>0.6701477394211158</td>\n",
       "      <td>0.6289180980654409</td>\n",
       "      <td>0.6289180384607961</td>\n",
       "      <td>8900</td>\n",
       "      <td>989</td>\n",
       "      <td>0.654971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.7282692017903197</td>\n",
       "      <td>0.7269153104189549</td>\n",
       "      <td>0.7013793127290133</td>\n",
       "      <td>0.7007852863443309</td>\n",
       "      <td>6523</td>\n",
       "      <td>725</td>\n",
       "      <td>0.588193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.6762268120811857</td>\n",
       "      <td>0.6766642140191004</td>\n",
       "      <td>0.691124259367497</td>\n",
       "      <td>0.6911241997628522</td>\n",
       "      <td>7601</td>\n",
       "      <td>845</td>\n",
       "      <td>0.595989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.6914585593353927</td>\n",
       "      <td>0.6914877581688417</td>\n",
       "      <td>0.6400651469033911</td>\n",
       "      <td>0.639862831265349</td>\n",
       "      <td>8289</td>\n",
       "      <td>921</td>\n",
       "      <td>0.661392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.6077291008920459</td>\n",
       "      <td>0.6083322320506422</td>\n",
       "      <td>0.5874877840076607</td>\n",
       "      <td>0.5934945286077721</td>\n",
       "      <td>9199</td>\n",
       "      <td>1023</td>\n",
       "      <td>0.605322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.6717795086592668</td>\n",
       "      <td>0.6717915984740008</td>\n",
       "      <td>0.5775862068965517</td>\n",
       "      <td>0.5771213050546318</td>\n",
       "      <td>8345</td>\n",
       "      <td>928</td>\n",
       "      <td>0.664982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.6336286451612083</td>\n",
       "      <td>0.6336396099369039</td>\n",
       "      <td>0.5719026548672567</td>\n",
       "      <td>0.623729364534395</td>\n",
       "      <td>8127</td>\n",
       "      <td>904</td>\n",
       "      <td>0.658379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.5465954607414786</td>\n",
       "      <td>0.5521336324861117</td>\n",
       "      <td>0.525210084248276</td>\n",
       "      <td>0.525589579723033</td>\n",
       "      <td>7490</td>\n",
       "      <td>833</td>\n",
       "      <td>0.518464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.7398878222221105</td>\n",
       "      <td>0.7398089759494607</td>\n",
       "      <td>0.7235693502611147</td>\n",
       "      <td>0.7231541801493336</td>\n",
       "      <td>9271</td>\n",
       "      <td>1031</td>\n",
       "      <td>0.763274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.7512418153754851</td>\n",
       "      <td>0.7513941498689873</td>\n",
       "      <td>0.7274111693885725</td>\n",
       "      <td>0.7292030234022189</td>\n",
       "      <td>8858</td>\n",
       "      <td>985</td>\n",
       "      <td>0.791000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.8679595494111623</td>\n",
       "      <td>0.8684809782599036</td>\n",
       "      <td>0.7759216620076087</td>\n",
       "      <td>0.776976333785167</td>\n",
       "      <td>7812</td>\n",
       "      <td>868</td>\n",
       "      <td>0.859911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.7980365451328493</td>\n",
       "      <td>0.7976882072613142</td>\n",
       "      <td>0.7093973456291183</td>\n",
       "      <td>0.707999492142125</td>\n",
       "      <td>8811</td>\n",
       "      <td>979</td>\n",
       "      <td>0.751777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.8265582654672771</td>\n",
       "      <td>0.8252567464743203</td>\n",
       "      <td>0.744995645899893</td>\n",
       "      <td>0.7432273452545478</td>\n",
       "      <td>10332</td>\n",
       "      <td>1149</td>\n",
       "      <td>0.809795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Hour        Training_Acc         Training_F1         Testing_Acc  \\\n",
       "0      0  0.6237812130544755  0.6262034862135026   0.616577541254421   \n",
       "1      1   0.715049571112932  0.7159719670736485  0.6903807615230461   \n",
       "2      2   0.657981562278966  0.6574413338140167  0.5921397397091295   \n",
       "3      3  0.7537721613360908  0.7526012328560512  0.6738761656864707   \n",
       "4      4  0.7606801175423008   0.760896008032972  0.7061328795537023   \n",
       "5      5  0.6550590745178773  0.6547077368480587  0.6558752980925958   \n",
       "6      6  0.7238263171334505  0.7234123975189609  0.6243805760080207   \n",
       "7      7  0.5518204025353084   0.547626847158383  0.5428870285405275   \n",
       "8      8  0.5476323120219905  0.5463962823567616  0.5430242272347535   \n",
       "9      9  0.6417988412615037  0.6418362098186713  0.6384615383596501   \n",
       "10    10  0.6668892665763918  0.6664926553305172  0.6069868995633187   \n",
       "11    11  0.6700561797752809  0.6701477394211158  0.6289180980654409   \n",
       "12    12  0.7282692017903197  0.7269153104189549  0.7013793127290133   \n",
       "13    13  0.6762268120811857  0.6766642140191004   0.691124259367497   \n",
       "14    14  0.6914585593353927  0.6914877581688417  0.6400651469033911   \n",
       "15    15  0.6077291008920459  0.6083322320506422  0.5874877840076607   \n",
       "16    16  0.6717795086592668  0.6717915984740008  0.5775862068965517   \n",
       "17    17  0.6336286451612083  0.6336396099369039  0.5719026548672567   \n",
       "18    18  0.5465954607414786  0.5521336324861117   0.525210084248276   \n",
       "19    19  0.7398878222221105  0.7398089759494607  0.7235693502611147   \n",
       "20    20  0.7512418153754851  0.7513941498689873  0.7274111693885725   \n",
       "21    21  0.8679595494111623  0.8684809782599036  0.7759216620076087   \n",
       "22    22  0.7980365451328493  0.7976882072613142  0.7093973456291183   \n",
       "23    23  0.8265582654672771  0.8252567464743203   0.744995645899893   \n",
       "\n",
       "            Testing_F1  Training_Sample  Testing_Sample       AUC  \n",
       "0   0.6147016297687183             8410             935  0.619762  \n",
       "1    0.694121945716575             8977             998  0.675822  \n",
       "2   0.5917629279424009            10305            1145  0.610477  \n",
       "3   0.6731303084998741            10604            1179  0.691539  \n",
       "4   0.7071572448772709            10557            1174  0.762644  \n",
       "5    0.654651168915484            11257            1251  0.687353  \n",
       "6   0.6247544152767381             9074            1009  0.655783  \n",
       "7   0.5433048179458874             8597             956  0.523834  \n",
       "8   0.5370567169105798            10770            1197  0.560332  \n",
       "9   0.6384614787550054            10529            1170  0.608563  \n",
       "10  0.5879612289141359             8236             916  0.735967  \n",
       "11  0.6289180384607961             8900             989  0.654971  \n",
       "12  0.7007852863443309             6523             725  0.588193  \n",
       "13  0.6911241997628522             7601             845  0.595989  \n",
       "14   0.639862831265349             8289             921  0.661392  \n",
       "15  0.5934945286077721             9199            1023  0.605322  \n",
       "16  0.5771213050546318             8345             928  0.664982  \n",
       "17   0.623729364534395             8127             904  0.658379  \n",
       "18   0.525589579723033             7490             833  0.518464  \n",
       "19  0.7231541801493336             9271            1031  0.763274  \n",
       "20  0.7292030234022189             8858             985  0.791000  \n",
       "21   0.776976333785167             7812             868  0.859911  \n",
       "22   0.707999492142125             8811             979  0.751777  \n",
       "23  0.7432273452545478            10332            1149  0.809795  "
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Summary_T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(9345, 10001)\n",
      "(9345,)\n",
      "Baseline # : 3692\n",
      "Peak # : 5653\n",
      "x_train :  8410\n",
      "x_test  :  935\n",
      "y_train :  Counter({1: 5095, 0: 3315})\n",
      "y_test  :  Counter({1: 558, 0: 377})\n",
      "(10001,)\n",
      "(8410,)\n",
      "(935, 10001)\n",
      "(935,)\n",
      "Train on 8410 samples, validate on 935 samples\n",
      "Epoch 1/25\n",
      "8410/8410 [==============================] - 1s 144us/step - loss: 0.7374 - acc: 0.5461 - f1: 0.5609 - val_loss: 0.6827 - val_acc: 0.5952 - val_f1: 0.5958\n",
      "Epoch 2/25\n",
      "8410/8410 [==============================] - 1s 76us/step - loss: 0.7048 - acc: 0.5685 - f1: 0.5687 - val_loss: 0.6822 - val_acc: 0.5957 - val_f1: 0.5949\n",
      "Epoch 3/25\n",
      "8410/8410 [==============================] - 1s 73us/step - loss: 0.6967 - acc: 0.5883 - f1: 0.5828 - val_loss: 0.6802 - val_acc: 0.5968 - val_f1: 0.5968\n",
      "Epoch 4/25\n",
      "8410/8410 [==============================] - 1s 70us/step - loss: 0.6944 - acc: 0.5905 - f1: 0.5879 - val_loss: 0.6795 - val_acc: 0.5968 - val_f1: 0.5968\n",
      "Epoch 5/25\n",
      "8410/8410 [==============================] - 1s 73us/step - loss: 0.6866 - acc: 0.5963 - f1: 0.5927 - val_loss: 0.6785 - val_acc: 0.5968 - val_f1: 0.5968\n",
      "Epoch 6/25\n",
      "8410/8410 [==============================] - 1s 71us/step - loss: 0.6856 - acc: 0.5998 - f1: 0.5980 - val_loss: 0.6778 - val_acc: 0.5968 - val_f1: 0.5968\n",
      "Epoch 7/25\n",
      "8410/8410 [==============================] - 1s 80us/step - loss: 0.6783 - acc: 0.6016 - f1: 0.5996 - val_loss: 0.6764 - val_acc: 0.5968 - val_f1: 0.5968\n",
      "Epoch 8/25\n",
      "8410/8410 [==============================] - 1s 126us/step - loss: 0.6774 - acc: 0.6017 - f1: 0.5998 - val_loss: 0.6751 - val_acc: 0.5968 - val_f1: 0.5968\n",
      "Epoch 9/25\n",
      "8410/8410 [==============================] - 1s 109us/step - loss: 0.6772 - acc: 0.6042 - f1: 0.6033 - val_loss: 0.6743 - val_acc: 0.5968 - val_f1: 0.5968\n",
      "Epoch 10/25\n",
      "8410/8410 [==============================] - 1s 74us/step - loss: 0.6739 - acc: 0.6039 - f1: 0.6027 - val_loss: 0.6741 - val_acc: 0.5968 - val_f1: 0.5968\n",
      "Epoch 11/25\n",
      "8410/8410 [==============================] - 1s 76us/step - loss: 0.6700 - acc: 0.6039 - f1: 0.6032 - val_loss: 0.6737 - val_acc: 0.5968 - val_f1: 0.5968\n",
      "Epoch 12/25\n",
      "8410/8410 [==============================] - 1s 80us/step - loss: 0.6767 - acc: 0.6048 - f1: 0.6044 - val_loss: 0.6739 - val_acc: 0.5968 - val_f1: 0.5968\n",
      "Epoch 13/25\n",
      "8410/8410 [==============================] - 1s 90us/step - loss: 0.6684 - acc: 0.6039 - f1: 0.6028 - val_loss: 0.6712 - val_acc: 0.5968 - val_f1: 0.5968\n",
      "Epoch 14/25\n",
      "8410/8410 [==============================] - 1s 73us/step - loss: 0.6651 - acc: 0.6047 - f1: 0.6039 - val_loss: 0.6714 - val_acc: 0.5968 - val_f1: 0.5968\n",
      "Epoch 15/25\n",
      "8410/8410 [==============================] - 1s 73us/step - loss: 0.6631 - acc: 0.6058 - f1: 0.6054 - val_loss: 0.6691 - val_acc: 0.5968 - val_f1: 0.5968\n",
      "Epoch 16/25\n",
      "8410/8410 [==============================] - 1s 71us/step - loss: 0.6607 - acc: 0.6046 - f1: 0.6041 - val_loss: 0.6678 - val_acc: 0.5968 - val_f1: 0.5968\n",
      "Epoch 17/25\n",
      "8410/8410 [==============================] - 1s 73us/step - loss: 0.6582 - acc: 0.6056 - f1: 0.6054 - val_loss: 0.6673 - val_acc: 0.5968 - val_f1: 0.5968\n",
      "Epoch 18/25\n",
      "8410/8410 [==============================] - 1s 69us/step - loss: 0.6502 - acc: 0.6055 - f1: 0.6052 - val_loss: 0.6642 - val_acc: 0.5968 - val_f1: 0.5968\n",
      "Epoch 19/25\n",
      "8410/8410 [==============================] - 1s 68us/step - loss: 0.6452 - acc: 0.6058 - f1: 0.6057 - val_loss: 0.6630 - val_acc: 0.5968 - val_f1: 0.5968\n",
      "Epoch 20/25\n",
      "8410/8410 [==============================] - 1s 68us/step - loss: 0.6359 - acc: 0.6058 - f1: 0.6056 - val_loss: 0.6613 - val_acc: 0.5968 - val_f1: 0.5968\n",
      "Epoch 21/25\n",
      "8410/8410 [==============================] - 1s 67us/step - loss: 0.6272 - acc: 0.6054 - f1: 0.6052 - val_loss: 0.6579 - val_acc: 0.5968 - val_f1: 0.5968\n",
      "Epoch 22/25\n",
      "8410/8410 [==============================] - 1s 73us/step - loss: 0.6175 - acc: 0.6060 - f1: 0.6059 - val_loss: 0.6571 - val_acc: 0.5968 - val_f1: 0.5968\n",
      "Epoch 23/25\n",
      "8410/8410 [==============================] - 1s 85us/step - loss: 0.6058 - acc: 0.6059 - f1: 0.6054 - val_loss: 0.6591 - val_acc: 0.5968 - val_f1: 0.5968\n",
      "Epoch 24/25\n",
      "8410/8410 [==============================] - 1s 99us/step - loss: 0.5920 - acc: 0.6120 - f1: 0.6018 - val_loss: 0.6612 - val_acc: 0.5888 - val_f1: 0.4580\n",
      "Epoch 25/25\n",
      "8410/8410 [==============================] - 1s 69us/step - loss: 0.5943 - acc: 0.6644 - f1: 0.6373 - val_loss: 0.6600 - val_acc: 0.5759 - val_f1: 0.5726\n",
      "1\n",
      "(9975, 10001)\n",
      "(9975,)\n",
      "Baseline # : 3702\n",
      "Peak # : 6273\n",
      "x_train :  8977\n",
      "x_test  :  998\n",
      "y_train :  Counter({1: 5648, 0: 3329})\n",
      "y_test  :  Counter({1: 625, 0: 373})\n",
      "(10001,)\n",
      "(8977,)\n",
      "(998, 10001)\n",
      "(998,)\n",
      "Train on 8977 samples, validate on 998 samples\n",
      "Epoch 1/25\n",
      "8977/8977 [==============================] - 1s 166us/step - loss: 0.7432 - acc: 0.5492 - f1: 0.5870 - val_loss: 0.6767 - val_acc: 0.6263 - val_f1: 0.6259\n",
      "Epoch 2/25\n",
      "8977/8977 [==============================] - 1s 76us/step - loss: 0.7044 - acc: 0.5952 - f1: 0.6097 - val_loss: 0.6715 - val_acc: 0.6273 - val_f1: 0.6276\n",
      "Epoch 3/25\n",
      "8977/8977 [==============================] - 1s 74us/step - loss: 0.6892 - acc: 0.6143 - f1: 0.6217 - val_loss: 0.6684 - val_acc: 0.6263 - val_f1: 0.6263\n",
      "Epoch 4/25\n",
      "8977/8977 [==============================] - 1s 70us/step - loss: 0.6810 - acc: 0.6186 - f1: 0.6224 - val_loss: 0.6665 - val_acc: 0.6263 - val_f1: 0.6263\n",
      "Epoch 5/25\n",
      "8977/8977 [==============================] - 1s 71us/step - loss: 0.6740 - acc: 0.6228 - f1: 0.6241 - val_loss: 0.6647 - val_acc: 0.6263 - val_f1: 0.6263\n",
      "Epoch 6/25\n",
      "8977/8977 [==============================] - 1s 78us/step - loss: 0.6692 - acc: 0.6252 - f1: 0.6260 - val_loss: 0.6608 - val_acc: 0.6263 - val_f1: 0.6263\n",
      "Epoch 7/25\n",
      "8977/8977 [==============================] - 1s 89us/step - loss: 0.6677 - acc: 0.6266 - f1: 0.6270 - val_loss: 0.6581 - val_acc: 0.6263 - val_f1: 0.6263\n",
      "Epoch 8/25\n",
      "8977/8977 [==============================] - 1s 68us/step - loss: 0.6645 - acc: 0.6262 - f1: 0.6260 - val_loss: 0.6597 - val_acc: 0.6263 - val_f1: 0.6263\n",
      "Epoch 9/25\n",
      "8977/8977 [==============================] - 1s 68us/step - loss: 0.6633 - acc: 0.6279 - f1: 0.6282 - val_loss: 0.6592 - val_acc: 0.6263 - val_f1: 0.6263\n",
      "Epoch 10/25\n",
      "8977/8977 [==============================] - 1s 68us/step - loss: 0.6577 - acc: 0.6272 - f1: 0.6274 - val_loss: 0.6565 - val_acc: 0.6263 - val_f1: 0.6263\n",
      "Epoch 11/25\n",
      "8977/8977 [==============================] - 1s 68us/step - loss: 0.6525 - acc: 0.6297 - f1: 0.6299 - val_loss: 0.6558 - val_acc: 0.6263 - val_f1: 0.6263\n",
      "Epoch 12/25\n",
      "8977/8977 [==============================] - 1s 67us/step - loss: 0.6479 - acc: 0.6292 - f1: 0.6299 - val_loss: 0.6526 - val_acc: 0.6268 - val_f1: 0.6269\n",
      "Epoch 13/25\n",
      "8977/8977 [==============================] - 1s 67us/step - loss: 0.6413 - acc: 0.6297 - f1: 0.6298 - val_loss: 0.6482 - val_acc: 0.6263 - val_f1: 0.6263\n",
      "Epoch 14/25\n",
      "8977/8977 [==============================] - 1s 67us/step - loss: 0.6352 - acc: 0.6307 - f1: 0.6317 - val_loss: 0.6467 - val_acc: 0.6288 - val_f1: 0.6286\n",
      "Epoch 15/25\n",
      "8977/8977 [==============================] - 1s 67us/step - loss: 0.6269 - acc: 0.6340 - f1: 0.6347 - val_loss: 0.6447 - val_acc: 0.6293 - val_f1: 0.6293\n",
      "Epoch 16/25\n",
      "8977/8977 [==============================] - 1s 68us/step - loss: 0.6174 - acc: 0.6362 - f1: 0.6376 - val_loss: 0.6428 - val_acc: 0.6293 - val_f1: 0.6289\n",
      "Epoch 17/25\n",
      "8977/8977 [==============================] - 1s 69us/step - loss: 0.6017 - acc: 0.6465 - f1: 0.6474 - val_loss: 0.6381 - val_acc: 0.6338 - val_f1: 0.6328\n",
      "Epoch 18/25\n",
      "8977/8977 [==============================] - 1s 74us/step - loss: 0.5947 - acc: 0.6556 - f1: 0.6569 - val_loss: 0.6400 - val_acc: 0.6373 - val_f1: 0.6365\n",
      "Epoch 19/25\n",
      "8977/8977 [==============================] - 1s 88us/step - loss: 0.5831 - acc: 0.6668 - f1: 0.6676 - val_loss: 0.6382 - val_acc: 0.6473 - val_f1: 0.6456\n",
      "Epoch 20/25\n",
      "8977/8977 [==============================] - 1s 68us/step - loss: 0.5651 - acc: 0.6846 - f1: 0.6863 - val_loss: 0.6388 - val_acc: 0.6648 - val_f1: 0.6640\n",
      "Epoch 21/25\n",
      "8977/8977 [==============================] - 1s 66us/step - loss: 0.5470 - acc: 0.7138 - f1: 0.7136 - val_loss: 0.6350 - val_acc: 0.6784 - val_f1: 0.6743\n",
      "Epoch 22/25\n",
      "8977/8977 [==============================] - 1s 66us/step - loss: 0.5432 - acc: 0.7253 - f1: 0.7255 - val_loss: 0.6339 - val_acc: 0.6748 - val_f1: 0.6714\n",
      "Epoch 23/25\n",
      "8977/8977 [==============================] - 1s 66us/step - loss: 0.5180 - acc: 0.7393 - f1: 0.7395 - val_loss: 0.6433 - val_acc: 0.6814 - val_f1: 0.6784\n",
      "Epoch 24/25\n",
      "8977/8977 [==============================] - 1s 67us/step - loss: 0.5091 - acc: 0.7495 - f1: 0.7494 - val_loss: 0.6537 - val_acc: 0.6844 - val_f1: 0.6805\n",
      "Epoch 25/25\n",
      "8977/8977 [==============================] - 1s 67us/step - loss: 0.5020 - acc: 0.7551 - f1: 0.7545 - val_loss: 0.6515 - val_acc: 0.6884 - val_f1: 0.6849\n",
      "2\n",
      "(11450, 10001)\n",
      "(11450,)\n",
      "Baseline # : 5688\n",
      "Peak # : 5762\n",
      "x_train :  10305\n",
      "x_test  :  1145\n",
      "y_train :  Counter({1: 5198, 0: 5107})\n",
      "y_test  :  Counter({0: 581, 1: 564})\n",
      "(10001,)\n",
      "(10305,)\n",
      "(1145, 10001)\n",
      "(1145,)\n",
      "Train on 10305 samples, validate on 1145 samples\n",
      "Epoch 1/25\n",
      "10305/10305 [==============================] - 1s 141us/step - loss: 0.7537 - acc: 0.5073 - f1: 0.5025 - val_loss: 0.6912 - val_acc: 0.4913 - val_f1: 0.3873\n",
      "Epoch 2/25\n",
      "10305/10305 [==============================] - 1s 94us/step - loss: 0.7199 - acc: 0.5063 - f1: 0.4816 - val_loss: 0.6932 - val_acc: 0.4969 - val_f1: 0.2263\n",
      "Epoch 3/25\n",
      "10305/10305 [==============================] - 1s 69us/step - loss: 0.7093 - acc: 0.4941 - f1: 0.4827 - val_loss: 0.6936 - val_acc: 0.4751 - val_f1: 0.4787\n",
      "Epoch 4/25\n",
      "10305/10305 [==============================] - 1s 67us/step - loss: 0.7024 - acc: 0.5039 - f1: 0.4997 - val_loss: 0.6932 - val_acc: 0.4913 - val_f1: 0.4994\n",
      "Epoch 5/25\n",
      "10305/10305 [==============================] - 1s 69us/step - loss: 0.6996 - acc: 0.5037 - f1: 0.5141 - val_loss: 0.6923 - val_acc: 0.4961 - val_f1: 0.5052\n",
      "Epoch 6/25\n",
      "10305/10305 [==============================] - 1s 72us/step - loss: 0.6998 - acc: 0.5047 - f1: 0.5274 - val_loss: 0.6927 - val_acc: 0.4917 - val_f1: 0.5080\n",
      "Epoch 7/25\n",
      "10305/10305 [==============================] - 1s 87us/step - loss: 0.6988 - acc: 0.5118 - f1: 0.5829 - val_loss: 0.6925 - val_acc: 0.5148 - val_f1: 0.6630\n",
      "Epoch 8/25\n",
      "10305/10305 [==============================] - 1s 71us/step - loss: 0.6965 - acc: 0.5125 - f1: 0.5611 - val_loss: 0.6920 - val_acc: 0.5467 - val_f1: 0.5526\n",
      "Epoch 9/25\n",
      "10305/10305 [==============================] - 1s 66us/step - loss: 0.6959 - acc: 0.5327 - f1: 0.5473 - val_loss: 0.6904 - val_acc: 0.5603 - val_f1: 0.5639\n",
      "Epoch 10/25\n",
      "10305/10305 [==============================] - 1s 67us/step - loss: 0.6938 - acc: 0.5367 - f1: 0.5472 - val_loss: 0.6898 - val_acc: 0.5463 - val_f1: 0.5492\n",
      "Epoch 11/25\n",
      "10305/10305 [==============================] - 1s 67us/step - loss: 0.6900 - acc: 0.5504 - f1: 0.5572 - val_loss: 0.6851 - val_acc: 0.5742 - val_f1: 0.5792\n",
      "Epoch 12/25\n",
      "10305/10305 [==============================] - 1s 67us/step - loss: 0.6842 - acc: 0.5704 - f1: 0.5740 - val_loss: 0.6826 - val_acc: 0.5703 - val_f1: 0.5733\n",
      "Epoch 13/25\n",
      "10305/10305 [==============================] - 1s 66us/step - loss: 0.6798 - acc: 0.5823 - f1: 0.5846 - val_loss: 0.6809 - val_acc: 0.5646 - val_f1: 0.5667\n",
      "Epoch 14/25\n",
      "10305/10305 [==============================] - 1s 68us/step - loss: 0.6746 - acc: 0.5821 - f1: 0.5830 - val_loss: 0.6738 - val_acc: 0.5729 - val_f1: 0.5758\n",
      "Epoch 15/25\n",
      "10305/10305 [==============================] - 1s 69us/step - loss: 0.6581 - acc: 0.6161 - f1: 0.6168 - val_loss: 0.6688 - val_acc: 0.5786 - val_f1: 0.5795\n",
      "Epoch 16/25\n",
      "10305/10305 [==============================] - 1s 78us/step - loss: 0.6567 - acc: 0.6190 - f1: 0.6205 - val_loss: 0.6667 - val_acc: 0.5738 - val_f1: 0.5760\n",
      "Epoch 17/25\n",
      "10305/10305 [==============================] - 1s 81us/step - loss: 0.6407 - acc: 0.6321 - f1: 0.6339 - val_loss: 0.6642 - val_acc: 0.5769 - val_f1: 0.5796\n",
      "Epoch 18/25\n",
      "10305/10305 [==============================] - 1s 69us/step - loss: 0.6408 - acc: 0.6504 - f1: 0.6517 - val_loss: 0.6557 - val_acc: 0.5856 - val_f1: 0.5879\n",
      "Epoch 19/25\n",
      "10305/10305 [==============================] - 1s 67us/step - loss: 0.6210 - acc: 0.6604 - f1: 0.6611 - val_loss: 0.6529 - val_acc: 0.5856 - val_f1: 0.5869\n",
      "Epoch 20/25\n",
      "10305/10305 [==============================] - 1s 67us/step - loss: 0.6026 - acc: 0.6761 - f1: 0.6770 - val_loss: 0.6494 - val_acc: 0.6000 - val_f1: 0.6017\n",
      "Epoch 21/25\n",
      "10305/10305 [==============================] - 1s 66us/step - loss: 0.5895 - acc: 0.6902 - f1: 0.6911 - val_loss: 0.6469 - val_acc: 0.6017 - val_f1: 0.6035\n",
      "Epoch 22/25\n",
      "10305/10305 [==============================] - 1s 66us/step - loss: 0.5847 - acc: 0.7037 - f1: 0.7044 - val_loss: 0.6474 - val_acc: 0.5873 - val_f1: 0.5886\n",
      "Epoch 23/25\n",
      "10305/10305 [==============================] - 1s 66us/step - loss: 0.5747 - acc: 0.7128 - f1: 0.7138 - val_loss: 0.6414 - val_acc: 0.5948 - val_f1: 0.5962\n",
      "Epoch 24/25\n",
      "10305/10305 [==============================] - 1s 66us/step - loss: 0.5551 - acc: 0.7254 - f1: 0.7258 - val_loss: 0.6438 - val_acc: 0.6013 - val_f1: 0.6029\n",
      "Epoch 25/25\n",
      "10305/10305 [==============================] - 1s 66us/step - loss: 0.5422 - acc: 0.7344 - f1: 0.7349 - val_loss: 0.6413 - val_acc: 0.6074 - val_f1: 0.6079\n",
      "3\n",
      "(11783, 10001)\n",
      "(11783,)\n",
      "Baseline # : 4338\n",
      "Peak # : 7445\n",
      "x_train :  10604\n",
      "x_test  :  1179\n",
      "y_train :  Counter({1: 6646, 0: 3958})\n",
      "y_test  :  Counter({1: 799, 0: 380})\n",
      "(10001,)\n",
      "(10604,)\n",
      "(1179, 10001)\n",
      "(1179,)\n",
      "Train on 10604 samples, validate on 1179 samples\n",
      "Epoch 1/25\n",
      "10604/10604 [==============================] - 1s 141us/step - loss: 0.7245 - acc: 0.5611 - f1: 0.5550 - val_loss: 0.6661 - val_acc: 0.6777 - val_f1: 0.6777\n",
      "Epoch 2/25\n",
      "10604/10604 [==============================] - 1s 73us/step - loss: 0.6917 - acc: 0.5999 - f1: 0.5906 - val_loss: 0.6569 - val_acc: 0.6777 - val_f1: 0.6777\n",
      "Epoch 3/25\n",
      "10604/10604 [==============================] - 1s 71us/step - loss: 0.6775 - acc: 0.6116 - f1: 0.6077 - val_loss: 0.6536 - val_acc: 0.6777 - val_f1: 0.6777\n",
      "Epoch 4/25\n",
      "10604/10604 [==============================] - 1s 77us/step - loss: 0.6738 - acc: 0.6189 - f1: 0.6166 - val_loss: 0.6460 - val_acc: 0.6777 - val_f1: 0.6777\n",
      "Epoch 5/25\n",
      "10604/10604 [==============================] - 1s 92us/step - loss: 0.6676 - acc: 0.6231 - f1: 0.6218 - val_loss: 0.6440 - val_acc: 0.6777 - val_f1: 0.6777\n",
      "Epoch 6/25\n",
      "10604/10604 [==============================] - 1s 76us/step - loss: 0.6661 - acc: 0.6230 - f1: 0.6224 - val_loss: 0.6428 - val_acc: 0.6777 - val_f1: 0.6777\n",
      "Epoch 7/25\n",
      "10604/10604 [==============================] - 1s 74us/step - loss: 0.6603 - acc: 0.6244 - f1: 0.6237 - val_loss: 0.6379 - val_acc: 0.6777 - val_f1: 0.6777\n",
      "Epoch 8/25\n",
      "10604/10604 [==============================] - 1s 69us/step - loss: 0.6512 - acc: 0.6250 - f1: 0.6244 - val_loss: 0.6279 - val_acc: 0.6777 - val_f1: 0.6777\n",
      "Epoch 9/25\n",
      "10604/10604 [==============================] - 1s 68us/step - loss: 0.6459 - acc: 0.6256 - f1: 0.6250 - val_loss: 0.6239 - val_acc: 0.6777 - val_f1: 0.6777\n",
      "Epoch 10/25\n",
      "10604/10604 [==============================] - 1s 68us/step - loss: 0.6372 - acc: 0.6264 - f1: 0.6263 - val_loss: 0.6175 - val_acc: 0.6777 - val_f1: 0.6777\n",
      "Epoch 11/25\n",
      "10604/10604 [==============================] - 1s 68us/step - loss: 0.6288 - acc: 0.6266 - f1: 0.6265 - val_loss: 0.6158 - val_acc: 0.6777 - val_f1: 0.6777\n",
      "Epoch 12/25\n",
      "10604/10604 [==============================] - 1s 68us/step - loss: 0.6207 - acc: 0.6262 - f1: 0.6260 - val_loss: 0.6140 - val_acc: 0.6777 - val_f1: 0.6777\n",
      "Epoch 13/25\n",
      "10604/10604 [==============================] - 1s 67us/step - loss: 0.6071 - acc: 0.6264 - f1: 0.6262 - val_loss: 0.6093 - val_acc: 0.6777 - val_f1: 0.6777\n",
      "Epoch 14/25\n",
      "10604/10604 [==============================] - 1s 68us/step - loss: 0.6014 - acc: 0.6270 - f1: 0.6265 - val_loss: 0.6099 - val_acc: 0.6777 - val_f1: 0.6777\n",
      "Epoch 15/25\n",
      "10604/10604 [==============================] - 1s 73us/step - loss: 0.5849 - acc: 0.6275 - f1: 0.6270 - val_loss: 0.6065 - val_acc: 0.6777 - val_f1: 0.6777\n",
      "Epoch 16/25\n",
      "10604/10604 [==============================] - 1s 83us/step - loss: 0.5771 - acc: 0.6281 - f1: 0.6276 - val_loss: 0.6042 - val_acc: 0.6773 - val_f1: 0.6771\n",
      "Epoch 17/25\n",
      "10604/10604 [==============================] - 1s 79us/step - loss: 0.5656 - acc: 0.6680 - f1: 0.6461 - val_loss: 0.6056 - val_acc: 0.6361 - val_f1: 0.6322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25\n",
      "10604/10604 [==============================] - 1s 67us/step - loss: 0.5522 - acc: 0.7112 - f1: 0.7093 - val_loss: 0.6131 - val_acc: 0.6438 - val_f1: 0.6426\n",
      "Epoch 19/25\n",
      "10604/10604 [==============================] - 1s 67us/step - loss: 0.5479 - acc: 0.7220 - f1: 0.7214 - val_loss: 0.6046 - val_acc: 0.6285 - val_f1: 0.6278\n",
      "Epoch 20/25\n",
      "10604/10604 [==============================] - 1s 68us/step - loss: 0.5316 - acc: 0.7406 - f1: 0.7406 - val_loss: 0.6114 - val_acc: 0.6323 - val_f1: 0.6321\n",
      "Epoch 21/25\n",
      "10604/10604 [==============================] - 1s 68us/step - loss: 0.5162 - acc: 0.7528 - f1: 0.7528 - val_loss: 0.6193 - val_acc: 0.6272 - val_f1: 0.6271\n",
      "Epoch 22/25\n",
      "10604/10604 [==============================] - 1s 67us/step - loss: 0.5091 - acc: 0.7509 - f1: 0.7513 - val_loss: 0.6249 - val_acc: 0.6412 - val_f1: 0.6415\n",
      "Epoch 23/25\n",
      "10604/10604 [==============================] - 1s 67us/step - loss: 0.4987 - acc: 0.7640 - f1: 0.7643 - val_loss: 0.6250 - val_acc: 0.6264 - val_f1: 0.6265\n",
      "Epoch 24/25\n",
      "10604/10604 [==============================] - 1s 68us/step - loss: 0.4863 - acc: 0.7748 - f1: 0.7750 - val_loss: 0.6230 - val_acc: 0.6306 - val_f1: 0.6308\n",
      "Epoch 25/25\n",
      "10604/10604 [==============================] - 1s 67us/step - loss: 0.4761 - acc: 0.7790 - f1: 0.7791 - val_loss: 0.6398 - val_acc: 0.6476 - val_f1: 0.6478\n",
      "4\n",
      "(11731, 10001)\n",
      "(11731,)\n",
      "Baseline # : 6506\n",
      "Peak # : 5225\n",
      "x_train :  10557\n",
      "x_test  :  1174\n",
      "y_train :  Counter({0: 5869, 1: 4688})\n",
      "y_test  :  Counter({0: 637, 1: 537})\n",
      "(10001,)\n",
      "(10557,)\n",
      "(1174, 10001)\n",
      "(1174,)\n",
      "Train on 10557 samples, validate on 1174 samples\n",
      "Epoch 1/25\n",
      "10557/10557 [==============================] - 2s 155us/step - loss: 0.7348 - acc: 0.5056 - f1: 0.4588 - val_loss: 0.6904 - val_acc: 0.5447 - val_f1: 0.5423\n",
      "Epoch 2/25\n",
      "10557/10557 [==============================] - 1s 68us/step - loss: 0.7127 - acc: 0.5387 - f1: 0.5358 - val_loss: 0.6898 - val_acc: 0.5413 - val_f1: 0.5423\n",
      "Epoch 3/25\n",
      "10557/10557 [==============================] - 1s 67us/step - loss: 0.6974 - acc: 0.5502 - f1: 0.5584 - val_loss: 0.6898 - val_acc: 0.5417 - val_f1: 0.5425\n",
      "Epoch 4/25\n",
      "10557/10557 [==============================] - 1s 67us/step - loss: 0.6954 - acc: 0.5557 - f1: 0.5655 - val_loss: 0.6898 - val_acc: 0.5417 - val_f1: 0.5417\n",
      "Epoch 5/25\n",
      "10557/10557 [==============================] - 1s 67us/step - loss: 0.6951 - acc: 0.5539 - f1: 0.5610 - val_loss: 0.6895 - val_acc: 0.5417 - val_f1: 0.5417\n",
      "Epoch 6/25\n",
      "10557/10557 [==============================] - 1s 67us/step - loss: 0.6902 - acc: 0.5562 - f1: 0.5625 - val_loss: 0.6887 - val_acc: 0.5426 - val_f1: 0.5426\n",
      "Epoch 7/25\n",
      "10557/10557 [==============================] - 1s 69us/step - loss: 0.6899 - acc: 0.5601 - f1: 0.5657 - val_loss: 0.6882 - val_acc: 0.5426 - val_f1: 0.5430\n",
      "Epoch 8/25\n",
      "10557/10557 [==============================] - 1s 70us/step - loss: 0.6869 - acc: 0.5601 - f1: 0.5656 - val_loss: 0.6850 - val_acc: 0.5430 - val_f1: 0.5432\n",
      "Epoch 9/25\n",
      "10557/10557 [==============================] - 1s 84us/step - loss: 0.6842 - acc: 0.5620 - f1: 0.5656 - val_loss: 0.6830 - val_acc: 0.5430 - val_f1: 0.5432\n",
      "Epoch 10/25\n",
      "10557/10557 [==============================] - 1s 75us/step - loss: 0.6818 - acc: 0.5645 - f1: 0.5678 - val_loss: 0.6783 - val_acc: 0.5434 - val_f1: 0.5441\n",
      "Epoch 11/25\n",
      "10557/10557 [==============================] - 1s 68us/step - loss: 0.6754 - acc: 0.5703 - f1: 0.5737 - val_loss: 0.6694 - val_acc: 0.5473 - val_f1: 0.5490\n",
      "Epoch 12/25\n",
      "10557/10557 [==============================] - 1s 66us/step - loss: 0.6662 - acc: 0.5838 - f1: 0.5847 - val_loss: 0.6577 - val_acc: 0.5541 - val_f1: 0.5519\n",
      "Epoch 13/25\n",
      "10557/10557 [==============================] - 1s 67us/step - loss: 0.6546 - acc: 0.5979 - f1: 0.5990 - val_loss: 0.6423 - val_acc: 0.5643 - val_f1: 0.5619\n",
      "Epoch 14/25\n",
      "10557/10557 [==============================] - 1s 67us/step - loss: 0.6363 - acc: 0.6186 - f1: 0.6194 - val_loss: 0.6268 - val_acc: 0.5886 - val_f1: 0.5876\n",
      "Epoch 15/25\n",
      "10557/10557 [==============================] - 1s 68us/step - loss: 0.6254 - acc: 0.6316 - f1: 0.6328 - val_loss: 0.6175 - val_acc: 0.6193 - val_f1: 0.6178\n",
      "Epoch 16/25\n",
      "10557/10557 [==============================] - 1s 67us/step - loss: 0.6133 - acc: 0.6581 - f1: 0.6586 - val_loss: 0.6037 - val_acc: 0.6282 - val_f1: 0.6245\n",
      "Epoch 17/25\n",
      "10557/10557 [==============================] - 1s 66us/step - loss: 0.5957 - acc: 0.6797 - f1: 0.6787 - val_loss: 0.5897 - val_acc: 0.6559 - val_f1: 0.6508\n",
      "Epoch 18/25\n",
      "10557/10557 [==============================] - 1s 67us/step - loss: 0.5896 - acc: 0.6950 - f1: 0.6941 - val_loss: 0.5918 - val_acc: 0.6661 - val_f1: 0.6626\n",
      "Epoch 19/25\n",
      "10557/10557 [==============================] - 1s 69us/step - loss: 0.5593 - acc: 0.7210 - f1: 0.7194 - val_loss: 0.5808 - val_acc: 0.6712 - val_f1: 0.6667\n",
      "Epoch 20/25\n",
      "10557/10557 [==============================] - 1s 70us/step - loss: 0.5589 - acc: 0.7276 - f1: 0.7258 - val_loss: 0.5748 - val_acc: 0.6870 - val_f1: 0.6830\n",
      "Epoch 21/25\n",
      "10557/10557 [==============================] - 1s 78us/step - loss: 0.5421 - acc: 0.7513 - f1: 0.7500 - val_loss: 0.5729 - val_acc: 0.6963 - val_f1: 0.6931\n",
      "Epoch 22/25\n",
      "10557/10557 [==============================] - 1s 76us/step - loss: 0.5258 - acc: 0.7571 - f1: 0.7557 - val_loss: 0.5711 - val_acc: 0.6857 - val_f1: 0.6838\n",
      "Epoch 23/25\n",
      "10557/10557 [==============================] - 1s 68us/step - loss: 0.5043 - acc: 0.7707 - f1: 0.7701 - val_loss: 0.5670 - val_acc: 0.7023 - val_f1: 0.7001\n",
      "Epoch 24/25\n",
      "10557/10557 [==============================] - 1s 67us/step - loss: 0.4915 - acc: 0.7825 - f1: 0.7817 - val_loss: 0.5623 - val_acc: 0.7202 - val_f1: 0.7193\n",
      "Epoch 25/25\n",
      "10557/10557 [==============================] - 1s 69us/step - loss: 0.4834 - acc: 0.7884 - f1: 0.7876 - val_loss: 0.5643 - val_acc: 0.7185 - val_f1: 0.7169\n",
      "5\n",
      "(12508, 10001)\n",
      "(12508,)\n",
      "Baseline # : 5027\n",
      "Peak # : 7481\n",
      "x_train :  11257\n",
      "x_test  :  1251\n",
      "y_train :  Counter({1: 6735, 0: 4522})\n",
      "y_test  :  Counter({1: 746, 0: 505})\n",
      "(10001,)\n",
      "(11257,)\n",
      "(1251, 10001)\n",
      "(1251,)\n",
      "Train on 11257 samples, validate on 1251 samples\n",
      "Epoch 1/25\n",
      "11257/11257 [==============================] - 2s 148us/step - loss: 0.7481 - acc: 0.5384 - f1: 0.5634 - val_loss: 0.6835 - val_acc: 0.5951 - val_f1: 0.5941\n",
      "Epoch 2/25\n",
      "11257/11257 [==============================] - 1s 67us/step - loss: 0.7082 - acc: 0.5708 - f1: 0.5860 - val_loss: 0.6822 - val_acc: 0.5955 - val_f1: 0.5955\n",
      "Epoch 3/25\n",
      "11257/11257 [==============================] - 1s 68us/step - loss: 0.6959 - acc: 0.5863 - f1: 0.5953 - val_loss: 0.6792 - val_acc: 0.5959 - val_f1: 0.5961\n",
      "Epoch 4/25\n",
      "11257/11257 [==============================] - 1s 67us/step - loss: 0.6900 - acc: 0.5879 - f1: 0.5931 - val_loss: 0.6774 - val_acc: 0.5963 - val_f1: 0.5963\n",
      "Epoch 5/25\n",
      "11257/11257 [==============================] - 1s 67us/step - loss: 0.6847 - acc: 0.5929 - f1: 0.5966 - val_loss: 0.6765 - val_acc: 0.5963 - val_f1: 0.5963\n",
      "Epoch 6/25\n",
      "11257/11257 [==============================] - 1s 66us/step - loss: 0.6828 - acc: 0.5952 - f1: 0.5980 - val_loss: 0.6754 - val_acc: 0.5963 - val_f1: 0.5963\n",
      "Epoch 7/25\n",
      "11257/11257 [==============================] - 1s 67us/step - loss: 0.6788 - acc: 0.5963 - f1: 0.5982 - val_loss: 0.6745 - val_acc: 0.5963 - val_f1: 0.5963\n",
      "Epoch 8/25\n",
      "11257/11257 [==============================] - 1s 66us/step - loss: 0.6760 - acc: 0.5975 - f1: 0.5996 - val_loss: 0.6738 - val_acc: 0.5963 - val_f1: 0.5963\n",
      "Epoch 9/25\n",
      "11257/11257 [==============================] - 1s 66us/step - loss: 0.6740 - acc: 0.5971 - f1: 0.5990 - val_loss: 0.6725 - val_acc: 0.5963 - val_f1: 0.5963\n",
      "Epoch 10/25\n",
      "11257/11257 [==============================] - 1s 67us/step - loss: 0.6705 - acc: 0.5979 - f1: 0.5993 - val_loss: 0.6724 - val_acc: 0.5963 - val_f1: 0.5963\n",
      "Epoch 11/25\n",
      "11257/11257 [==============================] - 1s 70us/step - loss: 0.6690 - acc: 0.5974 - f1: 0.5982 - val_loss: 0.6701 - val_acc: 0.5963 - val_f1: 0.5963\n",
      "Epoch 12/25\n",
      "11257/11257 [==============================] - 1s 77us/step - loss: 0.6657 - acc: 0.5974 - f1: 0.5986 - val_loss: 0.6688 - val_acc: 0.5955 - val_f1: 0.5955\n",
      "Epoch 13/25\n",
      "11257/11257 [==============================] - 1s 82us/step - loss: 0.6555 - acc: 0.5980 - f1: 0.5994 - val_loss: 0.6615 - val_acc: 0.5955 - val_f1: 0.5955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25\n",
      "11257/11257 [==============================] - 1s 68us/step - loss: 0.6463 - acc: 0.5989 - f1: 0.6002 - val_loss: 0.6588 - val_acc: 0.5955 - val_f1: 0.5955\n",
      "Epoch 15/25\n",
      "11257/11257 [==============================] - 1s 67us/step - loss: 0.6345 - acc: 0.6020 - f1: 0.6034 - val_loss: 0.6524 - val_acc: 0.5951 - val_f1: 0.5953\n",
      "Epoch 16/25\n",
      "11257/11257 [==============================] - 1s 67us/step - loss: 0.6230 - acc: 0.6113 - f1: 0.6128 - val_loss: 0.6459 - val_acc: 0.6055 - val_f1: 0.6060\n",
      "Epoch 17/25\n",
      "11257/11257 [==============================] - 1s 67us/step - loss: 0.6062 - acc: 0.6253 - f1: 0.6270 - val_loss: 0.6431 - val_acc: 0.6363 - val_f1: 0.6342\n",
      "Epoch 18/25\n",
      "11257/11257 [==============================] - 1s 66us/step - loss: 0.5992 - acc: 0.6449 - f1: 0.6466 - val_loss: 0.6367 - val_acc: 0.6623 - val_f1: 0.6611\n",
      "Epoch 19/25\n",
      "11257/11257 [==============================] - 1s 66us/step - loss: 0.5857 - acc: 0.6838 - f1: 0.6833 - val_loss: 0.6324 - val_acc: 0.6443 - val_f1: 0.6428\n",
      "Epoch 20/25\n",
      "11257/11257 [==============================] - 1s 65us/step - loss: 0.5593 - acc: 0.7214 - f1: 0.7214 - val_loss: 0.6191 - val_acc: 0.6343 - val_f1: 0.6335\n",
      "Epoch 21/25\n",
      "11257/11257 [==============================] - 1s 66us/step - loss: 0.5612 - acc: 0.7365 - f1: 0.7364 - val_loss: 0.6204 - val_acc: 0.6415 - val_f1: 0.6410\n",
      "Epoch 22/25\n",
      "11257/11257 [==============================] - 1s 66us/step - loss: 0.5333 - acc: 0.7468 - f1: 0.7465 - val_loss: 0.6171 - val_acc: 0.6415 - val_f1: 0.6407\n",
      "Epoch 23/25\n",
      "11257/11257 [==============================] - 1s 66us/step - loss: 0.5125 - acc: 0.7604 - f1: 0.7600 - val_loss: 0.6079 - val_acc: 0.6639 - val_f1: 0.6635\n",
      "Epoch 24/25\n",
      "11257/11257 [==============================] - 1s 67us/step - loss: 0.5020 - acc: 0.7708 - f1: 0.7704 - val_loss: 0.6021 - val_acc: 0.6595 - val_f1: 0.6587\n",
      "Epoch 25/25\n",
      "11257/11257 [==============================] - 1s 66us/step - loss: 0.4849 - acc: 0.7778 - f1: 0.7774 - val_loss: 0.6260 - val_acc: 0.6627 - val_f1: 0.6615\n",
      "6\n",
      "(10083, 10001)\n",
      "(10083,)\n",
      "Baseline # : 4929\n",
      "Peak # : 5154\n",
      "x_train :  9074\n",
      "x_test  :  1009\n",
      "y_train :  Counter({1: 4622, 0: 4452})\n",
      "y_test  :  Counter({1: 532, 0: 477})\n",
      "(10001,)\n",
      "(9074,)\n",
      "(1009, 10001)\n",
      "(1009,)\n",
      "Train on 9074 samples, validate on 1009 samples\n",
      "Epoch 1/25\n",
      "9074/9074 [==============================] - 2s 179us/step - loss: 0.7468 - acc: 0.4993 - f1: 0.4769 - val_loss: 0.6930 - val_acc: 0.5282 - val_f1: 0.5273\n",
      "Epoch 2/25\n",
      "9074/9074 [==============================] - 1s 71us/step - loss: 0.7158 - acc: 0.5106 - f1: 0.4859 - val_loss: 0.6928 - val_acc: 0.5282 - val_f1: 0.5268\n",
      "Epoch 3/25\n",
      "9074/9074 [==============================] - 1s 68us/step - loss: 0.7081 - acc: 0.5107 - f1: 0.4905 - val_loss: 0.6923 - val_acc: 0.5273 - val_f1: 0.5273\n",
      "Epoch 4/25\n",
      "9074/9074 [==============================] - 1s 67us/step - loss: 0.7043 - acc: 0.5112 - f1: 0.4949 - val_loss: 0.6922 - val_acc: 0.5273 - val_f1: 0.5273\n",
      "Epoch 5/25\n",
      "9074/9074 [==============================] - 1s 70us/step - loss: 0.7006 - acc: 0.5153 - f1: 0.5028 - val_loss: 0.6919 - val_acc: 0.5307 - val_f1: 0.5328\n",
      "Epoch 6/25\n",
      "9074/9074 [==============================] - 1s 82us/step - loss: 0.6968 - acc: 0.5148 - f1: 0.5095 - val_loss: 0.6919 - val_acc: 0.5273 - val_f1: 0.5282\n",
      "Epoch 7/25\n",
      "9074/9074 [==============================] - 1s 92us/step - loss: 0.6964 - acc: 0.5183 - f1: 0.5133 - val_loss: 0.6916 - val_acc: 0.5322 - val_f1: 0.5350\n",
      "Epoch 8/25\n",
      "9074/9074 [==============================] - 1s 69us/step - loss: 0.6951 - acc: 0.5183 - f1: 0.5125 - val_loss: 0.6916 - val_acc: 0.5302 - val_f1: 0.5312\n",
      "Epoch 9/25\n",
      "9074/9074 [==============================] - 1s 68us/step - loss: 0.6958 - acc: 0.5190 - f1: 0.5156 - val_loss: 0.6912 - val_acc: 0.5322 - val_f1: 0.5332\n",
      "Epoch 10/25\n",
      "9074/9074 [==============================] - 1s 68us/step - loss: 0.6962 - acc: 0.5272 - f1: 0.5205 - val_loss: 0.6911 - val_acc: 0.5362 - val_f1: 0.5371\n",
      "Epoch 11/25\n",
      "9074/9074 [==============================] - 1s 71us/step - loss: 0.6923 - acc: 0.5272 - f1: 0.5197 - val_loss: 0.6908 - val_acc: 0.5337 - val_f1: 0.5353\n",
      "Epoch 12/25\n",
      "9074/9074 [==============================] - 1s 71us/step - loss: 0.6929 - acc: 0.5361 - f1: 0.5299 - val_loss: 0.6901 - val_acc: 0.5401 - val_f1: 0.5438\n",
      "Epoch 13/25\n",
      "9074/9074 [==============================] - 1s 84us/step - loss: 0.6878 - acc: 0.5488 - f1: 0.5437 - val_loss: 0.6877 - val_acc: 0.5610 - val_f1: 0.5651\n",
      "Epoch 14/25\n",
      "9074/9074 [==============================] - 1s 78us/step - loss: 0.6859 - acc: 0.5543 - f1: 0.5512 - val_loss: 0.6858 - val_acc: 0.5600 - val_f1: 0.5634\n",
      "Epoch 15/25\n",
      "9074/9074 [==============================] - 1s 67us/step - loss: 0.6789 - acc: 0.5719 - f1: 0.5702 - val_loss: 0.6806 - val_acc: 0.5852 - val_f1: 0.5871\n",
      "Epoch 16/25\n",
      "9074/9074 [==============================] - 1s 68us/step - loss: 0.6757 - acc: 0.5816 - f1: 0.5815 - val_loss: 0.6747 - val_acc: 0.5917 - val_f1: 0.5948\n",
      "Epoch 17/25\n",
      "9074/9074 [==============================] - 1s 68us/step - loss: 0.6649 - acc: 0.5969 - f1: 0.5981 - val_loss: 0.6726 - val_acc: 0.5872 - val_f1: 0.5887\n",
      "Epoch 18/25\n",
      "9074/9074 [==============================] - 1s 68us/step - loss: 0.6559 - acc: 0.6207 - f1: 0.6210 - val_loss: 0.6694 - val_acc: 0.5892 - val_f1: 0.5894\n",
      "Epoch 19/25\n",
      "9074/9074 [==============================] - 1s 67us/step - loss: 0.6496 - acc: 0.6303 - f1: 0.6314 - val_loss: 0.6671 - val_acc: 0.5937 - val_f1: 0.5945\n",
      "Epoch 20/25\n",
      "9074/9074 [==============================] - 1s 67us/step - loss: 0.6423 - acc: 0.6392 - f1: 0.6396 - val_loss: 0.6652 - val_acc: 0.5932 - val_f1: 0.5933\n",
      "Epoch 21/25\n",
      "9074/9074 [==============================] - 1s 67us/step - loss: 0.6259 - acc: 0.6637 - f1: 0.6635 - val_loss: 0.6654 - val_acc: 0.5966 - val_f1: 0.5962\n",
      "Epoch 22/25\n",
      "9074/9074 [==============================] - 1s 67us/step - loss: 0.6184 - acc: 0.6753 - f1: 0.6753 - val_loss: 0.6646 - val_acc: 0.6011 - val_f1: 0.6001\n",
      "Epoch 23/25\n",
      "9074/9074 [==============================] - 1s 68us/step - loss: 0.6007 - acc: 0.6926 - f1: 0.6924 - val_loss: 0.6614 - val_acc: 0.6065 - val_f1: 0.6061\n",
      "Epoch 24/25\n",
      "9074/9074 [==============================] - 1s 68us/step - loss: 0.5913 - acc: 0.7059 - f1: 0.7062 - val_loss: 0.6618 - val_acc: 0.6189 - val_f1: 0.6187\n",
      "Epoch 25/25\n",
      "9074/9074 [==============================] - 1s 69us/step - loss: 0.5805 - acc: 0.7128 - f1: 0.7126 - val_loss: 0.6642 - val_acc: 0.6234 - val_f1: 0.6234\n",
      "7\n",
      "(9553, 10001)\n",
      "(9553,)\n",
      "Baseline # : 4963\n",
      "Peak # : 4590\n",
      "x_train :  8597\n",
      "x_test  :  956\n",
      "y_train :  Counter({0: 4448, 1: 4149})\n",
      "y_test  :  Counter({0: 515, 1: 441})\n",
      "(10001,)\n",
      "(8597,)\n",
      "(956, 10001)\n",
      "(956,)\n",
      "Train on 8597 samples, validate on 956 samples\n",
      "Epoch 1/25\n",
      "8597/8597 [==============================] - 2s 191us/step - loss: 0.8186 - acc: 0.5087 - f1: 0.4841 - val_loss: 0.6921 - val_acc: 0.5329 - val_f1: 0.5336\n",
      "Epoch 2/25\n",
      "8597/8597 [==============================] - 1s 101us/step - loss: 0.7427 - acc: 0.5046 - f1: 0.4926 - val_loss: 0.6909 - val_acc: 0.5335 - val_f1: 0.5459\n",
      "Epoch 3/25\n",
      "8597/8597 [==============================] - 1s 81us/step - loss: 0.7179 - acc: 0.5133 - f1: 0.5089 - val_loss: 0.6915 - val_acc: 0.5382 - val_f1: 0.5373\n",
      "Epoch 4/25\n",
      "8597/8597 [==============================] - 1s 76us/step - loss: 0.7101 - acc: 0.5111 - f1: 0.5053 - val_loss: 0.6912 - val_acc: 0.5377 - val_f1: 0.5381\n",
      "Epoch 5/25\n",
      "8597/8597 [==============================] - 1s 69us/step - loss: 0.7026 - acc: 0.5192 - f1: 0.5164 - val_loss: 0.6907 - val_acc: 0.5408 - val_f1: 0.5408\n",
      "Epoch 6/25\n",
      "8597/8597 [==============================] - 1s 72us/step - loss: 0.7001 - acc: 0.5131 - f1: 0.5078 - val_loss: 0.6913 - val_acc: 0.5387 - val_f1: 0.5387\n",
      "Epoch 7/25\n",
      "8597/8597 [==============================] - 1s 88us/step - loss: 0.7008 - acc: 0.5186 - f1: 0.5144 - val_loss: 0.6912 - val_acc: 0.5387 - val_f1: 0.5387\n",
      "Epoch 8/25\n",
      "8597/8597 [==============================] - 1s 73us/step - loss: 0.6982 - acc: 0.5148 - f1: 0.5081 - val_loss: 0.6912 - val_acc: 0.5387 - val_f1: 0.5387\n",
      "Epoch 9/25\n",
      "8597/8597 [==============================] - 1s 67us/step - loss: 0.6965 - acc: 0.5130 - f1: 0.5093 - val_loss: 0.6911 - val_acc: 0.5387 - val_f1: 0.5387\n",
      "Epoch 10/25\n",
      "8597/8597 [==============================] - 1s 67us/step - loss: 0.6964 - acc: 0.5151 - f1: 0.5100 - val_loss: 0.6909 - val_acc: 0.5392 - val_f1: 0.5395\n",
      "Epoch 11/25\n",
      "8597/8597 [==============================] - 1s 67us/step - loss: 0.6956 - acc: 0.5161 - f1: 0.5116 - val_loss: 0.6910 - val_acc: 0.5377 - val_f1: 0.5371\n",
      "Epoch 12/25\n",
      "8597/8597 [==============================] - 1s 67us/step - loss: 0.6947 - acc: 0.5146 - f1: 0.5105 - val_loss: 0.6910 - val_acc: 0.5387 - val_f1: 0.5387\n",
      "Epoch 13/25\n",
      "8597/8597 [==============================] - 1s 68us/step - loss: 0.6936 - acc: 0.5210 - f1: 0.5167 - val_loss: 0.6908 - val_acc: 0.5403 - val_f1: 0.5400\n",
      "Epoch 14/25\n",
      "8597/8597 [==============================] - 1s 68us/step - loss: 0.6950 - acc: 0.5198 - f1: 0.5168 - val_loss: 0.6910 - val_acc: 0.5392 - val_f1: 0.5390\n",
      "Epoch 15/25\n",
      "8597/8597 [==============================] - 1s 68us/step - loss: 0.6946 - acc: 0.5167 - f1: 0.5131 - val_loss: 0.6910 - val_acc: 0.5387 - val_f1: 0.5387\n",
      "Epoch 16/25\n",
      "8597/8597 [==============================] - 1s 68us/step - loss: 0.6951 - acc: 0.5166 - f1: 0.5126 - val_loss: 0.6911 - val_acc: 0.5377 - val_f1: 0.5377\n",
      "Epoch 17/25\n",
      "8597/8597 [==============================] - 1s 67us/step - loss: 0.6930 - acc: 0.5244 - f1: 0.5212 - val_loss: 0.6911 - val_acc: 0.5382 - val_f1: 0.5379\n",
      "Epoch 18/25\n",
      "8597/8597 [==============================] - 1s 72us/step - loss: 0.6943 - acc: 0.5224 - f1: 0.5201 - val_loss: 0.6910 - val_acc: 0.5397 - val_f1: 0.5397\n",
      "Epoch 19/25\n",
      "8597/8597 [==============================] - 1s 80us/step - loss: 0.6934 - acc: 0.5175 - f1: 0.5153 - val_loss: 0.6909 - val_acc: 0.5403 - val_f1: 0.5400\n",
      "Epoch 20/25\n",
      "8597/8597 [==============================] - 1s 83us/step - loss: 0.6938 - acc: 0.5148 - f1: 0.5129 - val_loss: 0.6905 - val_acc: 0.5387 - val_f1: 0.5387\n",
      "Epoch 21/25\n",
      "8597/8597 [==============================] - 1s 68us/step - loss: 0.6937 - acc: 0.5168 - f1: 0.5149 - val_loss: 0.6905 - val_acc: 0.5382 - val_f1: 0.5379\n",
      "Epoch 22/25\n",
      "8597/8597 [==============================] - 1s 67us/step - loss: 0.6926 - acc: 0.5172 - f1: 0.5150 - val_loss: 0.6904 - val_acc: 0.5387 - val_f1: 0.5387\n",
      "Epoch 23/25\n",
      "8597/8597 [==============================] - 1s 68us/step - loss: 0.6912 - acc: 0.5183 - f1: 0.5171 - val_loss: 0.6898 - val_acc: 0.5387 - val_f1: 0.5387\n",
      "Epoch 24/25\n",
      "8597/8597 [==============================] - 1s 67us/step - loss: 0.6897 - acc: 0.5166 - f1: 0.5154 - val_loss: 0.6880 - val_acc: 0.5371 - val_f1: 0.5369\n",
      "Epoch 25/25\n",
      "8597/8597 [==============================] - 1s 68us/step - loss: 0.6862 - acc: 0.5204 - f1: 0.5193 - val_loss: 0.6860 - val_acc: 0.5371 - val_f1: 0.5369\n",
      "8\n",
      "(11967, 10001)\n",
      "(11967,)\n",
      "Baseline # : 5878\n",
      "Peak # : 6089\n",
      "x_train :  10770\n",
      "x_test  :  1197\n",
      "y_train :  Counter({1: 5483, 0: 5287})\n",
      "y_test  :  Counter({1: 606, 0: 591})\n",
      "(10001,)\n",
      "(10770,)\n",
      "(1197, 10001)\n",
      "(1197,)\n",
      "Train on 10770 samples, validate on 1197 samples\n",
      "Epoch 1/25\n",
      "10770/10770 [==============================] - 2s 179us/step - loss: 0.7815 - acc: 0.4955 - f1: 0.5095 - val_loss: 0.6916 - val_acc: 0.5255 - val_f1: 0.4719\n",
      "Epoch 2/25\n",
      "10770/10770 [==============================] - 1s 92us/step - loss: 0.7158 - acc: 0.5045 - f1: 0.5071 - val_loss: 0.6930 - val_acc: 0.5067 - val_f1: 0.4862\n",
      "Epoch 3/25\n",
      "10770/10770 [==============================] - 1s 87us/step - loss: 0.7064 - acc: 0.5071 - f1: 0.5012 - val_loss: 0.6935 - val_acc: 0.5017 - val_f1: 0.4929\n",
      "Epoch 4/25\n",
      "10770/10770 [==============================] - 1s 72us/step - loss: 0.7016 - acc: 0.5101 - f1: 0.5002 - val_loss: 0.6926 - val_acc: 0.5129 - val_f1: 0.4867\n",
      "Epoch 5/25\n",
      "10770/10770 [==============================] - 1s 70us/step - loss: 0.6972 - acc: 0.5120 - f1: 0.4998 - val_loss: 0.6925 - val_acc: 0.5113 - val_f1: 0.4880\n",
      "Epoch 6/25\n",
      "10770/10770 [==============================] - 1s 72us/step - loss: 0.6942 - acc: 0.5127 - f1: 0.5031 - val_loss: 0.6926 - val_acc: 0.5050 - val_f1: 0.5022\n",
      "Epoch 7/25\n",
      "10770/10770 [==============================] - 1s 80us/step - loss: 0.6943 - acc: 0.5221 - f1: 0.5138 - val_loss: 0.6929 - val_acc: 0.5013 - val_f1: 0.4931\n",
      "Epoch 8/25\n",
      "10770/10770 [==============================] - 1s 85us/step - loss: 0.6918 - acc: 0.5227 - f1: 0.5133 - val_loss: 0.6922 - val_acc: 0.5134 - val_f1: 0.5039\n",
      "Epoch 9/25\n",
      "10770/10770 [==============================] - 1s 68us/step - loss: 0.6910 - acc: 0.5241 - f1: 0.5174 - val_loss: 0.6922 - val_acc: 0.5096 - val_f1: 0.5044\n",
      "Epoch 10/25\n",
      "10770/10770 [==============================] - 1s 68us/step - loss: 0.6885 - acc: 0.5348 - f1: 0.5278 - val_loss: 0.6919 - val_acc: 0.5113 - val_f1: 0.5043\n",
      "Epoch 11/25\n",
      "10770/10770 [==============================] - 1s 68us/step - loss: 0.6873 - acc: 0.5355 - f1: 0.5307 - val_loss: 0.6922 - val_acc: 0.5092 - val_f1: 0.5068\n",
      "Epoch 12/25\n",
      "10770/10770 [==============================] - 1s 68us/step - loss: 0.6864 - acc: 0.5399 - f1: 0.5353 - val_loss: 0.6899 - val_acc: 0.5368 - val_f1: 0.5342\n",
      "Epoch 13/25\n",
      "10770/10770 [==============================] - 1s 68us/step - loss: 0.6836 - acc: 0.5529 - f1: 0.5490 - val_loss: 0.6889 - val_acc: 0.5347 - val_f1: 0.5320\n",
      "Epoch 14/25\n",
      "10770/10770 [==============================] - 1s 69us/step - loss: 0.6817 - acc: 0.5616 - f1: 0.5586 - val_loss: 0.6874 - val_acc: 0.5405 - val_f1: 0.5375\n",
      "Epoch 15/25\n",
      "10770/10770 [==============================] - 1s 70us/step - loss: 0.6780 - acc: 0.5680 - f1: 0.5654 - val_loss: 0.6878 - val_acc: 0.5347 - val_f1: 0.5307\n",
      "Epoch 16/25\n",
      "10770/10770 [==============================] - 1s 74us/step - loss: 0.6720 - acc: 0.5769 - f1: 0.5739 - val_loss: 0.6879 - val_acc: 0.5334 - val_f1: 0.5316\n",
      "Epoch 17/25\n",
      "10770/10770 [==============================] - 1s 87us/step - loss: 0.6652 - acc: 0.5913 - f1: 0.5893 - val_loss: 0.6824 - val_acc: 0.5418 - val_f1: 0.5396\n",
      "Epoch 18/25\n",
      "10770/10770 [==============================] - 1s 75us/step - loss: 0.6596 - acc: 0.5987 - f1: 0.5973 - val_loss: 0.6826 - val_acc: 0.5355 - val_f1: 0.5339\n",
      "Epoch 19/25\n",
      "10770/10770 [==============================] - 1s 68us/step - loss: 0.6542 - acc: 0.6150 - f1: 0.6138 - val_loss: 0.6813 - val_acc: 0.5388 - val_f1: 0.5376\n",
      "Epoch 20/25\n",
      "10770/10770 [==============================] - 1s 68us/step - loss: 0.6477 - acc: 0.6188 - f1: 0.6181 - val_loss: 0.6834 - val_acc: 0.5405 - val_f1: 0.5405\n",
      "Epoch 21/25\n",
      "10770/10770 [==============================] - 1s 69us/step - loss: 0.6402 - acc: 0.6296 - f1: 0.6297 - val_loss: 0.6832 - val_acc: 0.5489 - val_f1: 0.5488\n",
      "Epoch 22/25\n",
      "10770/10770 [==============================] - 1s 67us/step - loss: 0.6346 - acc: 0.6450 - f1: 0.6450 - val_loss: 0.6845 - val_acc: 0.5464 - val_f1: 0.5460\n",
      "Epoch 23/25\n",
      "10770/10770 [==============================] - 1s 68us/step - loss: 0.6261 - acc: 0.6508 - f1: 0.6508 - val_loss: 0.6858 - val_acc: 0.5622 - val_f1: 0.5619\n",
      "Epoch 24/25\n",
      "10770/10770 [==============================] - 1s 68us/step - loss: 0.6156 - acc: 0.6630 - f1: 0.6632 - val_loss: 0.6861 - val_acc: 0.5455 - val_f1: 0.5455\n",
      "Epoch 25/25\n",
      "10770/10770 [==============================] - 1s 68us/step - loss: 0.6101 - acc: 0.6660 - f1: 0.6659 - val_loss: 0.6887 - val_acc: 0.5493 - val_f1: 0.5494\n",
      "9\n",
      "(11699, 10001)\n",
      "(11699,)\n",
      "Baseline # : 4505\n",
      "Peak # : 7194\n",
      "x_train :  10529\n",
      "x_test  :  1170\n",
      "y_train :  Counter({1: 6455, 0: 4074})\n",
      "y_test  :  Counter({1: 739, 0: 431})\n",
      "(10001,)\n",
      "(10529,)\n",
      "(1170, 10001)\n",
      "(1170,)\n",
      "Train on 10529 samples, validate on 1170 samples\n",
      "Epoch 1/25\n",
      "10529/10529 [==============================] - 2s 187us/step - loss: 0.7730 - acc: 0.5579 - f1: 0.5786 - val_loss: 0.6848 - val_acc: 0.6278 - val_f1: 0.6330\n",
      "Epoch 2/25\n",
      "10529/10529 [==============================] - 1s 70us/step - loss: 0.7116 - acc: 0.5883 - f1: 0.6009 - val_loss: 0.6771 - val_acc: 0.6316 - val_f1: 0.6319\n",
      "Epoch 3/25\n",
      "10529/10529 [==============================] - 1s 90us/step - loss: 0.6933 - acc: 0.6017 - f1: 0.6099 - val_loss: 0.6724 - val_acc: 0.6316 - val_f1: 0.6316\n",
      "Epoch 4/25\n",
      "10529/10529 [==============================] - 1s 71us/step - loss: 0.6900 - acc: 0.6080 - f1: 0.6134 - val_loss: 0.6681 - val_acc: 0.6316 - val_f1: 0.6316\n",
      "Epoch 5/25\n",
      "10529/10529 [==============================] - 1s 68us/step - loss: 0.6814 - acc: 0.6115 - f1: 0.6152 - val_loss: 0.6650 - val_acc: 0.6316 - val_f1: 0.6316\n",
      "Epoch 6/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10529/10529 [==============================] - 1s 70us/step - loss: 0.6763 - acc: 0.6114 - f1: 0.6142 - val_loss: 0.6627 - val_acc: 0.6316 - val_f1: 0.6316\n",
      "Epoch 7/25\n",
      "10529/10529 [==============================] - 1s 69us/step - loss: 0.6707 - acc: 0.6157 - f1: 0.6175 - val_loss: 0.6611 - val_acc: 0.6316 - val_f1: 0.6316\n",
      "Epoch 8/25\n",
      "10529/10529 [==============================] - 1s 68us/step - loss: 0.6699 - acc: 0.6125 - f1: 0.6145 - val_loss: 0.6599 - val_acc: 0.6316 - val_f1: 0.6316\n",
      "Epoch 9/25\n",
      "10529/10529 [==============================] - 1s 68us/step - loss: 0.6700 - acc: 0.6119 - f1: 0.6146 - val_loss: 0.6590 - val_acc: 0.6316 - val_f1: 0.6316\n",
      "Epoch 10/25\n",
      "10529/10529 [==============================] - 1s 67us/step - loss: 0.6671 - acc: 0.6154 - f1: 0.6172 - val_loss: 0.6584 - val_acc: 0.6316 - val_f1: 0.6316\n",
      "Epoch 11/25\n",
      "10529/10529 [==============================] - 1s 68us/step - loss: 0.6656 - acc: 0.6161 - f1: 0.6179 - val_loss: 0.6573 - val_acc: 0.6316 - val_f1: 0.6316\n",
      "Epoch 12/25\n",
      "10529/10529 [==============================] - 1s 68us/step - loss: 0.6651 - acc: 0.6159 - f1: 0.6170 - val_loss: 0.6564 - val_acc: 0.6325 - val_f1: 0.6325\n",
      "Epoch 13/25\n",
      "10529/10529 [==============================] - 1s 68us/step - loss: 0.6645 - acc: 0.6178 - f1: 0.6191 - val_loss: 0.6548 - val_acc: 0.6316 - val_f1: 0.6316\n",
      "Epoch 14/25\n",
      "10529/10529 [==============================] - 1s 69us/step - loss: 0.6657 - acc: 0.6174 - f1: 0.6184 - val_loss: 0.6555 - val_acc: 0.6325 - val_f1: 0.6325\n",
      "Epoch 15/25\n",
      "10529/10529 [==============================] - 1s 68us/step - loss: 0.6645 - acc: 0.6159 - f1: 0.6168 - val_loss: 0.6547 - val_acc: 0.6325 - val_f1: 0.6325\n",
      "Epoch 16/25\n",
      "10529/10529 [==============================] - 1s 69us/step - loss: 0.6612 - acc: 0.6194 - f1: 0.6204 - val_loss: 0.6523 - val_acc: 0.6325 - val_f1: 0.6325\n",
      "Epoch 17/25\n",
      "10529/10529 [==============================] - 1s 75us/step - loss: 0.6592 - acc: 0.6248 - f1: 0.6256 - val_loss: 0.6512 - val_acc: 0.6333 - val_f1: 0.6333\n",
      "Epoch 18/25\n",
      "10529/10529 [==============================] - 1s 85us/step - loss: 0.6522 - acc: 0.6364 - f1: 0.6371 - val_loss: 0.6446 - val_acc: 0.6397 - val_f1: 0.6396\n",
      "Epoch 19/25\n",
      "10529/10529 [==============================] - 1s 71us/step - loss: 0.6489 - acc: 0.6381 - f1: 0.6386 - val_loss: 0.6414 - val_acc: 0.6397 - val_f1: 0.6396\n",
      "Epoch 20/25\n",
      "10529/10529 [==============================] - 1s 68us/step - loss: 0.6395 - acc: 0.6471 - f1: 0.6476 - val_loss: 0.6361 - val_acc: 0.6466 - val_f1: 0.6464\n",
      "Epoch 21/25\n",
      "10529/10529 [==============================] - 1s 69us/step - loss: 0.6345 - acc: 0.6479 - f1: 0.6484 - val_loss: 0.6327 - val_acc: 0.6432 - val_f1: 0.6430\n",
      "Epoch 22/25\n",
      "10529/10529 [==============================] - 1s 69us/step - loss: 0.6222 - acc: 0.6592 - f1: 0.6595 - val_loss: 0.6272 - val_acc: 0.6521 - val_f1: 0.6518\n",
      "Epoch 23/25\n",
      "10529/10529 [==============================] - 1s 68us/step - loss: 0.6169 - acc: 0.6730 - f1: 0.6733 - val_loss: 0.6241 - val_acc: 0.6620 - val_f1: 0.6615\n",
      "Epoch 24/25\n",
      "10529/10529 [==============================] - 1s 68us/step - loss: 0.6076 - acc: 0.6765 - f1: 0.6764 - val_loss: 0.6183 - val_acc: 0.6607 - val_f1: 0.6604\n",
      "Epoch 25/25\n",
      "10529/10529 [==============================] - 1s 67us/step - loss: 0.5962 - acc: 0.6802 - f1: 0.6802 - val_loss: 0.6090 - val_acc: 0.6718 - val_f1: 0.6718\n",
      "10\n",
      "(9152, 10001)\n",
      "(9152,)\n",
      "Baseline # : 4479\n",
      "Peak # : 4673\n",
      "x_train :  8236\n",
      "x_test  :  916\n",
      "y_train :  Counter({1: 4225, 0: 4011})\n",
      "y_test  :  Counter({0: 468, 1: 448})\n",
      "(10001,)\n",
      "(8236,)\n",
      "(916, 10001)\n",
      "(916,)\n",
      "Train on 8236 samples, validate on 916 samples\n",
      "Epoch 1/25\n",
      "8236/8236 [==============================] - 2s 222us/step - loss: 0.8155 - acc: 0.5106 - f1: 0.4965 - val_loss: 0.6812 - val_acc: 0.6037 - val_f1: 0.5770\n",
      "Epoch 2/25\n",
      "8236/8236 [==============================] - 1s 69us/step - loss: 0.7328 - acc: 0.5348 - f1: 0.5254 - val_loss: 0.6876 - val_acc: 0.5628 - val_f1: 0.5574\n",
      "Epoch 3/25\n",
      "8236/8236 [==============================] - 1s 68us/step - loss: 0.7031 - acc: 0.5645 - f1: 0.5609 - val_loss: 0.6890 - val_acc: 0.5295 - val_f1: 0.5257\n",
      "Epoch 4/25\n",
      "8236/8236 [==============================] - 1s 68us/step - loss: 0.6936 - acc: 0.5738 - f1: 0.5688 - val_loss: 0.6917 - val_acc: 0.5098 - val_f1: 0.5067\n",
      "Epoch 5/25\n",
      "8236/8236 [==============================] - 1s 68us/step - loss: 0.6835 - acc: 0.5820 - f1: 0.5769 - val_loss: 0.6899 - val_acc: 0.5267 - val_f1: 0.5281\n",
      "Epoch 6/25\n",
      "8236/8236 [==============================] - 1s 69us/step - loss: 0.6893 - acc: 0.5861 - f1: 0.5845 - val_loss: 0.6922 - val_acc: 0.5049 - val_f1: 0.5047\n",
      "Epoch 7/25\n",
      "8236/8236 [==============================] - 1s 73us/step - loss: 0.6756 - acc: 0.5973 - f1: 0.5964 - val_loss: 0.6899 - val_acc: 0.5197 - val_f1: 0.5195\n",
      "Epoch 8/25\n",
      "8236/8236 [==============================] - 1s 73us/step - loss: 0.6755 - acc: 0.6038 - f1: 0.6035 - val_loss: 0.6857 - val_acc: 0.5322 - val_f1: 0.5315\n",
      "Epoch 9/25\n",
      "8236/8236 [==============================] - 1s 92us/step - loss: 0.6704 - acc: 0.6023 - f1: 0.6011 - val_loss: 0.6889 - val_acc: 0.5180 - val_f1: 0.5178\n",
      "Epoch 10/25\n",
      "8236/8236 [==============================] - 1s 71us/step - loss: 0.6742 - acc: 0.6090 - f1: 0.6089 - val_loss: 0.6891 - val_acc: 0.5267 - val_f1: 0.5259\n",
      "Epoch 11/25\n",
      "8236/8236 [==============================] - 1s 67us/step - loss: 0.6636 - acc: 0.6187 - f1: 0.6186 - val_loss: 0.6846 - val_acc: 0.5398 - val_f1: 0.5396\n",
      "Epoch 12/25\n",
      "8236/8236 [==============================] - 1s 67us/step - loss: 0.6576 - acc: 0.6211 - f1: 0.6225 - val_loss: 0.6868 - val_acc: 0.5295 - val_f1: 0.5291\n",
      "Epoch 13/25\n",
      "8236/8236 [==============================] - 1s 68us/step - loss: 0.6552 - acc: 0.6251 - f1: 0.6265 - val_loss: 0.6842 - val_acc: 0.5426 - val_f1: 0.5440\n",
      "Epoch 14/25\n",
      "8236/8236 [==============================] - 1s 67us/step - loss: 0.6511 - acc: 0.6416 - f1: 0.6428 - val_loss: 0.6791 - val_acc: 0.5513 - val_f1: 0.5523\n",
      "Epoch 15/25\n",
      "8236/8236 [==============================] - 1s 68us/step - loss: 0.6510 - acc: 0.6456 - f1: 0.6465 - val_loss: 0.6756 - val_acc: 0.5584 - val_f1: 0.5587\n",
      "Epoch 16/25\n",
      "8236/8236 [==============================] - 1s 67us/step - loss: 0.6406 - acc: 0.6541 - f1: 0.6554 - val_loss: 0.6797 - val_acc: 0.5486 - val_f1: 0.5489\n",
      "Epoch 17/25\n",
      "8236/8236 [==============================] - 1s 69us/step - loss: 0.6407 - acc: 0.6587 - f1: 0.6597 - val_loss: 0.6698 - val_acc: 0.5764 - val_f1: 0.5764\n",
      "Epoch 18/25\n",
      "8236/8236 [==============================] - 1s 69us/step - loss: 0.6306 - acc: 0.6703 - f1: 0.6711 - val_loss: 0.6674 - val_acc: 0.5802 - val_f1: 0.5805\n",
      "Epoch 19/25\n",
      "8236/8236 [==============================] - 1s 68us/step - loss: 0.6125 - acc: 0.6792 - f1: 0.6802 - val_loss: 0.6695 - val_acc: 0.5819 - val_f1: 0.5819\n",
      "Epoch 20/25\n",
      "8236/8236 [==============================] - 1s 68us/step - loss: 0.6141 - acc: 0.6893 - f1: 0.6897 - val_loss: 0.6602 - val_acc: 0.6037 - val_f1: 0.6037\n",
      "Epoch 21/25\n",
      "8236/8236 [==============================] - 1s 69us/step - loss: 0.6078 - acc: 0.6945 - f1: 0.6950 - val_loss: 0.6538 - val_acc: 0.6021 - val_f1: 0.6019\n",
      "Epoch 22/25\n",
      "8236/8236 [==============================] - 1s 68us/step - loss: 0.5959 - acc: 0.7051 - f1: 0.7055 - val_loss: 0.6511 - val_acc: 0.6174 - val_f1: 0.6171\n",
      "Epoch 23/25\n",
      "8236/8236 [==============================] - 1s 69us/step - loss: 0.5788 - acc: 0.7176 - f1: 0.7181 - val_loss: 0.6489 - val_acc: 0.6168 - val_f1: 0.6164\n",
      "Epoch 24/25\n",
      "8236/8236 [==============================] - 1s 68us/step - loss: 0.5809 - acc: 0.7213 - f1: 0.7215 - val_loss: 0.6457 - val_acc: 0.6152 - val_f1: 0.6154\n",
      "Epoch 25/25\n",
      "8236/8236 [==============================] - 1s 68us/step - loss: 0.5787 - acc: 0.7207 - f1: 0.7210 - val_loss: 0.6459 - val_acc: 0.6141 - val_f1: 0.6143\n",
      "11\n",
      "(9889, 10001)\n",
      "(9889,)\n",
      "Baseline # : 3624\n",
      "Peak # : 6265\n",
      "x_train :  8900\n",
      "x_test  :  989\n",
      "y_train :  Counter({1: 5633, 0: 3267})\n",
      "y_test  :  Counter({1: 632, 0: 357})\n",
      "(10001,)\n",
      "(8900,)\n",
      "(989, 10001)\n",
      "(989,)\n",
      "Train on 8900 samples, validate on 989 samples\n",
      "Epoch 1/25\n",
      "8900/8900 [==============================] - 2s 220us/step - loss: 0.8068 - acc: 0.5486 - f1: 0.5878 - val_loss: 0.6776 - val_acc: 0.6416 - val_f1: 0.6383\n",
      "Epoch 2/25\n",
      "8900/8900 [==============================] - 1s 67us/step - loss: 0.7278 - acc: 0.5913 - f1: 0.6004 - val_loss: 0.6713 - val_acc: 0.6421 - val_f1: 0.6425\n",
      "Epoch 3/25\n",
      "8900/8900 [==============================] - 1s 68us/step - loss: 0.6947 - acc: 0.6216 - f1: 0.6242 - val_loss: 0.6648 - val_acc: 0.6385 - val_f1: 0.6384\n",
      "Epoch 4/25\n",
      "8900/8900 [==============================] - 1s 68us/step - loss: 0.6805 - acc: 0.6288 - f1: 0.6305 - val_loss: 0.6604 - val_acc: 0.6385 - val_f1: 0.6384\n",
      "Epoch 5/25\n",
      "8900/8900 [==============================] - 1s 69us/step - loss: 0.6739 - acc: 0.6326 - f1: 0.6350 - val_loss: 0.6566 - val_acc: 0.6395 - val_f1: 0.6394\n",
      "Epoch 6/25\n",
      "8900/8900 [==============================] - 1s 69us/step - loss: 0.6689 - acc: 0.6306 - f1: 0.6317 - val_loss: 0.6534 - val_acc: 0.6390 - val_f1: 0.6390\n",
      "Epoch 7/25\n",
      "8900/8900 [==============================] - 1s 68us/step - loss: 0.6615 - acc: 0.6412 - f1: 0.6414 - val_loss: 0.6529 - val_acc: 0.6380 - val_f1: 0.6380\n",
      "Epoch 8/25\n",
      "8900/8900 [==============================] - 1s 68us/step - loss: 0.6579 - acc: 0.6388 - f1: 0.6382 - val_loss: 0.6508 - val_acc: 0.6390 - val_f1: 0.6390\n",
      "Epoch 9/25\n",
      "8900/8900 [==============================] - 1s 68us/step - loss: 0.6536 - acc: 0.6402 - f1: 0.6409 - val_loss: 0.6505 - val_acc: 0.6380 - val_f1: 0.6380\n",
      "Epoch 10/25\n",
      "8900/8900 [==============================] - 1s 68us/step - loss: 0.6550 - acc: 0.6399 - f1: 0.6411 - val_loss: 0.6491 - val_acc: 0.6395 - val_f1: 0.6394\n",
      "Epoch 11/25\n",
      "8900/8900 [==============================] - 1s 69us/step - loss: 0.6496 - acc: 0.6412 - f1: 0.6418 - val_loss: 0.6485 - val_acc: 0.6390 - val_f1: 0.6390\n",
      "Epoch 12/25\n",
      "8900/8900 [==============================] - 1s 69us/step - loss: 0.6485 - acc: 0.6453 - f1: 0.6454 - val_loss: 0.6481 - val_acc: 0.6411 - val_f1: 0.6411\n",
      "Epoch 13/25\n",
      "8900/8900 [==============================] - 1s 68us/step - loss: 0.6441 - acc: 0.6476 - f1: 0.6473 - val_loss: 0.6444 - val_acc: 0.6400 - val_f1: 0.6400\n",
      "Epoch 14/25\n",
      "8900/8900 [==============================] - 1s 68us/step - loss: 0.6432 - acc: 0.6507 - f1: 0.6510 - val_loss: 0.6465 - val_acc: 0.6426 - val_f1: 0.6428\n",
      "Epoch 15/25\n",
      "8900/8900 [==============================] - 1s 67us/step - loss: 0.6340 - acc: 0.6575 - f1: 0.6578 - val_loss: 0.6444 - val_acc: 0.6421 - val_f1: 0.6420\n",
      "Epoch 16/25\n",
      "8900/8900 [==============================] - 1s 68us/step - loss: 0.6296 - acc: 0.6651 - f1: 0.6646 - val_loss: 0.6419 - val_acc: 0.6461 - val_f1: 0.6461\n",
      "Epoch 17/25\n",
      "8900/8900 [==============================] - 1s 68us/step - loss: 0.6208 - acc: 0.6681 - f1: 0.6678 - val_loss: 0.6394 - val_acc: 0.6446 - val_f1: 0.6449\n",
      "Epoch 18/25\n",
      "8900/8900 [==============================] - 1s 68us/step - loss: 0.6240 - acc: 0.6703 - f1: 0.6700 - val_loss: 0.6409 - val_acc: 0.6471 - val_f1: 0.6471\n",
      "Epoch 19/25\n",
      "8900/8900 [==============================] - 1s 67us/step - loss: 0.6161 - acc: 0.6729 - f1: 0.6725 - val_loss: 0.6423 - val_acc: 0.6451 - val_f1: 0.6451\n",
      "Epoch 20/25\n",
      "8900/8900 [==============================] - 1s 67us/step - loss: 0.6086 - acc: 0.6819 - f1: 0.6819 - val_loss: 0.6369 - val_acc: 0.6512 - val_f1: 0.6512\n",
      "Epoch 21/25\n",
      "8900/8900 [==============================] - 1s 68us/step - loss: 0.5986 - acc: 0.6914 - f1: 0.6914 - val_loss: 0.6365 - val_acc: 0.6486 - val_f1: 0.6488\n",
      "Epoch 22/25\n",
      "8900/8900 [==============================] - 1s 67us/step - loss: 0.5928 - acc: 0.6944 - f1: 0.6945 - val_loss: 0.6385 - val_acc: 0.6461 - val_f1: 0.6458\n",
      "Epoch 23/25\n",
      "8900/8900 [==============================] - 1s 67us/step - loss: 0.5824 - acc: 0.7040 - f1: 0.7042 - val_loss: 0.6326 - val_acc: 0.6547 - val_f1: 0.6544\n",
      "Epoch 24/25\n",
      "8900/8900 [==============================] - 1s 67us/step - loss: 0.5726 - acc: 0.7104 - f1: 0.7105 - val_loss: 0.6311 - val_acc: 0.6547 - val_f1: 0.6535\n",
      "Epoch 25/25\n",
      "8900/8900 [==============================] - 1s 67us/step - loss: 0.5657 - acc: 0.7206 - f1: 0.7205 - val_loss: 0.6345 - val_acc: 0.6598 - val_f1: 0.6592\n",
      "12\n",
      "(7248, 10001)\n",
      "(7248,)\n",
      "Baseline # : 2006\n",
      "Peak # : 5242\n",
      "x_train :  6523\n",
      "x_test  :  725\n",
      "y_train :  Counter({1: 4709, 0: 1814})\n",
      "y_test  :  Counter({1: 533, 0: 192})\n",
      "(10001,)\n",
      "(6523,)\n",
      "(725, 10001)\n",
      "(725,)\n",
      "Train on 6523 samples, validate on 725 samples\n",
      "Epoch 1/25\n",
      "6523/6523 [==============================] - 2s 299us/step - loss: 0.7991 - acc: 0.5915 - f1: 0.5619 - val_loss: 0.6552 - val_acc: 0.7345 - val_f1: 0.7346\n",
      "Epoch 2/25\n",
      "6523/6523 [==============================] - 0s 73us/step - loss: 0.7131 - acc: 0.6558 - f1: 0.6416 - val_loss: 0.6447 - val_acc: 0.7352 - val_f1: 0.7352\n",
      "Epoch 3/25\n",
      "6523/6523 [==============================] - 0s 72us/step - loss: 0.6804 - acc: 0.6889 - f1: 0.6809 - val_loss: 0.6307 - val_acc: 0.7352 - val_f1: 0.7352\n",
      "Epoch 4/25\n",
      "6523/6523 [==============================] - 0s 71us/step - loss: 0.6637 - acc: 0.7031 - f1: 0.6989 - val_loss: 0.6232 - val_acc: 0.7352 - val_f1: 0.7352\n",
      "Epoch 5/25\n",
      "6523/6523 [==============================] - 0s 71us/step - loss: 0.6456 - acc: 0.7167 - f1: 0.7141 - val_loss: 0.6103 - val_acc: 0.7352 - val_f1: 0.7352\n",
      "Epoch 6/25\n",
      "6523/6523 [==============================] - 0s 71us/step - loss: 0.6315 - acc: 0.7182 - f1: 0.7164 - val_loss: 0.6036 - val_acc: 0.7352 - val_f1: 0.7352\n",
      "Epoch 7/25\n",
      "6523/6523 [==============================] - 0s 72us/step - loss: 0.6251 - acc: 0.7205 - f1: 0.7198 - val_loss: 0.5975 - val_acc: 0.7352 - val_f1: 0.7352\n",
      "Epoch 8/25\n",
      "6523/6523 [==============================] - 1s 97us/step - loss: 0.6169 - acc: 0.7205 - f1: 0.7199 - val_loss: 0.5893 - val_acc: 0.7352 - val_f1: 0.7352\n",
      "Epoch 9/25\n",
      "6523/6523 [==============================] - 1s 110us/step - loss: 0.6139 - acc: 0.7193 - f1: 0.7193 - val_loss: 0.5875 - val_acc: 0.7352 - val_f1: 0.7352\n",
      "Epoch 10/25\n",
      "6523/6523 [==============================] - 0s 68us/step - loss: 0.6062 - acc: 0.7203 - f1: 0.7205 - val_loss: 0.5835 - val_acc: 0.7352 - val_f1: 0.7352\n",
      "Epoch 11/25\n",
      "6523/6523 [==============================] - 0s 68us/step - loss: 0.6016 - acc: 0.7201 - f1: 0.7204 - val_loss: 0.5799 - val_acc: 0.7352 - val_f1: 0.7352\n",
      "Epoch 12/25\n",
      "6523/6523 [==============================] - 0s 69us/step - loss: 0.5971 - acc: 0.7222 - f1: 0.7222 - val_loss: 0.5782 - val_acc: 0.7352 - val_f1: 0.7352\n",
      "Epoch 13/25\n",
      "6523/6523 [==============================] - 0s 70us/step - loss: 0.5944 - acc: 0.7218 - f1: 0.7215 - val_loss: 0.5765 - val_acc: 0.7352 - val_f1: 0.7352\n",
      "Epoch 14/25\n",
      "6523/6523 [==============================] - 0s 69us/step - loss: 0.5915 - acc: 0.7225 - f1: 0.7224 - val_loss: 0.5753 - val_acc: 0.7352 - val_f1: 0.7352\n",
      "Epoch 15/25\n",
      "6523/6523 [==============================] - 0s 69us/step - loss: 0.5933 - acc: 0.7214 - f1: 0.7212 - val_loss: 0.5751 - val_acc: 0.7352 - val_f1: 0.7352\n",
      "Epoch 16/25\n",
      "6523/6523 [==============================] - 0s 69us/step - loss: 0.5872 - acc: 0.7211 - f1: 0.7210 - val_loss: 0.5717 - val_acc: 0.7352 - val_f1: 0.7352\n",
      "Epoch 17/25\n",
      "6523/6523 [==============================] - 0s 69us/step - loss: 0.5801 - acc: 0.7234 - f1: 0.7231 - val_loss: 0.5690 - val_acc: 0.7352 - val_f1: 0.7352\n",
      "Epoch 18/25\n",
      "6523/6523 [==============================] - 0s 69us/step - loss: 0.5785 - acc: 0.7201 - f1: 0.7198 - val_loss: 0.5705 - val_acc: 0.7352 - val_f1: 0.7352\n",
      "Epoch 19/25\n",
      "6523/6523 [==============================] - 0s 68us/step - loss: 0.5697 - acc: 0.7236 - f1: 0.7235 - val_loss: 0.5671 - val_acc: 0.7352 - val_f1: 0.7352\n",
      "Epoch 20/25\n",
      "6523/6523 [==============================] - 0s 68us/step - loss: 0.5671 - acc: 0.7234 - f1: 0.7231 - val_loss: 0.5665 - val_acc: 0.7352 - val_f1: 0.7352\n",
      "Epoch 21/25\n",
      "6523/6523 [==============================] - 0s 70us/step - loss: 0.5594 - acc: 0.7260 - f1: 0.7258 - val_loss: 0.5677 - val_acc: 0.7352 - val_f1: 0.7352\n",
      "Epoch 22/25\n",
      "6523/6523 [==============================] - 0s 74us/step - loss: 0.5443 - acc: 0.7253 - f1: 0.7251 - val_loss: 0.5693 - val_acc: 0.7352 - val_f1: 0.7352\n",
      "Epoch 23/25\n",
      "6523/6523 [==============================] - 0s 69us/step - loss: 0.5411 - acc: 0.7249 - f1: 0.7246 - val_loss: 0.5678 - val_acc: 0.7352 - val_f1: 0.7352\n",
      "Epoch 24/25\n",
      "6523/6523 [==============================] - 0s 68us/step - loss: 0.5287 - acc: 0.7254 - f1: 0.7256 - val_loss: 0.5830 - val_acc: 0.7352 - val_f1: 0.7352\n",
      "Epoch 25/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6523/6523 [==============================] - 0s 76us/step - loss: 0.5229 - acc: 0.7275 - f1: 0.7276 - val_loss: 0.5892 - val_acc: 0.7352 - val_f1: 0.7352\n",
      "13\n",
      "(8446, 10001)\n",
      "(8446,)\n",
      "Baseline # : 2740\n",
      "Peak # : 5706\n",
      "x_train :  7601\n",
      "x_test  :  845\n",
      "y_train :  Counter({1: 5125, 0: 2476})\n",
      "y_test  :  Counter({1: 581, 0: 264})\n",
      "(10001,)\n",
      "(7601,)\n",
      "(845, 10001)\n",
      "(845,)\n",
      "Train on 7601 samples, validate on 845 samples\n",
      "Epoch 1/25\n",
      "7601/7601 [==============================] - 2s 275us/step - loss: 0.7961 - acc: 0.5339 - f1: 0.5542 - val_loss: 0.6710 - val_acc: 0.6888 - val_f1: 0.6881\n",
      "Epoch 2/25\n",
      "7601/7601 [==============================] - 1s 68us/step - loss: 0.7095 - acc: 0.6169 - f1: 0.6238 - val_loss: 0.6566 - val_acc: 0.6882 - val_f1: 0.6880\n",
      "Epoch 3/25\n",
      "7601/7601 [==============================] - 1s 68us/step - loss: 0.6873 - acc: 0.6470 - f1: 0.6497 - val_loss: 0.6459 - val_acc: 0.6876 - val_f1: 0.6876\n",
      "Epoch 4/25\n",
      "7601/7601 [==============================] - 1s 68us/step - loss: 0.6693 - acc: 0.6602 - f1: 0.6641 - val_loss: 0.6393 - val_acc: 0.6876 - val_f1: 0.6876\n",
      "Epoch 5/25\n",
      "7601/7601 [==============================] - 1s 69us/step - loss: 0.6591 - acc: 0.6681 - f1: 0.6701 - val_loss: 0.6323 - val_acc: 0.6876 - val_f1: 0.6876\n",
      "Epoch 6/25\n",
      "7601/7601 [==============================] - 1s 68us/step - loss: 0.6527 - acc: 0.6680 - f1: 0.6696 - val_loss: 0.6284 - val_acc: 0.6876 - val_f1: 0.6876\n",
      "Epoch 7/25\n",
      "7601/7601 [==============================] - 1s 69us/step - loss: 0.6463 - acc: 0.6704 - f1: 0.6712 - val_loss: 0.6252 - val_acc: 0.6876 - val_f1: 0.6876\n",
      "Epoch 8/25\n",
      "7601/7601 [==============================] - 1s 83us/step - loss: 0.6442 - acc: 0.6731 - f1: 0.6739 - val_loss: 0.6236 - val_acc: 0.6876 - val_f1: 0.6876\n",
      "Epoch 9/25\n",
      "7601/7601 [==============================] - 1s 84us/step - loss: 0.6410 - acc: 0.6729 - f1: 0.6737 - val_loss: 0.6221 - val_acc: 0.6876 - val_f1: 0.6876\n",
      "Epoch 10/25\n",
      "7601/7601 [==============================] - 1s 69us/step - loss: 0.6386 - acc: 0.6730 - f1: 0.6734 - val_loss: 0.6199 - val_acc: 0.6876 - val_f1: 0.6876\n",
      "Epoch 11/25\n",
      "7601/7601 [==============================] - 1s 70us/step - loss: 0.6353 - acc: 0.6729 - f1: 0.6730 - val_loss: 0.6197 - val_acc: 0.6876 - val_f1: 0.6876\n",
      "Epoch 12/25\n",
      "7601/7601 [==============================] - 1s 69us/step - loss: 0.6329 - acc: 0.6733 - f1: 0.6737 - val_loss: 0.6185 - val_acc: 0.6876 - val_f1: 0.6876\n",
      "Epoch 13/25\n",
      "7601/7601 [==============================] - 1s 68us/step - loss: 0.6340 - acc: 0.6729 - f1: 0.6732 - val_loss: 0.6184 - val_acc: 0.6876 - val_f1: 0.6876\n",
      "Epoch 14/25\n",
      "7601/7601 [==============================] - 1s 69us/step - loss: 0.6323 - acc: 0.6729 - f1: 0.6738 - val_loss: 0.6161 - val_acc: 0.6870 - val_f1: 0.6872\n",
      "Epoch 15/25\n",
      "7601/7601 [==============================] - 1s 67us/step - loss: 0.6322 - acc: 0.6733 - f1: 0.6738 - val_loss: 0.6162 - val_acc: 0.6876 - val_f1: 0.6876\n",
      "Epoch 16/25\n",
      "7601/7601 [==============================] - 1s 71us/step - loss: 0.6322 - acc: 0.6725 - f1: 0.6726 - val_loss: 0.6142 - val_acc: 0.6876 - val_f1: 0.6876\n",
      "Epoch 17/25\n",
      "7601/7601 [==============================] - 1s 67us/step - loss: 0.6306 - acc: 0.6737 - f1: 0.6737 - val_loss: 0.6137 - val_acc: 0.6876 - val_f1: 0.6876\n",
      "Epoch 18/25\n",
      "7601/7601 [==============================] - 1s 68us/step - loss: 0.6276 - acc: 0.6744 - f1: 0.6745 - val_loss: 0.6158 - val_acc: 0.6876 - val_f1: 0.6876\n",
      "Epoch 19/25\n",
      "7601/7601 [==============================] - 1s 68us/step - loss: 0.6284 - acc: 0.6725 - f1: 0.6727 - val_loss: 0.6162 - val_acc: 0.6876 - val_f1: 0.6876\n",
      "Epoch 20/25\n",
      "7601/7601 [==============================] - 1s 68us/step - loss: 0.6279 - acc: 0.6742 - f1: 0.6746 - val_loss: 0.6160 - val_acc: 0.6876 - val_f1: 0.6876\n",
      "Epoch 21/25\n",
      "7601/7601 [==============================] - 1s 68us/step - loss: 0.6277 - acc: 0.6750 - f1: 0.6755 - val_loss: 0.6145 - val_acc: 0.6876 - val_f1: 0.6876\n",
      "Epoch 22/25\n",
      "7601/7601 [==============================] - 1s 68us/step - loss: 0.6258 - acc: 0.6760 - f1: 0.6763 - val_loss: 0.6142 - val_acc: 0.6876 - val_f1: 0.6876\n",
      "Epoch 23/25\n",
      "7601/7601 [==============================] - 1s 69us/step - loss: 0.6249 - acc: 0.6756 - f1: 0.6756 - val_loss: 0.6123 - val_acc: 0.6864 - val_f1: 0.6864\n",
      "Epoch 24/25\n",
      "7601/7601 [==============================] - 1s 72us/step - loss: 0.6285 - acc: 0.6710 - f1: 0.6712 - val_loss: 0.6118 - val_acc: 0.6864 - val_f1: 0.6864\n",
      "Epoch 25/25\n",
      "7601/7601 [==============================] - 1s 70us/step - loss: 0.6177 - acc: 0.6777 - f1: 0.6782 - val_loss: 0.6109 - val_acc: 0.6870 - val_f1: 0.6868\n",
      "14\n",
      "(9210, 10001)\n",
      "(9210,)\n",
      "Baseline # : 3243\n",
      "Peak # : 5967\n",
      "x_train :  8289\n",
      "x_test  :  921\n",
      "y_train :  Counter({1: 5381, 0: 2908})\n",
      "y_test  :  Counter({1: 586, 0: 335})\n",
      "(10001,)\n",
      "(8289,)\n",
      "(921, 10001)\n",
      "(921,)\n",
      "Train on 8289 samples, validate on 921 samples\n",
      "Epoch 1/25\n",
      "8289/8289 [==============================] - 2s 267us/step - loss: 0.8094 - acc: 0.5542 - f1: 0.5605 - val_loss: 0.7527 - val_acc: 0.6357 - val_f1: 0.6373\n",
      "Epoch 2/25\n",
      "8289/8289 [==============================] - 1s 69us/step - loss: 0.7476 - acc: 0.6130 - f1: 0.6157 - val_loss: 0.7442 - val_acc: 0.6374 - val_f1: 0.6374\n",
      "Epoch 3/25\n",
      "8289/8289 [==============================] - 1s 69us/step - loss: 0.7305 - acc: 0.6381 - f1: 0.6390 - val_loss: 0.7307 - val_acc: 0.6379 - val_f1: 0.6377\n",
      "Epoch 4/25\n",
      "8289/8289 [==============================] - 1s 68us/step - loss: 0.7056 - acc: 0.6465 - f1: 0.6447 - val_loss: 0.7219 - val_acc: 0.6374 - val_f1: 0.6377\n",
      "Epoch 5/25\n",
      "8289/8289 [==============================] - 1s 68us/step - loss: 0.6997 - acc: 0.6526 - f1: 0.6514 - val_loss: 0.6928 - val_acc: 0.6390 - val_f1: 0.6392\n",
      "Epoch 6/25\n",
      "8289/8289 [==============================] - 1s 68us/step - loss: 0.6858 - acc: 0.6554 - f1: 0.6544 - val_loss: 0.7344 - val_acc: 0.6363 - val_f1: 0.6363\n",
      "Epoch 7/25\n",
      "8289/8289 [==============================] - 1s 70us/step - loss: 0.6754 - acc: 0.6565 - f1: 0.6546 - val_loss: 0.6638 - val_acc: 0.6406 - val_f1: 0.6406\n",
      "Epoch 8/25\n",
      "8289/8289 [==============================] - 1s 69us/step - loss: 0.6681 - acc: 0.6564 - f1: 0.6547 - val_loss: 0.6566 - val_acc: 0.6406 - val_f1: 0.6406\n",
      "Epoch 9/25\n",
      "8289/8289 [==============================] - 1s 69us/step - loss: 0.6658 - acc: 0.6545 - f1: 0.6525 - val_loss: 0.6535 - val_acc: 0.6412 - val_f1: 0.6414\n",
      "Epoch 10/25\n",
      "8289/8289 [==============================] - 1s 69us/step - loss: 0.6605 - acc: 0.6551 - f1: 0.6528 - val_loss: 0.6520 - val_acc: 0.6412 - val_f1: 0.6414\n",
      "Epoch 11/25\n",
      "8289/8289 [==============================] - 1s 69us/step - loss: 0.6520 - acc: 0.6600 - f1: 0.6576 - val_loss: 0.6456 - val_acc: 0.6417 - val_f1: 0.6417\n",
      "Epoch 12/25\n",
      "8289/8289 [==============================] - 1s 69us/step - loss: 0.6452 - acc: 0.6600 - f1: 0.6581 - val_loss: 0.6421 - val_acc: 0.6417 - val_f1: 0.6417\n",
      "Epoch 13/25\n",
      "8289/8289 [==============================] - 1s 68us/step - loss: 0.6427 - acc: 0.6631 - f1: 0.6614 - val_loss: 0.6427 - val_acc: 0.6401 - val_f1: 0.6399\n",
      "Epoch 14/25\n",
      "8289/8289 [==============================] - 1s 69us/step - loss: 0.6427 - acc: 0.6625 - f1: 0.6604 - val_loss: 0.6428 - val_acc: 0.6412 - val_f1: 0.6409\n",
      "Epoch 15/25\n",
      "8289/8289 [==============================] - 1s 69us/step - loss: 0.6360 - acc: 0.6680 - f1: 0.6659 - val_loss: 0.6338 - val_acc: 0.6439 - val_f1: 0.6435\n",
      "Epoch 16/25\n",
      "8289/8289 [==============================] - 1s 68us/step - loss: 0.6262 - acc: 0.6740 - f1: 0.6723 - val_loss: 0.6315 - val_acc: 0.6450 - val_f1: 0.6445\n",
      "Epoch 17/25\n",
      "8289/8289 [==============================] - 1s 68us/step - loss: 0.6261 - acc: 0.6738 - f1: 0.6731 - val_loss: 0.6302 - val_acc: 0.6439 - val_f1: 0.6438\n",
      "Epoch 18/25\n",
      "8289/8289 [==============================] - 1s 73us/step - loss: 0.6252 - acc: 0.6752 - f1: 0.6741 - val_loss: 0.6334 - val_acc: 0.6450 - val_f1: 0.6450\n",
      "Epoch 19/25\n",
      "8289/8289 [==============================] - 1s 72us/step - loss: 0.6133 - acc: 0.6829 - f1: 0.6824 - val_loss: 0.6279 - val_acc: 0.6455 - val_f1: 0.6449\n",
      "Epoch 20/25\n",
      "8289/8289 [==============================] - 1s 70us/step - loss: 0.6107 - acc: 0.6888 - f1: 0.6882 - val_loss: 0.6215 - val_acc: 0.6498 - val_f1: 0.6493\n",
      "Epoch 21/25\n",
      "8289/8289 [==============================] - 1s 69us/step - loss: 0.6088 - acc: 0.6957 - f1: 0.6951 - val_loss: 0.6141 - val_acc: 0.6504 - val_f1: 0.6500\n",
      "Epoch 22/25\n",
      "8289/8289 [==============================] - 1s 70us/step - loss: 0.6007 - acc: 0.6977 - f1: 0.6973 - val_loss: 0.6206 - val_acc: 0.6504 - val_f1: 0.6501\n",
      "Epoch 23/25\n",
      "8289/8289 [==============================] - 1s 69us/step - loss: 0.5908 - acc: 0.7025 - f1: 0.7024 - val_loss: 0.6175 - val_acc: 0.6558 - val_f1: 0.6558\n",
      "Epoch 24/25\n",
      "8289/8289 [==============================] - 1s 70us/step - loss: 0.5958 - acc: 0.7071 - f1: 0.7069 - val_loss: 0.6168 - val_acc: 0.6509 - val_f1: 0.6504\n",
      "Epoch 25/25\n",
      "8289/8289 [==============================] - 1s 70us/step - loss: 0.5798 - acc: 0.7112 - f1: 0.7111 - val_loss: 0.6256 - val_acc: 0.6531 - val_f1: 0.6526\n",
      "15\n",
      "(10222, 10001)\n",
      "(10222,)\n",
      "Baseline # : 4407\n",
      "Peak # : 5815\n",
      "x_train :  9199\n",
      "x_test  :  1023\n",
      "y_train :  Counter({1: 5230, 0: 3969})\n",
      "y_test  :  Counter({1: 585, 0: 438})\n",
      "(10001,)\n",
      "(9199,)\n",
      "(1023, 10001)\n",
      "(1023,)\n",
      "Train on 9199 samples, validate on 1023 samples\n",
      "Epoch 1/25\n",
      "9199/9199 [==============================] - 2s 259us/step - loss: 0.8660 - acc: 0.5261 - f1: 0.5416 - val_loss: 0.7191 - val_acc: 0.5728 - val_f1: 0.5870\n",
      "Epoch 2/25\n",
      "9199/9199 [==============================] - 1s 69us/step - loss: 0.7946 - acc: 0.5452 - f1: 0.5602 - val_loss: 0.7245 - val_acc: 0.5777 - val_f1: 0.5786\n",
      "Epoch 3/25\n",
      "9199/9199 [==============================] - 1s 69us/step - loss: 0.7587 - acc: 0.5580 - f1: 0.5665 - val_loss: 0.7140 - val_acc: 0.5723 - val_f1: 0.5730\n",
      "Epoch 4/25\n",
      "9199/9199 [==============================] - 1s 69us/step - loss: 0.7497 - acc: 0.5667 - f1: 0.5723 - val_loss: 0.7056 - val_acc: 0.5748 - val_f1: 0.5748\n",
      "Epoch 5/25\n",
      "9199/9199 [==============================] - 1s 71us/step - loss: 0.7351 - acc: 0.5743 - f1: 0.5808 - val_loss: 0.6973 - val_acc: 0.5748 - val_f1: 0.5744\n",
      "Epoch 6/25\n",
      "9199/9199 [==============================] - 1s 93us/step - loss: 0.7218 - acc: 0.5796 - f1: 0.5848 - val_loss: 0.7031 - val_acc: 0.5748 - val_f1: 0.5752\n",
      "Epoch 7/25\n",
      "9199/9199 [==============================] - 1s 74us/step - loss: 0.7164 - acc: 0.5760 - f1: 0.5806 - val_loss: 0.6789 - val_acc: 0.5777 - val_f1: 0.5785\n",
      "Epoch 8/25\n",
      "9199/9199 [==============================] - 1s 70us/step - loss: 0.7072 - acc: 0.5781 - f1: 0.5823 - val_loss: 0.6859 - val_acc: 0.5772 - val_f1: 0.5778\n",
      "Epoch 9/25\n",
      "9199/9199 [==============================] - 1s 69us/step - loss: 0.6985 - acc: 0.5785 - f1: 0.5825 - val_loss: 0.6783 - val_acc: 0.5777 - val_f1: 0.5781\n",
      "Epoch 10/25\n",
      "9199/9199 [==============================] - 1s 69us/step - loss: 0.6981 - acc: 0.5811 - f1: 0.5856 - val_loss: 0.6778 - val_acc: 0.5782 - val_f1: 0.5784\n",
      "Epoch 11/25\n",
      "9199/9199 [==============================] - 1s 68us/step - loss: 0.6934 - acc: 0.5805 - f1: 0.5836 - val_loss: 0.6770 - val_acc: 0.5767 - val_f1: 0.5775\n",
      "Epoch 12/25\n",
      "9199/9199 [==============================] - 1s 69us/step - loss: 0.6895 - acc: 0.5805 - f1: 0.5840 - val_loss: 0.6762 - val_acc: 0.5787 - val_f1: 0.5794\n",
      "Epoch 13/25\n",
      "9199/9199 [==============================] - 1s 69us/step - loss: 0.6858 - acc: 0.5898 - f1: 0.5925 - val_loss: 0.6773 - val_acc: 0.5767 - val_f1: 0.5767\n",
      "Epoch 14/25\n",
      "9199/9199 [==============================] - 1s 70us/step - loss: 0.6821 - acc: 0.5883 - f1: 0.5897 - val_loss: 0.6766 - val_acc: 0.5772 - val_f1: 0.5775\n",
      "Epoch 15/25\n",
      "9199/9199 [==============================] - 1s 69us/step - loss: 0.6797 - acc: 0.5964 - f1: 0.5975 - val_loss: 0.6758 - val_acc: 0.5806 - val_f1: 0.5811\n",
      "Epoch 16/25\n",
      "9199/9199 [==============================] - 1s 68us/step - loss: 0.6772 - acc: 0.5994 - f1: 0.6001 - val_loss: 0.6735 - val_acc: 0.5836 - val_f1: 0.5840\n",
      "Epoch 17/25\n",
      "9199/9199 [==============================] - 1s 68us/step - loss: 0.6692 - acc: 0.6056 - f1: 0.6061 - val_loss: 0.6719 - val_acc: 0.5826 - val_f1: 0.5829\n",
      "Epoch 18/25\n",
      "9199/9199 [==============================] - 1s 68us/step - loss: 0.6660 - acc: 0.6118 - f1: 0.6117 - val_loss: 0.6741 - val_acc: 0.5821 - val_f1: 0.5823\n",
      "Epoch 19/25\n",
      "9199/9199 [==============================] - 1s 68us/step - loss: 0.6600 - acc: 0.6206 - f1: 0.6200 - val_loss: 0.6711 - val_acc: 0.5846 - val_f1: 0.5850\n",
      "Epoch 20/25\n",
      "9199/9199 [==============================] - 1s 68us/step - loss: 0.6564 - acc: 0.6277 - f1: 0.6276 - val_loss: 0.6698 - val_acc: 0.5865 - val_f1: 0.5861\n",
      "Epoch 21/25\n",
      "9199/9199 [==============================] - 1s 68us/step - loss: 0.6536 - acc: 0.6307 - f1: 0.6308 - val_loss: 0.6711 - val_acc: 0.5875 - val_f1: 0.5875\n",
      "Epoch 22/25\n",
      "9199/9199 [==============================] - 1s 69us/step - loss: 0.6476 - acc: 0.6381 - f1: 0.6383 - val_loss: 0.6714 - val_acc: 0.5880 - val_f1: 0.5874\n",
      "Epoch 23/25\n",
      "9199/9199 [==============================] - 1s 70us/step - loss: 0.6417 - acc: 0.6518 - f1: 0.6519 - val_loss: 0.6733 - val_acc: 0.5836 - val_f1: 0.5832\n",
      "Epoch 24/25\n",
      "9199/9199 [==============================] - 1s 77us/step - loss: 0.6368 - acc: 0.6608 - f1: 0.6610 - val_loss: 0.6746 - val_acc: 0.5816 - val_f1: 0.5816\n",
      "Epoch 25/25\n",
      "9199/9199 [==============================] - 1s 70us/step - loss: 0.6271 - acc: 0.6571 - f1: 0.6573 - val_loss: 0.6762 - val_acc: 0.5821 - val_f1: 0.5819\n",
      "16\n",
      "(9273, 10001)\n",
      "(9273,)\n",
      "Baseline # : 4727\n",
      "Peak # : 4546\n",
      "x_train :  8345\n",
      "x_test  :  928\n",
      "y_train :  Counter({0: 4258, 1: 4087})\n",
      "y_test  :  Counter({0: 469, 1: 459})\n",
      "(10001,)\n",
      "(8345,)\n",
      "(928, 10001)\n",
      "(928,)\n",
      "Train on 8345 samples, validate on 928 samples\n",
      "Epoch 1/25\n",
      "8345/8345 [==============================] - 3s 301us/step - loss: 0.7581 - acc: 0.5028 - f1: 0.4814 - val_loss: 0.6929 - val_acc: 0.5199 - val_f1: 0.5160\n",
      "Epoch 2/25\n",
      "8345/8345 [==============================] - 1s 72us/step - loss: 0.7099 - acc: 0.5213 - f1: 0.5090 - val_loss: 0.6920 - val_acc: 0.5199 - val_f1: 0.5064\n",
      "Epoch 3/25\n",
      "8345/8345 [==============================] - 1s 70us/step - loss: 0.6966 - acc: 0.5299 - f1: 0.5161 - val_loss: 0.6907 - val_acc: 0.5140 - val_f1: 0.5097\n",
      "Epoch 4/25\n",
      "8345/8345 [==============================] - 1s 70us/step - loss: 0.6952 - acc: 0.5262 - f1: 0.5206 - val_loss: 0.6900 - val_acc: 0.5199 - val_f1: 0.5177\n",
      "Epoch 5/25\n",
      "8345/8345 [==============================] - 1s 69us/step - loss: 0.6831 - acc: 0.5552 - f1: 0.5581 - val_loss: 0.6882 - val_acc: 0.5334 - val_f1: 0.5325\n",
      "Epoch 6/25\n",
      "8345/8345 [==============================] - 1s 75us/step - loss: 0.6821 - acc: 0.5603 - f1: 0.5636 - val_loss: 0.6878 - val_acc: 0.5318 - val_f1: 0.5272\n",
      "Epoch 7/25\n",
      "8345/8345 [==============================] - 1s 82us/step - loss: 0.6783 - acc: 0.5690 - f1: 0.5718 - val_loss: 0.6843 - val_acc: 0.5555 - val_f1: 0.5553\n",
      "Epoch 8/25\n",
      "8345/8345 [==============================] - 1s 86us/step - loss: 0.6697 - acc: 0.5897 - f1: 0.5951 - val_loss: 0.6806 - val_acc: 0.5566 - val_f1: 0.5558\n",
      "Epoch 9/25\n",
      "8345/8345 [==============================] - 1s 69us/step - loss: 0.6719 - acc: 0.5805 - f1: 0.5855 - val_loss: 0.6808 - val_acc: 0.5544 - val_f1: 0.5537\n",
      "Epoch 10/25\n",
      "8345/8345 [==============================] - 1s 70us/step - loss: 0.6666 - acc: 0.5929 - f1: 0.5958 - val_loss: 0.6769 - val_acc: 0.5598 - val_f1: 0.5605\n",
      "Epoch 11/25\n",
      "8345/8345 [==============================] - 1s 69us/step - loss: 0.6523 - acc: 0.6152 - f1: 0.6178 - val_loss: 0.6735 - val_acc: 0.5668 - val_f1: 0.5668\n",
      "Epoch 12/25\n",
      "8345/8345 [==============================] - 1s 69us/step - loss: 0.6504 - acc: 0.6196 - f1: 0.6202 - val_loss: 0.6770 - val_acc: 0.5614 - val_f1: 0.5610\n",
      "Epoch 13/25\n",
      "8345/8345 [==============================] - 1s 70us/step - loss: 0.6444 - acc: 0.6303 - f1: 0.6313 - val_loss: 0.6732 - val_acc: 0.5641 - val_f1: 0.5634\n",
      "Epoch 14/25\n",
      "8345/8345 [==============================] - 1s 70us/step - loss: 0.6346 - acc: 0.6445 - f1: 0.6455 - val_loss: 0.6724 - val_acc: 0.5690 - val_f1: 0.5680\n",
      "Epoch 15/25\n",
      "8345/8345 [==============================] - 1s 71us/step - loss: 0.6221 - acc: 0.6608 - f1: 0.6618 - val_loss: 0.6665 - val_acc: 0.5857 - val_f1: 0.5850\n",
      "Epoch 16/25\n",
      "8345/8345 [==============================] - 1s 69us/step - loss: 0.6170 - acc: 0.6653 - f1: 0.6657 - val_loss: 0.6587 - val_acc: 0.6045 - val_f1: 0.6037\n",
      "Epoch 17/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8345/8345 [==============================] - 1s 68us/step - loss: 0.6084 - acc: 0.6835 - f1: 0.6834 - val_loss: 0.6507 - val_acc: 0.5975 - val_f1: 0.5973\n",
      "Epoch 18/25\n",
      "8345/8345 [==============================] - 1s 69us/step - loss: 0.5956 - acc: 0.6860 - f1: 0.6864 - val_loss: 0.6587 - val_acc: 0.5954 - val_f1: 0.5951\n",
      "Epoch 19/25\n",
      "8345/8345 [==============================] - 1s 69us/step - loss: 0.5817 - acc: 0.7074 - f1: 0.7074 - val_loss: 0.6592 - val_acc: 0.6008 - val_f1: 0.6001\n",
      "Epoch 20/25\n",
      "8345/8345 [==============================] - 1s 71us/step - loss: 0.5758 - acc: 0.7104 - f1: 0.7105 - val_loss: 0.6555 - val_acc: 0.5900 - val_f1: 0.5897\n",
      "Epoch 21/25\n",
      "8345/8345 [==============================] - 1s 71us/step - loss: 0.5622 - acc: 0.7298 - f1: 0.7298 - val_loss: 0.6625 - val_acc: 0.5943 - val_f1: 0.5936\n",
      "Epoch 22/25\n",
      "8345/8345 [==============================] - 1s 76us/step - loss: 0.5456 - acc: 0.7330 - f1: 0.7329 - val_loss: 0.6648 - val_acc: 0.6029 - val_f1: 0.6022\n",
      "Epoch 23/25\n",
      "8345/8345 [==============================] - 1s 90us/step - loss: 0.5347 - acc: 0.7460 - f1: 0.7461 - val_loss: 0.6632 - val_acc: 0.6051 - val_f1: 0.6040\n",
      "Epoch 24/25\n",
      "8345/8345 [==============================] - 1s 74us/step - loss: 0.5330 - acc: 0.7528 - f1: 0.7527 - val_loss: 0.6610 - val_acc: 0.5916 - val_f1: 0.5911\n",
      "Epoch 25/25\n",
      "8345/8345 [==============================] - 1s 70us/step - loss: 0.5083 - acc: 0.7658 - f1: 0.7659 - val_loss: 0.6626 - val_acc: 0.6008 - val_f1: 0.6005\n",
      "17\n",
      "(9031, 10001)\n",
      "(9031,)\n",
      "Baseline # : 4368\n",
      "Peak # : 4663\n",
      "x_train :  8127\n",
      "x_test  :  904\n",
      "y_train :  Counter({1: 4189, 0: 3938})\n",
      "y_test  :  Counter({1: 474, 0: 430})\n",
      "(10001,)\n",
      "(8127,)\n",
      "(904, 10001)\n",
      "(904,)\n",
      "Train on 8127 samples, validate on 904 samples\n",
      "Epoch 1/25\n",
      "8127/8127 [==============================] - 3s 317us/step - loss: 0.8207 - acc: 0.4953 - f1: 0.4867 - val_loss: 0.6886 - val_acc: 0.5404 - val_f1: 0.4707\n",
      "Epoch 2/25\n",
      "8127/8127 [==============================] - 1s 69us/step - loss: 0.7474 - acc: 0.5142 - f1: 0.4968 - val_loss: 0.6904 - val_acc: 0.5426 - val_f1: 0.5050\n",
      "Epoch 3/25\n",
      "8127/8127 [==============================] - 1s 68us/step - loss: 0.7164 - acc: 0.5314 - f1: 0.5180 - val_loss: 0.6912 - val_acc: 0.5376 - val_f1: 0.5284\n",
      "Epoch 4/25\n",
      "8127/8127 [==============================] - 1s 68us/step - loss: 0.7077 - acc: 0.5323 - f1: 0.5210 - val_loss: 0.6894 - val_acc: 0.5454 - val_f1: 0.5389\n",
      "Epoch 5/25\n",
      "8127/8127 [==============================] - 1s 68us/step - loss: 0.6974 - acc: 0.5416 - f1: 0.5336 - val_loss: 0.6894 - val_acc: 0.5420 - val_f1: 0.5385\n",
      "Epoch 6/25\n",
      "8127/8127 [==============================] - 1s 76us/step - loss: 0.6997 - acc: 0.5421 - f1: 0.5363 - val_loss: 0.6897 - val_acc: 0.5360 - val_f1: 0.5337\n",
      "Epoch 7/25\n",
      "8127/8127 [==============================] - 1s 91us/step - loss: 0.6921 - acc: 0.5473 - f1: 0.5431 - val_loss: 0.6886 - val_acc: 0.5393 - val_f1: 0.5395\n",
      "Epoch 8/25\n",
      "8127/8127 [==============================] - 1s 70us/step - loss: 0.6917 - acc: 0.5596 - f1: 0.5582 - val_loss: 0.6878 - val_acc: 0.5476 - val_f1: 0.5465\n",
      "Epoch 9/25\n",
      "8127/8127 [==============================] - 1s 69us/step - loss: 0.6872 - acc: 0.5571 - f1: 0.5559 - val_loss: 0.6886 - val_acc: 0.5376 - val_f1: 0.5371\n",
      "Epoch 10/25\n",
      "8127/8127 [==============================] - 1s 69us/step - loss: 0.6880 - acc: 0.5580 - f1: 0.5579 - val_loss: 0.6906 - val_acc: 0.5299 - val_f1: 0.5294\n",
      "Epoch 11/25\n",
      "8127/8127 [==============================] - 1s 68us/step - loss: 0.6901 - acc: 0.5559 - f1: 0.5529 - val_loss: 0.6897 - val_acc: 0.5332 - val_f1: 0.5327\n",
      "Epoch 12/25\n",
      "8127/8127 [==============================] - 1s 68us/step - loss: 0.6824 - acc: 0.5770 - f1: 0.5745 - val_loss: 0.6877 - val_acc: 0.5360 - val_f1: 0.5351\n",
      "Epoch 13/25\n",
      "8127/8127 [==============================] - 1s 68us/step - loss: 0.6855 - acc: 0.5601 - f1: 0.5588 - val_loss: 0.6881 - val_acc: 0.5315 - val_f1: 0.5318\n",
      "Epoch 14/25\n",
      "8127/8127 [==============================] - 1s 70us/step - loss: 0.6861 - acc: 0.5642 - f1: 0.5634 - val_loss: 0.6881 - val_acc: 0.5299 - val_f1: 0.5294\n",
      "Epoch 15/25\n",
      "8127/8127 [==============================] - 1s 69us/step - loss: 0.6803 - acc: 0.5804 - f1: 0.5781 - val_loss: 0.6877 - val_acc: 0.5376 - val_f1: 0.5345\n",
      "Epoch 16/25\n",
      "8127/8127 [==============================] - 1s 68us/step - loss: 0.6760 - acc: 0.5799 - f1: 0.5778 - val_loss: 0.6849 - val_acc: 0.5503 - val_f1: 0.5492\n",
      "Epoch 17/25\n",
      "8127/8127 [==============================] - 1s 69us/step - loss: 0.6727 - acc: 0.5954 - f1: 0.5941 - val_loss: 0.6823 - val_acc: 0.5619 - val_f1: 0.5624\n",
      "Epoch 18/25\n",
      "8127/8127 [==============================] - 1s 68us/step - loss: 0.6730 - acc: 0.5999 - f1: 0.6004 - val_loss: 0.6845 - val_acc: 0.5454 - val_f1: 0.5463\n",
      "Epoch 19/25\n",
      "8127/8127 [==============================] - 1s 69us/step - loss: 0.6641 - acc: 0.6133 - f1: 0.6124 - val_loss: 0.6796 - val_acc: 0.5686 - val_f1: 0.5690\n",
      "Epoch 20/25\n",
      "8127/8127 [==============================] - 1s 68us/step - loss: 0.6642 - acc: 0.6130 - f1: 0.6125 - val_loss: 0.6798 - val_acc: 0.5708 - val_f1: 0.5703\n",
      "Epoch 21/25\n",
      "8127/8127 [==============================] - 1s 69us/step - loss: 0.6581 - acc: 0.6218 - f1: 0.6215 - val_loss: 0.6746 - val_acc: 0.5841 - val_f1: 0.5836\n",
      "Epoch 22/25\n",
      "8127/8127 [==============================] - 1s 74us/step - loss: 0.6511 - acc: 0.6317 - f1: 0.6320 - val_loss: 0.6735 - val_acc: 0.5824 - val_f1: 0.5826\n",
      "Epoch 23/25\n",
      "8127/8127 [==============================] - 1s 73us/step - loss: 0.6424 - acc: 0.6451 - f1: 0.6446 - val_loss: 0.6710 - val_acc: 0.5808 - val_f1: 0.5812\n",
      "Epoch 24/25\n",
      "8127/8127 [==============================] - 1s 84us/step - loss: 0.6343 - acc: 0.6561 - f1: 0.6566 - val_loss: 0.6674 - val_acc: 0.5830 - val_f1: 0.5830\n",
      "Epoch 25/25\n",
      "8127/8127 [==============================] - 1s 78us/step - loss: 0.6250 - acc: 0.6638 - f1: 0.6641 - val_loss: 0.6608 - val_acc: 0.6012 - val_f1: 0.6015\n",
      "18\n",
      "(8323, 10001)\n",
      "(8323,)\n",
      "Baseline # : 4202\n",
      "Peak # : 4121\n",
      "x_train :  7490\n",
      "x_test  :  833\n",
      "y_train :  Counter({0: 3759, 1: 3731})\n",
      "y_test  :  Counter({0: 443, 1: 390})\n",
      "(10001,)\n",
      "(7490,)\n",
      "(833, 10001)\n",
      "(833,)\n",
      "Train on 7490 samples, validate on 833 samples\n",
      "Epoch 1/25\n",
      "7490/7490 [==============================] - 3s 343us/step - loss: 0.8070 - acc: 0.5003 - f1: 0.4997 - val_loss: 0.6954 - val_acc: 0.4820 - val_f1: 0.4559\n",
      "Epoch 2/25\n",
      "7490/7490 [==============================] - 1s 69us/step - loss: 0.7304 - acc: 0.5119 - f1: 0.5047 - val_loss: 0.6931 - val_acc: 0.4940 - val_f1: 0.4923\n",
      "Epoch 3/25\n",
      "7490/7490 [==============================] - 1s 69us/step - loss: 0.7183 - acc: 0.5146 - f1: 0.5076 - val_loss: 0.6941 - val_acc: 0.4862 - val_f1: 0.4992\n",
      "Epoch 4/25\n",
      "7490/7490 [==============================] - 1s 68us/step - loss: 0.7093 - acc: 0.5166 - f1: 0.5052 - val_loss: 0.6925 - val_acc: 0.4958 - val_f1: 0.4962\n",
      "Epoch 5/25\n",
      "7490/7490 [==============================] - 1s 70us/step - loss: 0.7029 - acc: 0.5185 - f1: 0.5127 - val_loss: 0.6935 - val_acc: 0.4832 - val_f1: 0.4873\n",
      "Epoch 6/25\n",
      "7490/7490 [==============================] - 1s 68us/step - loss: 0.6992 - acc: 0.5216 - f1: 0.5226 - val_loss: 0.6944 - val_acc: 0.4784 - val_f1: 0.4828\n",
      "Epoch 7/25\n",
      "7490/7490 [==============================] - 1s 70us/step - loss: 0.6965 - acc: 0.5248 - f1: 0.5226 - val_loss: 0.6947 - val_acc: 0.4712 - val_f1: 0.4735\n",
      "Epoch 8/25\n",
      "7490/7490 [==============================] - 1s 71us/step - loss: 0.6948 - acc: 0.5246 - f1: 0.5218 - val_loss: 0.6948 - val_acc: 0.4700 - val_f1: 0.4705\n",
      "Epoch 9/25\n",
      "7490/7490 [==============================] - 1s 69us/step - loss: 0.6946 - acc: 0.5208 - f1: 0.5175 - val_loss: 0.6948 - val_acc: 0.4712 - val_f1: 0.4696\n",
      "Epoch 10/25\n",
      "7490/7490 [==============================] - 1s 69us/step - loss: 0.6951 - acc: 0.5229 - f1: 0.5162 - val_loss: 0.6947 - val_acc: 0.4730 - val_f1: 0.4710\n",
      "Epoch 11/25\n",
      "7490/7490 [==============================] - 1s 69us/step - loss: 0.6939 - acc: 0.5281 - f1: 0.5214 - val_loss: 0.6950 - val_acc: 0.4724 - val_f1: 0.4726\n",
      "Epoch 12/25\n",
      "7490/7490 [==============================] - 1s 68us/step - loss: 0.6934 - acc: 0.5224 - f1: 0.5203 - val_loss: 0.6949 - val_acc: 0.4706 - val_f1: 0.4698\n",
      "Epoch 13/25\n",
      "7490/7490 [==============================] - 1s 69us/step - loss: 0.6925 - acc: 0.5294 - f1: 0.5256 - val_loss: 0.6948 - val_acc: 0.4694 - val_f1: 0.4694\n",
      "Epoch 14/25\n",
      "7490/7490 [==============================] - 1s 69us/step - loss: 0.6906 - acc: 0.5379 - f1: 0.5323 - val_loss: 0.6935 - val_acc: 0.4880 - val_f1: 0.4882\n",
      "Epoch 15/25\n",
      "7490/7490 [==============================] - 1s 68us/step - loss: 0.6919 - acc: 0.5454 - f1: 0.5402 - val_loss: 0.6943 - val_acc: 0.4748 - val_f1: 0.4738\n",
      "Epoch 16/25\n",
      "7490/7490 [==============================] - 1s 68us/step - loss: 0.6885 - acc: 0.5450 - f1: 0.5385 - val_loss: 0.6940 - val_acc: 0.4826 - val_f1: 0.4795\n",
      "Epoch 17/25\n",
      "7490/7490 [==============================] - 1s 69us/step - loss: 0.6916 - acc: 0.5451 - f1: 0.5387 - val_loss: 0.6943 - val_acc: 0.4784 - val_f1: 0.4761\n",
      "Epoch 18/25\n",
      "7490/7490 [==============================] - 1s 68us/step - loss: 0.6880 - acc: 0.5493 - f1: 0.5455 - val_loss: 0.6939 - val_acc: 0.4814 - val_f1: 0.4789\n",
      "Epoch 19/25\n",
      "7490/7490 [==============================] - 1s 68us/step - loss: 0.6855 - acc: 0.5575 - f1: 0.5540 - val_loss: 0.6931 - val_acc: 0.4898 - val_f1: 0.4879\n",
      "Epoch 20/25\n",
      "7490/7490 [==============================] - 1s 68us/step - loss: 0.6899 - acc: 0.5515 - f1: 0.5489 - val_loss: 0.6949 - val_acc: 0.4850 - val_f1: 0.4837\n",
      "Epoch 21/25\n",
      "7490/7490 [==============================] - 1s 69us/step - loss: 0.6842 - acc: 0.5679 - f1: 0.5656 - val_loss: 0.6948 - val_acc: 0.4790 - val_f1: 0.4771\n",
      "Epoch 22/25\n",
      "7490/7490 [==============================] - 1s 70us/step - loss: 0.6841 - acc: 0.5717 - f1: 0.5681 - val_loss: 0.6940 - val_acc: 0.4844 - val_f1: 0.4829\n",
      "Epoch 23/25\n",
      "7490/7490 [==============================] - 1s 70us/step - loss: 0.6821 - acc: 0.5714 - f1: 0.5697 - val_loss: 0.6939 - val_acc: 0.4880 - val_f1: 0.4878\n",
      "Epoch 24/25\n",
      "7490/7490 [==============================] - 1s 69us/step - loss: 0.6797 - acc: 0.5750 - f1: 0.5725 - val_loss: 0.6962 - val_acc: 0.4754 - val_f1: 0.4734\n",
      "Epoch 25/25\n",
      "7490/7490 [==============================] - 1s 69us/step - loss: 0.6809 - acc: 0.5805 - f1: 0.5792 - val_loss: 0.6938 - val_acc: 0.4886 - val_f1: 0.4872\n",
      "19\n",
      "(10302, 10001)\n",
      "(10302,)\n",
      "Baseline # : 4247\n",
      "Peak # : 6055\n",
      "x_train :  9271\n",
      "x_test  :  1031\n",
      "y_train :  Counter({1: 5453, 0: 3818})\n",
      "y_test  :  Counter({1: 602, 0: 429})\n",
      "(10001,)\n",
      "(9271,)\n",
      "(1031, 10001)\n",
      "(1031,)\n",
      "Train on 9271 samples, validate on 1031 samples\n",
      "Epoch 1/25\n",
      "9271/9271 [==============================] - 3s 306us/step - loss: 0.7892 - acc: 0.5087 - f1: 0.5229 - val_loss: 0.6853 - val_acc: 0.5800 - val_f1: 0.5779\n",
      "Epoch 2/25\n",
      "9271/9271 [==============================] - 1s 69us/step - loss: 0.7201 - acc: 0.5541 - f1: 0.5601 - val_loss: 0.6817 - val_acc: 0.5868 - val_f1: 0.5861\n",
      "Epoch 3/25\n",
      "9271/9271 [==============================] - 1s 69us/step - loss: 0.7051 - acc: 0.5755 - f1: 0.5794 - val_loss: 0.6804 - val_acc: 0.5839 - val_f1: 0.5839\n",
      "Epoch 4/25\n",
      "9271/9271 [==============================] - 1s 69us/step - loss: 0.6937 - acc: 0.5839 - f1: 0.5872 - val_loss: 0.6794 - val_acc: 0.5839 - val_f1: 0.5839\n",
      "Epoch 5/25\n",
      "9271/9271 [==============================] - 1s 70us/step - loss: 0.6841 - acc: 0.5917 - f1: 0.5934 - val_loss: 0.6781 - val_acc: 0.5839 - val_f1: 0.5839\n",
      "Epoch 6/25\n",
      "9271/9271 [==============================] - 1s 82us/step - loss: 0.6819 - acc: 0.5897 - f1: 0.5905 - val_loss: 0.6759 - val_acc: 0.5878 - val_f1: 0.5873\n",
      "Epoch 7/25\n",
      "9271/9271 [==============================] - 1s 87us/step - loss: 0.6794 - acc: 0.5980 - f1: 0.5991 - val_loss: 0.6774 - val_acc: 0.5844 - val_f1: 0.5842\n",
      "Epoch 8/25\n",
      "9271/9271 [==============================] - 1s 70us/step - loss: 0.6760 - acc: 0.6033 - f1: 0.6037 - val_loss: 0.6761 - val_acc: 0.5844 - val_f1: 0.5842\n",
      "Epoch 9/25\n",
      "9271/9271 [==============================] - 1s 70us/step - loss: 0.6740 - acc: 0.6038 - f1: 0.6045 - val_loss: 0.6749 - val_acc: 0.5883 - val_f1: 0.5880\n",
      "Epoch 10/25\n",
      "9271/9271 [==============================] - 1s 69us/step - loss: 0.6699 - acc: 0.6084 - f1: 0.6077 - val_loss: 0.6726 - val_acc: 0.5892 - val_f1: 0.5878\n",
      "Epoch 11/25\n",
      "9271/9271 [==============================] - 1s 68us/step - loss: 0.6683 - acc: 0.6144 - f1: 0.6141 - val_loss: 0.6710 - val_acc: 0.5873 - val_f1: 0.5867\n",
      "Epoch 12/25\n",
      "9271/9271 [==============================] - 1s 69us/step - loss: 0.6590 - acc: 0.6234 - f1: 0.6231 - val_loss: 0.6662 - val_acc: 0.5960 - val_f1: 0.5962\n",
      "Epoch 13/25\n",
      "9271/9271 [==============================] - 1s 69us/step - loss: 0.6545 - acc: 0.6342 - f1: 0.6350 - val_loss: 0.6645 - val_acc: 0.5989 - val_f1: 0.5992\n",
      "Epoch 14/25\n",
      "9271/9271 [==============================] - 1s 70us/step - loss: 0.6401 - acc: 0.6494 - f1: 0.6504 - val_loss: 0.6568 - val_acc: 0.6067 - val_f1: 0.6077\n",
      "Epoch 15/25\n",
      "9271/9271 [==============================] - 1s 69us/step - loss: 0.6276 - acc: 0.6668 - f1: 0.6667 - val_loss: 0.6497 - val_acc: 0.6193 - val_f1: 0.6199\n",
      "Epoch 16/25\n",
      "9271/9271 [==============================] - 1s 69us/step - loss: 0.6143 - acc: 0.6828 - f1: 0.6827 - val_loss: 0.6389 - val_acc: 0.6368 - val_f1: 0.6377\n",
      "Epoch 17/25\n",
      "9271/9271 [==============================] - 1s 70us/step - loss: 0.6035 - acc: 0.6970 - f1: 0.6969 - val_loss: 0.6308 - val_acc: 0.6455 - val_f1: 0.6463\n",
      "Epoch 18/25\n",
      "9271/9271 [==============================] - 1s 69us/step - loss: 0.5872 - acc: 0.7092 - f1: 0.7094 - val_loss: 0.6200 - val_acc: 0.6532 - val_f1: 0.6534\n",
      "Epoch 19/25\n",
      "9271/9271 [==============================] - 1s 69us/step - loss: 0.5819 - acc: 0.7183 - f1: 0.7179 - val_loss: 0.6160 - val_acc: 0.6615 - val_f1: 0.6615\n",
      "Epoch 20/25\n",
      "9271/9271 [==============================] - 1s 70us/step - loss: 0.5557 - acc: 0.7392 - f1: 0.7389 - val_loss: 0.6114 - val_acc: 0.6688 - val_f1: 0.6693\n",
      "Epoch 21/25\n",
      "9271/9271 [==============================] - 1s 69us/step - loss: 0.5391 - acc: 0.7435 - f1: 0.7435 - val_loss: 0.5962 - val_acc: 0.6872 - val_f1: 0.6880\n",
      "Epoch 22/25\n",
      "9271/9271 [==============================] - 1s 70us/step - loss: 0.5398 - acc: 0.7521 - f1: 0.7521 - val_loss: 0.6037 - val_acc: 0.6775 - val_f1: 0.6783\n",
      "Epoch 23/25\n",
      "9271/9271 [==============================] - 1s 69us/step - loss: 0.5138 - acc: 0.7662 - f1: 0.7661 - val_loss: 0.5958 - val_acc: 0.6877 - val_f1: 0.6882\n",
      "Epoch 24/25\n",
      "9271/9271 [==============================] - 1s 68us/step - loss: 0.5023 - acc: 0.7741 - f1: 0.7741 - val_loss: 0.5998 - val_acc: 0.6872 - val_f1: 0.6873\n",
      "Epoch 25/25\n",
      "9271/9271 [==============================] - 1s 69us/step - loss: 0.4881 - acc: 0.7862 - f1: 0.7859 - val_loss: 0.5888 - val_acc: 0.6979 - val_f1: 0.6985\n",
      "20\n",
      "(9843, 10001)\n",
      "(9843,)\n",
      "Baseline # : 4324\n",
      "Peak # : 5519\n",
      "x_train :  8858\n",
      "x_test  :  985\n",
      "y_train :  Counter({1: 4949, 0: 3909})\n",
      "y_test  :  Counter({1: 570, 0: 415})\n",
      "(10001,)\n",
      "(8858,)\n",
      "(985, 10001)\n",
      "(985,)\n",
      "Train on 8858 samples, validate on 985 samples\n",
      "Epoch 1/25\n",
      "8858/8858 [==============================] - 3s 332us/step - loss: 0.7294 - acc: 0.5153 - f1: 0.5174 - val_loss: 0.6894 - val_acc: 0.5827 - val_f1: 0.5741\n",
      "Epoch 2/25\n",
      "8858/8858 [==============================] - 1s 70us/step - loss: 0.7060 - acc: 0.5386 - f1: 0.5356 - val_loss: 0.6871 - val_acc: 0.5792 - val_f1: 0.5790\n",
      "Epoch 3/25\n",
      "8858/8858 [==============================] - 1s 71us/step - loss: 0.6979 - acc: 0.5494 - f1: 0.5473 - val_loss: 0.6854 - val_acc: 0.5787 - val_f1: 0.5787\n",
      "Epoch 4/25\n",
      "8858/8858 [==============================] - 1s 69us/step - loss: 0.6940 - acc: 0.5553 - f1: 0.5554 - val_loss: 0.6841 - val_acc: 0.5797 - val_f1: 0.5797\n",
      "Epoch 5/25\n",
      "8858/8858 [==============================] - 1s 74us/step - loss: 0.6899 - acc: 0.5595 - f1: 0.5609 - val_loss: 0.6831 - val_acc: 0.5777 - val_f1: 0.5777\n",
      "Epoch 6/25\n",
      "8858/8858 [==============================] - 1s 74us/step - loss: 0.6892 - acc: 0.5618 - f1: 0.5617 - val_loss: 0.6824 - val_acc: 0.5787 - val_f1: 0.5787\n",
      "Epoch 7/25\n",
      "8858/8858 [==============================] - 1s 93us/step - loss: 0.6882 - acc: 0.5618 - f1: 0.5619 - val_loss: 0.6811 - val_acc: 0.5782 - val_f1: 0.5780\n",
      "Epoch 8/25\n",
      "8858/8858 [==============================] - 1s 74us/step - loss: 0.6835 - acc: 0.5660 - f1: 0.5649 - val_loss: 0.6782 - val_acc: 0.5848 - val_f1: 0.5847\n",
      "Epoch 9/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8858/8858 [==============================] - 1s 69us/step - loss: 0.6802 - acc: 0.5775 - f1: 0.5771 - val_loss: 0.6718 - val_acc: 0.6030 - val_f1: 0.6029\n",
      "Epoch 10/25\n",
      "8858/8858 [==============================] - 1s 69us/step - loss: 0.6760 - acc: 0.5864 - f1: 0.5868 - val_loss: 0.6645 - val_acc: 0.6122 - val_f1: 0.6130\n",
      "Epoch 11/25\n",
      "8858/8858 [==============================] - 1s 69us/step - loss: 0.6657 - acc: 0.6004 - f1: 0.6004 - val_loss: 0.6489 - val_acc: 0.6193 - val_f1: 0.6200\n",
      "Epoch 12/25\n",
      "8858/8858 [==============================] - 1s 71us/step - loss: 0.6575 - acc: 0.6141 - f1: 0.6137 - val_loss: 0.6402 - val_acc: 0.6431 - val_f1: 0.6433\n",
      "Epoch 13/25\n",
      "8858/8858 [==============================] - 1s 70us/step - loss: 0.6517 - acc: 0.6218 - f1: 0.6213 - val_loss: 0.6358 - val_acc: 0.6553 - val_f1: 0.6551\n",
      "Epoch 14/25\n",
      "8858/8858 [==============================] - 1s 69us/step - loss: 0.6354 - acc: 0.6421 - f1: 0.6408 - val_loss: 0.6241 - val_acc: 0.6518 - val_f1: 0.6510\n",
      "Epoch 15/25\n",
      "8858/8858 [==============================] - 1s 68us/step - loss: 0.6145 - acc: 0.6662 - f1: 0.6652 - val_loss: 0.6185 - val_acc: 0.6706 - val_f1: 0.6704\n",
      "Epoch 16/25\n",
      "8858/8858 [==============================] - 1s 68us/step - loss: 0.6048 - acc: 0.6814 - f1: 0.6809 - val_loss: 0.6112 - val_acc: 0.6787 - val_f1: 0.6781\n",
      "Epoch 17/25\n",
      "8858/8858 [==============================] - 1s 69us/step - loss: 0.5834 - acc: 0.7001 - f1: 0.7003 - val_loss: 0.6029 - val_acc: 0.6985 - val_f1: 0.6981\n",
      "Epoch 18/25\n",
      "8858/8858 [==============================] - 1s 69us/step - loss: 0.5666 - acc: 0.7260 - f1: 0.7259 - val_loss: 0.5904 - val_acc: 0.6924 - val_f1: 0.6920\n",
      "Epoch 19/25\n",
      "8858/8858 [==============================] - 1s 69us/step - loss: 0.5436 - acc: 0.7446 - f1: 0.7448 - val_loss: 0.5853 - val_acc: 0.7076 - val_f1: 0.7075\n",
      "Epoch 20/25\n",
      "8858/8858 [==============================] - 1s 69us/step - loss: 0.5203 - acc: 0.7592 - f1: 0.7596 - val_loss: 0.5814 - val_acc: 0.7102 - val_f1: 0.7100\n",
      "Epoch 21/25\n",
      "8858/8858 [==============================] - 1s 69us/step - loss: 0.5068 - acc: 0.7717 - f1: 0.7718 - val_loss: 0.5912 - val_acc: 0.7076 - val_f1: 0.7076\n",
      "Epoch 22/25\n",
      "8858/8858 [==============================] - 1s 68us/step - loss: 0.4859 - acc: 0.7858 - f1: 0.7860 - val_loss: 0.5729 - val_acc: 0.7188 - val_f1: 0.7185\n",
      "Epoch 23/25\n",
      "8858/8858 [==============================] - 1s 69us/step - loss: 0.4802 - acc: 0.7949 - f1: 0.7949 - val_loss: 0.5680 - val_acc: 0.7269 - val_f1: 0.7269\n",
      "Epoch 24/25\n",
      "8858/8858 [==============================] - 1s 69us/step - loss: 0.4526 - acc: 0.8055 - f1: 0.8055 - val_loss: 0.5551 - val_acc: 0.7289 - val_f1: 0.7284\n",
      "Epoch 25/25\n",
      "8858/8858 [==============================] - 1s 69us/step - loss: 0.4315 - acc: 0.8218 - f1: 0.8219 - val_loss: 0.5829 - val_acc: 0.7213 - val_f1: 0.7206\n",
      "21\n",
      "(8680, 10001)\n",
      "(8680,)\n",
      "Baseline # : 4026\n",
      "Peak # : 4654\n",
      "x_train :  7812\n",
      "x_test  :  868\n",
      "y_train :  Counter({1: 4206, 0: 3606})\n",
      "y_test  :  Counter({1: 448, 0: 420})\n",
      "(10001,)\n",
      "(7812,)\n",
      "(868, 10001)\n",
      "(868,)\n",
      "Train on 7812 samples, validate on 868 samples\n",
      "Epoch 1/25\n",
      "7812/7812 [==============================] - 3s 387us/step - loss: 0.7507 - acc: 0.5123 - f1: 0.4773 - val_loss: 0.6864 - val_acc: 0.5507 - val_f1: 0.5988\n",
      "Epoch 2/25\n",
      "7812/7812 [==============================] - 1s 71us/step - loss: 0.7125 - acc: 0.5271 - f1: 0.5373 - val_loss: 0.6879 - val_acc: 0.5346 - val_f1: 0.5916\n",
      "Epoch 3/25\n",
      "7812/7812 [==============================] - 1s 71us/step - loss: 0.7026 - acc: 0.5371 - f1: 0.5518 - val_loss: 0.6828 - val_acc: 0.5472 - val_f1: 0.5638\n",
      "Epoch 4/25\n",
      "7812/7812 [==============================] - 1s 69us/step - loss: 0.6922 - acc: 0.5533 - f1: 0.5597 - val_loss: 0.6726 - val_acc: 0.5950 - val_f1: 0.6115\n",
      "Epoch 5/25\n",
      "7812/7812 [==============================] - 1s 69us/step - loss: 0.6766 - acc: 0.5752 - f1: 0.5777 - val_loss: 0.6522 - val_acc: 0.6158 - val_f1: 0.6293\n",
      "Epoch 6/25\n",
      "7812/7812 [==============================] - 1s 70us/step - loss: 0.6603 - acc: 0.6064 - f1: 0.6086 - val_loss: 0.6346 - val_acc: 0.6452 - val_f1: 0.6574\n",
      "Epoch 7/25\n",
      "7812/7812 [==============================] - 1s 70us/step - loss: 0.6388 - acc: 0.6347 - f1: 0.6356 - val_loss: 0.6097 - val_acc: 0.6740 - val_f1: 0.6810\n",
      "Epoch 8/25\n",
      "7812/7812 [==============================] - 1s 69us/step - loss: 0.6180 - acc: 0.6646 - f1: 0.6657 - val_loss: 0.5884 - val_acc: 0.6935 - val_f1: 0.6980\n",
      "Epoch 9/25\n",
      "7812/7812 [==============================] - 1s 69us/step - loss: 0.5981 - acc: 0.6900 - f1: 0.6899 - val_loss: 0.5720 - val_acc: 0.7033 - val_f1: 0.7063\n",
      "Epoch 10/25\n",
      "7812/7812 [==============================] - 1s 69us/step - loss: 0.5692 - acc: 0.7252 - f1: 0.7243 - val_loss: 0.5595 - val_acc: 0.7051 - val_f1: 0.7075\n",
      "Epoch 11/25\n",
      "7812/7812 [==============================] - 1s 69us/step - loss: 0.5520 - acc: 0.7353 - f1: 0.7350 - val_loss: 0.5378 - val_acc: 0.7316 - val_f1: 0.7341\n",
      "Epoch 12/25\n",
      "7812/7812 [==============================] - 1s 70us/step - loss: 0.5079 - acc: 0.7704 - f1: 0.7699 - val_loss: 0.5275 - val_acc: 0.7321 - val_f1: 0.7334\n",
      "Epoch 13/25\n",
      "7812/7812 [==============================] - 1s 70us/step - loss: 0.4878 - acc: 0.7909 - f1: 0.7907 - val_loss: 0.5151 - val_acc: 0.7535 - val_f1: 0.7543\n",
      "Epoch 14/25\n",
      "7812/7812 [==============================] - 1s 71us/step - loss: 0.4701 - acc: 0.8028 - f1: 0.8026 - val_loss: 0.5101 - val_acc: 0.7586 - val_f1: 0.7596\n",
      "Epoch 15/25\n",
      "7812/7812 [==============================] - 1s 70us/step - loss: 0.4435 - acc: 0.8098 - f1: 0.8095 - val_loss: 0.4916 - val_acc: 0.7609 - val_f1: 0.7619\n",
      "Epoch 16/25\n",
      "7812/7812 [==============================] - 1s 70us/step - loss: 0.4327 - acc: 0.8298 - f1: 0.8299 - val_loss: 0.4955 - val_acc: 0.7702 - val_f1: 0.7712\n",
      "Epoch 17/25\n",
      "7812/7812 [==============================] - 1s 69us/step - loss: 0.4066 - acc: 0.8311 - f1: 0.8311 - val_loss: 0.4903 - val_acc: 0.7753 - val_f1: 0.7759\n",
      "Epoch 18/25\n",
      "7812/7812 [==============================] - 1s 69us/step - loss: 0.3881 - acc: 0.8462 - f1: 0.8463 - val_loss: 0.4907 - val_acc: 0.7776 - val_f1: 0.7779\n",
      "Epoch 19/25\n",
      "7812/7812 [==============================] - 1s 70us/step - loss: 0.3753 - acc: 0.8551 - f1: 0.8551 - val_loss: 0.5081 - val_acc: 0.7707 - val_f1: 0.7717\n",
      "Epoch 20/25\n",
      "7812/7812 [==============================] - 1s 70us/step - loss: 0.3493 - acc: 0.8617 - f1: 0.8617 - val_loss: 0.5035 - val_acc: 0.7782 - val_f1: 0.7786\n",
      "Epoch 21/25\n",
      "7812/7812 [==============================] - 1s 71us/step - loss: 0.3367 - acc: 0.8792 - f1: 0.8791 - val_loss: 0.5103 - val_acc: 0.7575 - val_f1: 0.7577\n",
      "Epoch 22/25\n",
      "7812/7812 [==============================] - 1s 70us/step - loss: 0.3331 - acc: 0.8768 - f1: 0.8767 - val_loss: 0.5205 - val_acc: 0.7851 - val_f1: 0.7852\n",
      "Epoch 23/25\n",
      "7812/7812 [==============================] - 1s 69us/step - loss: 0.3078 - acc: 0.8842 - f1: 0.8842 - val_loss: 0.5248 - val_acc: 0.7736 - val_f1: 0.7735\n",
      "Epoch 24/25\n",
      "7812/7812 [==============================] - 1s 70us/step - loss: 0.2868 - acc: 0.8991 - f1: 0.8991 - val_loss: 0.5394 - val_acc: 0.7811 - val_f1: 0.7811\n",
      "Epoch 25/25\n",
      "7812/7812 [==============================] - 1s 70us/step - loss: 0.2851 - acc: 0.9009 - f1: 0.9010 - val_loss: 0.5636 - val_acc: 0.7736 - val_f1: 0.7737\n",
      "22\n",
      "(9790, 10001)\n",
      "(9790,)\n",
      "Baseline # : 4023\n",
      "Peak # : 5767\n",
      "x_train :  8811\n",
      "x_test  :  979\n",
      "y_train :  Counter({1: 5199, 0: 3612})\n",
      "y_test  :  Counter({1: 568, 0: 411})\n",
      "(10001,)\n",
      "(8811,)\n",
      "(979, 10001)\n",
      "(979,)\n",
      "Train on 8811 samples, validate on 979 samples\n",
      "Epoch 1/25\n",
      "8811/8811 [==============================] - 3s 355us/step - loss: 0.7387 - acc: 0.5392 - f1: 0.4856 - val_loss: 0.6888 - val_acc: 0.5868 - val_f1: 0.5835\n",
      "Epoch 2/25\n",
      "8811/8811 [==============================] - 1s 70us/step - loss: 0.7045 - acc: 0.5651 - f1: 0.5409 - val_loss: 0.6842 - val_acc: 0.5827 - val_f1: 0.5822\n",
      "Epoch 3/25\n",
      "8811/8811 [==============================] - 1s 70us/step - loss: 0.6918 - acc: 0.5884 - f1: 0.5730 - val_loss: 0.6810 - val_acc: 0.5843 - val_f1: 0.5851\n",
      "Epoch 4/25\n",
      "8811/8811 [==============================] - 1s 70us/step - loss: 0.6838 - acc: 0.5937 - f1: 0.5855 - val_loss: 0.6760 - val_acc: 0.5894 - val_f1: 0.5890\n",
      "Epoch 5/25\n",
      "8811/8811 [==============================] - 1s 71us/step - loss: 0.6800 - acc: 0.6058 - f1: 0.5982 - val_loss: 0.6670 - val_acc: 0.6108 - val_f1: 0.6097\n",
      "Epoch 6/25\n",
      "8811/8811 [==============================] - 1s 70us/step - loss: 0.6611 - acc: 0.6241 - f1: 0.6175 - val_loss: 0.6498 - val_acc: 0.6374 - val_f1: 0.6359\n",
      "Epoch 7/25\n",
      "8811/8811 [==============================] - 1s 69us/step - loss: 0.6500 - acc: 0.6409 - f1: 0.6374 - val_loss: 0.6391 - val_acc: 0.6619 - val_f1: 0.6601\n",
      "Epoch 8/25\n",
      "8811/8811 [==============================] - 1s 68us/step - loss: 0.6327 - acc: 0.6583 - f1: 0.6563 - val_loss: 0.6333 - val_acc: 0.6696 - val_f1: 0.6660\n",
      "Epoch 9/25\n",
      "8811/8811 [==============================] - 1s 69us/step - loss: 0.6309 - acc: 0.6612 - f1: 0.6585 - val_loss: 0.6205 - val_acc: 0.6793 - val_f1: 0.6780\n",
      "Epoch 10/25\n",
      "8811/8811 [==============================] - 1s 69us/step - loss: 0.6067 - acc: 0.6810 - f1: 0.6792 - val_loss: 0.6134 - val_acc: 0.6803 - val_f1: 0.6783\n",
      "Epoch 11/25\n",
      "8811/8811 [==============================] - 1s 69us/step - loss: 0.6009 - acc: 0.6954 - f1: 0.6940 - val_loss: 0.6094 - val_acc: 0.6854 - val_f1: 0.6834\n",
      "Epoch 12/25\n",
      "8811/8811 [==============================] - 1s 68us/step - loss: 0.5811 - acc: 0.7053 - f1: 0.7043 - val_loss: 0.6075 - val_acc: 0.6823 - val_f1: 0.6810\n",
      "Epoch 13/25\n",
      "8811/8811 [==============================] - 1s 68us/step - loss: 0.5658 - acc: 0.7167 - f1: 0.7158 - val_loss: 0.6031 - val_acc: 0.6900 - val_f1: 0.6895\n",
      "Epoch 14/25\n",
      "8811/8811 [==============================] - 1s 69us/step - loss: 0.5460 - acc: 0.7326 - f1: 0.7321 - val_loss: 0.6038 - val_acc: 0.6951 - val_f1: 0.6934\n",
      "Epoch 15/25\n",
      "8811/8811 [==============================] - 1s 71us/step - loss: 0.5322 - acc: 0.7377 - f1: 0.7372 - val_loss: 0.6008 - val_acc: 0.6951 - val_f1: 0.6943\n",
      "Epoch 16/25\n",
      "8811/8811 [==============================] - 1s 71us/step - loss: 0.5230 - acc: 0.7458 - f1: 0.7455 - val_loss: 0.6073 - val_acc: 0.6925 - val_f1: 0.6916\n",
      "Epoch 17/25\n",
      "8811/8811 [==============================] - 1s 69us/step - loss: 0.5052 - acc: 0.7644 - f1: 0.7641 - val_loss: 0.6180 - val_acc: 0.6905 - val_f1: 0.6905\n",
      "Epoch 18/25\n",
      "8811/8811 [==============================] - 1s 70us/step - loss: 0.4955 - acc: 0.7711 - f1: 0.7710 - val_loss: 0.6052 - val_acc: 0.6834 - val_f1: 0.6834\n",
      "Epoch 19/25\n",
      "8811/8811 [==============================] - 1s 69us/step - loss: 0.4756 - acc: 0.7841 - f1: 0.7838 - val_loss: 0.6091 - val_acc: 0.6823 - val_f1: 0.6826\n",
      "Epoch 20/25\n",
      "8811/8811 [==============================] - 1s 69us/step - loss: 0.4607 - acc: 0.7934 - f1: 0.7935 - val_loss: 0.6142 - val_acc: 0.6977 - val_f1: 0.6979\n",
      "Epoch 21/25\n",
      "8811/8811 [==============================] - 1s 69us/step - loss: 0.4512 - acc: 0.8008 - f1: 0.8005 - val_loss: 0.6186 - val_acc: 0.6925 - val_f1: 0.6925\n",
      "Epoch 22/25\n",
      "8811/8811 [==============================] - 1s 69us/step - loss: 0.4371 - acc: 0.8052 - f1: 0.8052 - val_loss: 0.6346 - val_acc: 0.6941 - val_f1: 0.6945\n",
      "Epoch 23/25\n",
      "8811/8811 [==============================] - 1s 69us/step - loss: 0.4287 - acc: 0.8090 - f1: 0.8089 - val_loss: 0.6460 - val_acc: 0.6951 - val_f1: 0.6956\n",
      "Epoch 24/25\n",
      "8811/8811 [==============================] - 1s 68us/step - loss: 0.4243 - acc: 0.8212 - f1: 0.8211 - val_loss: 0.6091 - val_acc: 0.7007 - val_f1: 0.7013\n",
      "Epoch 25/25\n",
      "8811/8811 [==============================] - 1s 69us/step - loss: 0.4050 - acc: 0.8261 - f1: 0.8259 - val_loss: 0.6464 - val_acc: 0.7063 - val_f1: 0.7073\n",
      "23\n",
      "(11481, 10001)\n",
      "(11481,)\n",
      "Baseline # : 6570\n",
      "Peak # : 4911\n",
      "x_train :  10332\n",
      "x_test  :  1149\n",
      "y_train :  Counter({0: 5902, 1: 4430})\n",
      "y_test  :  Counter({0: 668, 1: 481})\n",
      "(10001,)\n",
      "(10332,)\n",
      "(1149, 10001)\n",
      "(1149,)\n",
      "Train on 10332 samples, validate on 1149 samples\n",
      "Epoch 1/25\n",
      "10332/10332 [==============================] - 3s 322us/step - loss: 0.7424 - acc: 0.5200 - f1: 0.5113 - val_loss: 0.6832 - val_acc: 0.5805 - val_f1: 0.5826\n",
      "Epoch 2/25\n",
      "10332/10332 [==============================] - 1s 70us/step - loss: 0.7042 - acc: 0.5511 - f1: 0.5604 - val_loss: 0.6791 - val_acc: 0.5805 - val_f1: 0.5805\n",
      "Epoch 3/25\n",
      "10332/10332 [==============================] - 1s 70us/step - loss: 0.6944 - acc: 0.5594 - f1: 0.5630 - val_loss: 0.6701 - val_acc: 0.5809 - val_f1: 0.5808\n",
      "Epoch 4/25\n",
      "10332/10332 [==============================] - 1s 70us/step - loss: 0.6806 - acc: 0.5684 - f1: 0.5692 - val_loss: 0.6588 - val_acc: 0.5801 - val_f1: 0.5799\n",
      "Epoch 5/25\n",
      "10332/10332 [==============================] - 1s 71us/step - loss: 0.6739 - acc: 0.5684 - f1: 0.5659 - val_loss: 0.6498 - val_acc: 0.5796 - val_f1: 0.5792\n",
      "Epoch 6/25\n",
      "10332/10332 [==============================] - 1s 69us/step - loss: 0.6611 - acc: 0.5694 - f1: 0.5655 - val_loss: 0.6371 - val_acc: 0.5783 - val_f1: 0.5789\n",
      "Epoch 7/25\n",
      "10332/10332 [==============================] - 1s 69us/step - loss: 0.6557 - acc: 0.5806 - f1: 0.5740 - val_loss: 0.6328 - val_acc: 0.5770 - val_f1: 0.5745\n",
      "Epoch 8/25\n",
      "10332/10332 [==============================] - 1s 70us/step - loss: 0.6400 - acc: 0.5985 - f1: 0.5938 - val_loss: 0.6232 - val_acc: 0.6023 - val_f1: 0.5871\n",
      "Epoch 9/25\n",
      "10332/10332 [==============================] - 1s 70us/step - loss: 0.6309 - acc: 0.6126 - f1: 0.6083 - val_loss: 0.6132 - val_acc: 0.6201 - val_f1: 0.6013\n",
      "Epoch 10/25\n",
      "10332/10332 [==============================] - 1s 70us/step - loss: 0.6199 - acc: 0.6368 - f1: 0.6220 - val_loss: 0.6131 - val_acc: 0.6606 - val_f1: 0.6415\n",
      "Epoch 11/25\n",
      "10332/10332 [==============================] - 1s 70us/step - loss: 0.5994 - acc: 0.6663 - f1: 0.6506 - val_loss: 0.5995 - val_acc: 0.6671 - val_f1: 0.6631\n",
      "Epoch 12/25\n",
      "10332/10332 [==============================] - 1s 71us/step - loss: 0.5781 - acc: 0.6967 - f1: 0.6934 - val_loss: 0.5922 - val_acc: 0.6780 - val_f1: 0.6761\n",
      "Epoch 13/25\n",
      "10332/10332 [==============================] - 1s 70us/step - loss: 0.5611 - acc: 0.7080 - f1: 0.7063 - val_loss: 0.5804 - val_acc: 0.6832 - val_f1: 0.6824\n",
      "Epoch 14/25\n",
      "10332/10332 [==============================] - 1s 69us/step - loss: 0.5455 - acc: 0.7299 - f1: 0.7289 - val_loss: 0.5744 - val_acc: 0.6832 - val_f1: 0.6827\n",
      "Epoch 15/25\n",
      "10332/10332 [==============================] - 1s 71us/step - loss: 0.5336 - acc: 0.7476 - f1: 0.7468 - val_loss: 0.5631 - val_acc: 0.6884 - val_f1: 0.6882\n",
      "Epoch 16/25\n",
      "10332/10332 [==============================] - 1s 69us/step - loss: 0.5024 - acc: 0.7685 - f1: 0.7679 - val_loss: 0.5579 - val_acc: 0.6993 - val_f1: 0.6992\n",
      "Epoch 17/25\n",
      "10332/10332 [==============================] - 1s 70us/step - loss: 0.4855 - acc: 0.7753 - f1: 0.7750 - val_loss: 0.5567 - val_acc: 0.6958 - val_f1: 0.6957\n",
      "Epoch 18/25\n",
      "10332/10332 [==============================] - 1s 70us/step - loss: 0.4715 - acc: 0.7907 - f1: 0.7907 - val_loss: 0.5495 - val_acc: 0.7097 - val_f1: 0.7096\n",
      "Epoch 19/25\n",
      "10332/10332 [==============================] - 1s 69us/step - loss: 0.4541 - acc: 0.8047 - f1: 0.8045 - val_loss: 0.5538 - val_acc: 0.7128 - val_f1: 0.7128\n",
      "Epoch 20/25\n",
      "10332/10332 [==============================] - 1s 69us/step - loss: 0.4522 - acc: 0.8055 - f1: 0.8054 - val_loss: 0.5439 - val_acc: 0.6958 - val_f1: 0.6957\n",
      "Epoch 21/25\n",
      "10332/10332 [==============================] - 1s 70us/step - loss: 0.4266 - acc: 0.8169 - f1: 0.8167 - val_loss: 0.5623 - val_acc: 0.7076 - val_f1: 0.7073\n",
      "Epoch 22/25\n",
      "10332/10332 [==============================] - 1s 69us/step - loss: 0.4231 - acc: 0.8269 - f1: 0.8268 - val_loss: 0.5462 - val_acc: 0.7232 - val_f1: 0.7227\n",
      "Epoch 23/25\n",
      "10332/10332 [==============================] - 1s 71us/step - loss: 0.3979 - acc: 0.8329 - f1: 0.8329 - val_loss: 0.5597 - val_acc: 0.7219 - val_f1: 0.7218\n",
      "Epoch 24/25\n",
      "10332/10332 [==============================] - 1s 70us/step - loss: 0.3957 - acc: 0.8391 - f1: 0.8390 - val_loss: 0.5595 - val_acc: 0.7211 - val_f1: 0.7207\n",
      "Epoch 25/25\n",
      "10332/10332 [==============================] - 1s 70us/step - loss: 0.3721 - acc: 0.8533 - f1: 0.8533 - val_loss: 0.5675 - val_acc: 0.7285 - val_f1: 0.7285\n"
     ]
    }
   ],
   "source": [
    "### Train from Hour 0 to Hour 23\n",
    "results = list()\n",
    "sample_size = list()\n",
    "for k in range(0, 24):\n",
    "    print(k)\n",
    "    X_H = list()\n",
    "    Y_H = list()\n",
    "### Baseline\n",
    "    for i in range(0, 13):\n",
    "        dd = \"/mnt/data0/tao/ECG/\" + ECG_meta.Exp[i] + \"/Baseline/\" + ECG_meta.Species[i]\n",
    "        fname = '_H' + str(k) + '_'\n",
    "        file_hour = [x for x in os.listdir(dd) if fname in x]\n",
    "        for item in file_hour:\n",
    "            ff = dd + \"/\" + item\n",
    "            data = pd.read_csv(ff)\n",
    "            X_H.append(list(data.VALUE.values))\n",
    "            Y_H.append(0)\n",
    "### Peak\n",
    "    for i in range(0, 13):\n",
    "        dd = \"/mnt/data0/tao/ECG/\" + ECG_meta.Exp[i] + \"/Peak/\" + ECG_meta.Species[i]\n",
    "        fname = '_H' + str(k) + '_'\n",
    "        file_hour = [x for x in os.listdir(dd) if fname in x]\n",
    "        for item in file_hour:\n",
    "            ff = dd + \"/\" + item\n",
    "            data = pd.read_csv(ff)\n",
    "            X_H.append(list(data.VALUE.values))\n",
    "            Y_H.append(1)    \n",
    "    X_H = np.asarray(X_H)\n",
    "    Y_H = np.asarray(Y_H)\n",
    "    print(np.shape(X_H))\n",
    "    print(np.shape(Y_H))\n",
    "    print('Baseline # :', len([x for x in Y_H if x == 0]))\n",
    "    print('Peak # :', len([x for x in Y_H if x == 1]))\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_H, Y_H, test_size=0.1)\n",
    "    print(\"x_train : \", len(x_train))\n",
    "    print(\"x_test  : \", len(x_test))\n",
    "    print(\"y_train : \", collections.Counter(y_train))\n",
    "    print(\"y_test  : \", collections.Counter(y_test))\n",
    "    ## Print train & test\n",
    "    print(np.shape(x_train[0]))\n",
    "    print(np.shape(y_train))\n",
    "    print(np.shape(x_test))\n",
    "    print(np.shape(y_test))### Keras Sequential Models\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=10001, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "### Compilation\n",
    "# For a binary classification problem\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', f1])\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes=2)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes=2)\n",
    "    history = model.fit(x_train, y_train, epochs=25, batch_size=128, validation_data=(x_test, y_test))\n",
    "    results.append([k, max(history.history['acc']).astype(str), max(history.history['f1']).astype(str), \n",
    "                    max(history.history['val_acc']).astype(str), max(history.history['val_f1']).astype(str)])\n",
    "    sample_size.append([len(x_train), len(x_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results)\n",
    "results.columns = ['Hour', 'Training_Acc', 'Training_F1', 'Testing_Acc', 'Testing_F1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = pd.DataFrame(sample_size)\n",
    "sample_size.columns = ['Training_Sample', 'Testing_Sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hour</th>\n",
       "      <th>Training_Acc</th>\n",
       "      <th>Training_F1</th>\n",
       "      <th>Testing_Acc</th>\n",
       "      <th>Testing_F1</th>\n",
       "      <th>Training_Sample</th>\n",
       "      <th>Testing_Sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.6643876334716534</td>\n",
       "      <td>0.6373365035663745</td>\n",
       "      <td>0.5967914439458898</td>\n",
       "      <td>0.5967913855843365</td>\n",
       "      <td>8410</td>\n",
       "      <td>935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.7550963573377728</td>\n",
       "      <td>0.754469499020185</td>\n",
       "      <td>0.6883767523125321</td>\n",
       "      <td>0.6848854310287981</td>\n",
       "      <td>8977</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.7344007764320707</td>\n",
       "      <td>0.7348777705404956</td>\n",
       "      <td>0.60742357984901</td>\n",
       "      <td>0.6629905008332698</td>\n",
       "      <td>10305</td>\n",
       "      <td>1145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.7789513391847658</td>\n",
       "      <td>0.7791312528933637</td>\n",
       "      <td>0.6776929602115411</td>\n",
       "      <td>0.6776929012893923</td>\n",
       "      <td>10604</td>\n",
       "      <td>1179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.7884342142936462</td>\n",
       "      <td>0.7875614224526313</td>\n",
       "      <td>0.7201873931202409</td>\n",
       "      <td>0.7192936870674055</td>\n",
       "      <td>10557</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.7777827127667952</td>\n",
       "      <td>0.7773609263674345</td>\n",
       "      <td>0.6638689065913407</td>\n",
       "      <td>0.663453481037268</td>\n",
       "      <td>11257</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.71275071658199</td>\n",
       "      <td>0.7125885554654813</td>\n",
       "      <td>0.6233894976208538</td>\n",
       "      <td>0.623389438016209</td>\n",
       "      <td>9074</td>\n",
       "      <td>1009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.5243689659738091</td>\n",
       "      <td>0.5211813250114381</td>\n",
       "      <td>0.5407949790794979</td>\n",
       "      <td>0.545885609034215</td>\n",
       "      <td>8597</td>\n",
       "      <td>956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.6660167130697847</td>\n",
       "      <td>0.6659312105223107</td>\n",
       "      <td>0.5622389315064987</td>\n",
       "      <td>0.5618690470406286</td>\n",
       "      <td>10770</td>\n",
       "      <td>1197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.6802165448376908</td>\n",
       "      <td>0.6802107521368801</td>\n",
       "      <td>0.6717948719986484</td>\n",
       "      <td>0.671794811477009</td>\n",
       "      <td>10529</td>\n",
       "      <td>1170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.7212846042925782</td>\n",
       "      <td>0.7215388102864918</td>\n",
       "      <td>0.61735807860262</td>\n",
       "      <td>0.6170990027194461</td>\n",
       "      <td>8236</td>\n",
       "      <td>916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.7205617979135406</td>\n",
       "      <td>0.7205389769961325</td>\n",
       "      <td>0.6597573307575423</td>\n",
       "      <td>0.659235805423725</td>\n",
       "      <td>8900</td>\n",
       "      <td>989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.7275026823120871</td>\n",
       "      <td>0.7275741551526572</td>\n",
       "      <td>0.7351724156840094</td>\n",
       "      <td>0.7351723560793646</td>\n",
       "      <td>6523</td>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.6777397709964937</td>\n",
       "      <td>0.6781650052197966</td>\n",
       "      <td>0.6887573969434704</td>\n",
       "      <td>0.6880521147914187</td>\n",
       "      <td>7601</td>\n",
       "      <td>845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.7112438172580163</td>\n",
       "      <td>0.7111061998839717</td>\n",
       "      <td>0.6558089036894932</td>\n",
       "      <td>0.6558088457027812</td>\n",
       "      <td>8289</td>\n",
       "      <td>921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.660832698895421</td>\n",
       "      <td>0.6609532736684851</td>\n",
       "      <td>0.5879765378997706</td>\n",
       "      <td>0.5874877188096181</td>\n",
       "      <td>9199</td>\n",
       "      <td>1023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.7658478130974264</td>\n",
       "      <td>0.7659180858093797</td>\n",
       "      <td>0.6050646551724138</td>\n",
       "      <td>0.6039806460512096</td>\n",
       "      <td>8345</td>\n",
       "      <td>928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.6638365940031449</td>\n",
       "      <td>0.6641106259651226</td>\n",
       "      <td>0.6012168141592921</td>\n",
       "      <td>0.6014734532980792</td>\n",
       "      <td>8127</td>\n",
       "      <td>904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.5805073431878287</td>\n",
       "      <td>0.5792298410858745</td>\n",
       "      <td>0.4957983192561769</td>\n",
       "      <td>0.49921568416032186</td>\n",
       "      <td>7490</td>\n",
       "      <td>833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.7861611477354817</td>\n",
       "      <td>0.7859300637358331</td>\n",
       "      <td>0.6978661493695442</td>\n",
       "      <td>0.6985427234845758</td>\n",
       "      <td>9271</td>\n",
       "      <td>1031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.8217995032200455</td>\n",
       "      <td>0.8219482590124553</td>\n",
       "      <td>0.7289340115440679</td>\n",
       "      <td>0.7284044487222197</td>\n",
       "      <td>8858</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.9009216589861752</td>\n",
       "      <td>0.9009506520526689</td>\n",
       "      <td>0.7851382513200084</td>\n",
       "      <td>0.7852390438730267</td>\n",
       "      <td>7812</td>\n",
       "      <td>868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.8261264330912442</td>\n",
       "      <td>0.8259286662057801</td>\n",
       "      <td>0.7063329927280804</td>\n",
       "      <td>0.7073084435618813</td>\n",
       "      <td>8811</td>\n",
       "      <td>979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.8533197829670746</td>\n",
       "      <td>0.8532851933971468</td>\n",
       "      <td>0.7284595319454934</td>\n",
       "      <td>0.7284594658564443</td>\n",
       "      <td>10332</td>\n",
       "      <td>1149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Hour        Training_Acc         Training_F1         Testing_Acc  \\\n",
       "0      0  0.6643876334716534  0.6373365035663745  0.5967914439458898   \n",
       "1      1  0.7550963573377728   0.754469499020185  0.6883767523125321   \n",
       "2      2  0.7344007764320707  0.7348777705404956    0.60742357984901   \n",
       "3      3  0.7789513391847658  0.7791312528933637  0.6776929602115411   \n",
       "4      4  0.7884342142936462  0.7875614224526313  0.7201873931202409   \n",
       "5      5  0.7777827127667952  0.7773609263674345  0.6638689065913407   \n",
       "6      6    0.71275071658199  0.7125885554654813  0.6233894976208538   \n",
       "7      7  0.5243689659738091  0.5211813250114381  0.5407949790794979   \n",
       "8      8  0.6660167130697847  0.6659312105223107  0.5622389315064987   \n",
       "9      9  0.6802165448376908  0.6802107521368801  0.6717948719986484   \n",
       "10    10  0.7212846042925782  0.7215388102864918    0.61735807860262   \n",
       "11    11  0.7205617979135406  0.7205389769961325  0.6597573307575423   \n",
       "12    12  0.7275026823120871  0.7275741551526572  0.7351724156840094   \n",
       "13    13  0.6777397709964937  0.6781650052197966  0.6887573969434704   \n",
       "14    14  0.7112438172580163  0.7111061998839717  0.6558089036894932   \n",
       "15    15   0.660832698895421  0.6609532736684851  0.5879765378997706   \n",
       "16    16  0.7658478130974264  0.7659180858093797  0.6050646551724138   \n",
       "17    17  0.6638365940031449  0.6641106259651226  0.6012168141592921   \n",
       "18    18  0.5805073431878287  0.5792298410858745  0.4957983192561769   \n",
       "19    19  0.7861611477354817  0.7859300637358331  0.6978661493695442   \n",
       "20    20  0.8217995032200455  0.8219482590124553  0.7289340115440679   \n",
       "21    21  0.9009216589861752  0.9009506520526689  0.7851382513200084   \n",
       "22    22  0.8261264330912442  0.8259286662057801  0.7063329927280804   \n",
       "23    23  0.8533197829670746  0.8532851933971468  0.7284595319454934   \n",
       "\n",
       "             Testing_F1  Training_Sample  Testing_Sample  \n",
       "0    0.5967913855843365             8410             935  \n",
       "1    0.6848854310287981             8977             998  \n",
       "2    0.6629905008332698            10305            1145  \n",
       "3    0.6776929012893923            10604            1179  \n",
       "4    0.7192936870674055            10557            1174  \n",
       "5     0.663453481037268            11257            1251  \n",
       "6     0.623389438016209             9074            1009  \n",
       "7     0.545885609034215             8597             956  \n",
       "8    0.5618690470406286            10770            1197  \n",
       "9     0.671794811477009            10529            1170  \n",
       "10   0.6170990027194461             8236             916  \n",
       "11    0.659235805423725             8900             989  \n",
       "12   0.7351723560793646             6523             725  \n",
       "13   0.6880521147914187             7601             845  \n",
       "14   0.6558088457027812             8289             921  \n",
       "15   0.5874877188096181             9199            1023  \n",
       "16   0.6039806460512096             8345             928  \n",
       "17   0.6014734532980792             8127             904  \n",
       "18  0.49921568416032186             7490             833  \n",
       "19   0.6985427234845758             9271            1031  \n",
       "20   0.7284044487222197             8858             985  \n",
       "21   0.7852390438730267             7812             868  \n",
       "22   0.7073084435618813             8811             979  \n",
       "23   0.7284594658564443            10332            1149  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary_T = pd.concat([results, sample_size], axis=1)\n",
    "Summary_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary_T.to_csv(\"/home/tao/jupyter3/Tel_Data/Summarization_ECG_Classification.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "markers",
         "name": "10-CV Training",
         "type": "scatter",
         "uid": "537e2128-33b7-11e9-8c97-67fc199b385a",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23
         ],
         "y": [
          "0.6237812130544755",
          "0.715049571112932",
          "0.657981562278966",
          "0.7537721613360908",
          "0.7606801175423008",
          "0.6550590745178773",
          "0.7238263171334505",
          "0.5518204025353084",
          "0.5476323120219905",
          "0.6417988412615037",
          "0.6668892665763918",
          "0.6700561797752809",
          "0.7282692017903197",
          "0.6762268120811857",
          "0.6914585593353927",
          "0.6077291008920459",
          "0.6717795086592668",
          "0.6336286451612083",
          "0.5465954607414786",
          "0.7398878222221105",
          "0.7512418153754851",
          "0.8679595494111623",
          "0.7980365451328493",
          "0.8265582654672771"
         ]
        },
        {
         "mode": "markers",
         "name": "Testing",
         "opacity": 0.7,
         "type": "scatter",
         "uid": "537e22f4-33b7-11e9-8c97-67fc199b385a",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23
         ],
         "y": [
          "0.616577541254421",
          "0.6903807615230461",
          "0.5921397397091295",
          "0.6738761656864707",
          "0.7061328795537023",
          "0.6558752980925958",
          "0.6243805760080207",
          "0.5428870285405275",
          "0.5430242272347535",
          "0.6384615383596501",
          "0.6069868995633187",
          "0.6289180980654409",
          "0.7013793127290133",
          "0.691124259367497",
          "0.6400651469033911",
          "0.5874877840076607",
          "0.5775862068965517",
          "0.5719026548672567",
          "0.525210084248276",
          "0.7235693502611147",
          "0.7274111693885725",
          "0.7759216620076087",
          "0.7093973456291183",
          "0.744995645899893"
         ]
        }
       ],
       "layout": {
        "showlegend": true,
        "title": "Hourly Classification Results",
        "xaxis": {
         "title": "Time"
        },
        "yaxis": {
         "range": [
          0,
          0.95
         ],
         "title": "Accuracy"
        },
        "yaxis2": {
         "overlaying": "y",
         "range": [
          5000,
          12000
         ],
         "side": "right",
         "title": "Number of samples"
        }
       }
      },
      "text/html": [
       "<div id=\"59f808d9-7838-4670-a0a5-6d24f88d756c\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "        Plotly.plot(\n",
       "            '59f808d9-7838-4670-a0a5-6d24f88d756c',\n",
       "            [{\"mode\": \"markers\", \"name\": \"10-CV Training\", \"x\": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0], \"y\": [\"0.6237812130544755\", \"0.715049571112932\", \"0.657981562278966\", \"0.7537721613360908\", \"0.7606801175423008\", \"0.6550590745178773\", \"0.7238263171334505\", \"0.5518204025353084\", \"0.5476323120219905\", \"0.6417988412615037\", \"0.6668892665763918\", \"0.6700561797752809\", \"0.7282692017903197\", \"0.6762268120811857\", \"0.6914585593353927\", \"0.6077291008920459\", \"0.6717795086592668\", \"0.6336286451612083\", \"0.5465954607414786\", \"0.7398878222221105\", \"0.7512418153754851\", \"0.8679595494111623\", \"0.7980365451328493\", \"0.8265582654672771\"], \"type\": \"scatter\", \"uid\": \"537e2128-33b7-11e9-8c97-67fc199b385a\"}, {\"mode\": \"markers\", \"name\": \"Testing\", \"opacity\": 0.7, \"x\": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0], \"y\": [\"0.616577541254421\", \"0.6903807615230461\", \"0.5921397397091295\", \"0.6738761656864707\", \"0.7061328795537023\", \"0.6558752980925958\", \"0.6243805760080207\", \"0.5428870285405275\", \"0.5430242272347535\", \"0.6384615383596501\", \"0.6069868995633187\", \"0.6289180980654409\", \"0.7013793127290133\", \"0.691124259367497\", \"0.6400651469033911\", \"0.5874877840076607\", \"0.5775862068965517\", \"0.5719026548672567\", \"0.525210084248276\", \"0.7235693502611147\", \"0.7274111693885725\", \"0.7759216620076087\", \"0.7093973456291183\", \"0.744995645899893\"], \"type\": \"scatter\", \"uid\": \"537e22f4-33b7-11e9-8c97-67fc199b385a\"}],\n",
       "            {\"showlegend\": true, \"title\": \"Hourly Classification Results\", \"xaxis\": {\"title\": \"Time\"}, \"yaxis\": {\"range\": [0, 0.95], \"title\": \"Accuracy\"}, \"yaxis2\": {\"overlaying\": \"y\", \"range\": [5000, 12000], \"side\": \"right\", \"title\": \"Number of samples\"}},\n",
       "            {\"showLink\": false, \"linkText\": \"Export to plot.ly\"}\n",
       "        ).then(function () {return Plotly.addFrames('59f808d9-7838-4670-a0a5-6d24f88d756c',{});}).then(function(){Plotly.animate('59f808d9-7838-4670-a0a5-6d24f88d756c');})\n",
       "        });</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"59f808d9-7838-4670-a0a5-6d24f88d756c\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "        Plotly.plot(\n",
       "            '59f808d9-7838-4670-a0a5-6d24f88d756c',\n",
       "            [{\"mode\": \"markers\", \"name\": \"10-CV Training\", \"x\": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0], \"y\": [\"0.6237812130544755\", \"0.715049571112932\", \"0.657981562278966\", \"0.7537721613360908\", \"0.7606801175423008\", \"0.6550590745178773\", \"0.7238263171334505\", \"0.5518204025353084\", \"0.5476323120219905\", \"0.6417988412615037\", \"0.6668892665763918\", \"0.6700561797752809\", \"0.7282692017903197\", \"0.6762268120811857\", \"0.6914585593353927\", \"0.6077291008920459\", \"0.6717795086592668\", \"0.6336286451612083\", \"0.5465954607414786\", \"0.7398878222221105\", \"0.7512418153754851\", \"0.8679595494111623\", \"0.7980365451328493\", \"0.8265582654672771\"], \"type\": \"scatter\", \"uid\": \"537e2128-33b7-11e9-8c97-67fc199b385a\"}, {\"mode\": \"markers\", \"name\": \"Testing\", \"opacity\": 0.7, \"x\": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0], \"y\": [\"0.616577541254421\", \"0.6903807615230461\", \"0.5921397397091295\", \"0.6738761656864707\", \"0.7061328795537023\", \"0.6558752980925958\", \"0.6243805760080207\", \"0.5428870285405275\", \"0.5430242272347535\", \"0.6384615383596501\", \"0.6069868995633187\", \"0.6289180980654409\", \"0.7013793127290133\", \"0.691124259367497\", \"0.6400651469033911\", \"0.5874877840076607\", \"0.5775862068965517\", \"0.5719026548672567\", \"0.525210084248276\", \"0.7235693502611147\", \"0.7274111693885725\", \"0.7759216620076087\", \"0.7093973456291183\", \"0.744995645899893\"], \"type\": \"scatter\", \"uid\": \"537e22f4-33b7-11e9-8c97-67fc199b385a\"}],\n",
       "            {\"showlegend\": true, \"title\": \"Hourly Classification Results\", \"xaxis\": {\"title\": \"Time\"}, \"yaxis\": {\"range\": [0, 0.95], \"title\": \"Accuracy\"}, \"yaxis2\": {\"overlaying\": \"y\", \"range\": [5000, 12000], \"side\": \"right\", \"title\": \"Number of samples\"}},\n",
       "            {\"showLink\": false, \"linkText\": \"Export to plot.ly\"}\n",
       "        ).then(function () {return Plotly.addFrames('59f808d9-7838-4670-a0a5-6d24f88d756c',{});}).then(function(){Plotly.animate('59f808d9-7838-4670-a0a5-6d24f88d756c');})\n",
       "        });</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Plot summary\n",
    "trace1 = go.Scatter(\n",
    "    name='10-CV Training',\n",
    "    x=Summary_T2.Hour,\n",
    "    y=Summary_T2.Training_Acc,\n",
    "    mode='markers')\n",
    "trace2 = go.Scatter(\n",
    "    name='Testing',\n",
    "    x=Summary_T2.Hour,\n",
    "    y=Summary_T2.Testing_Acc,\n",
    "    mode='markers',\n",
    "    opacity=0.7)\n",
    "trace3 = go.Scatter(\n",
    "    name = 'Sample Size',\n",
    "    x=Summary_T2.Hour,\n",
    "    y=Summary_T2.Training_Sample,\n",
    "    yaxis='y2',\n",
    "    mode='markers')\n",
    "data = [trace1, trace2]\n",
    "layout = go.Layout(\n",
    "    yaxis=dict(title='Accuracy', range=[0,0.95]),\n",
    "    xaxis=dict(title='Time'),\n",
    "    yaxis2=dict(title='Number of samples',\n",
    "               overlaying='y',\n",
    "               side='right',\n",
    "               range=[5000,12000]),\n",
    "    title='Hourly Classification Results',\n",
    "    showlegend = True)\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "offline.iplot(fig, show_link=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "line": {
          "color": "darkorange",
          "width": 2
         },
         "mode": "lines",
         "name": "H23 AUC(area = 0.81)",
         "type": "scatter",
         "uid": "3d187242-3396-11e9-8c97-67fc199b385a",
         "x": [
          0,
          0,
          0.0020161290322580645,
          0.0020161290322580645,
          0.004032258064516129,
          0.004032258064516129,
          0.006048387096774193,
          0.006048387096774193,
          0.008064516129032258,
          0.008064516129032258,
          0.010080645161290322,
          0.010080645161290322,
          0.012096774193548387,
          0.012096774193548387,
          0.016129032258064516,
          0.016129032258064516,
          0.018145161290322582,
          0.018145161290322582,
          0.020161290322580645,
          0.020161290322580645,
          0.02217741935483871,
          0.02217741935483871,
          0.024193548387096774,
          0.024193548387096774,
          0.02620967741935484,
          0.02620967741935484,
          0.028225806451612902,
          0.028225806451612902,
          0.03024193548387097,
          0.03024193548387097,
          0.03225806451612903,
          0.03225806451612903,
          0.034274193548387094,
          0.034274193548387094,
          0.036290322580645164,
          0.036290322580645164,
          0.038306451612903226,
          0.038306451612903226,
          0.04032258064516129,
          0.04032258064516129,
          0.04233870967741935,
          0.04233870967741935,
          0.04435483870967742,
          0.04435483870967742,
          0.04838709677419355,
          0.04838709677419355,
          0.05040322580645161,
          0.05040322580645161,
          0.05443548387096774,
          0.05443548387096774,
          0.05846774193548387,
          0.05846774193548387,
          0.06451612903225806,
          0.06451612903225806,
          0.06854838709677419,
          0.06854838709677419,
          0.07258064516129033,
          0.07258064516129033,
          0.07459677419354839,
          0.07459677419354839,
          0.07661290322580645,
          0.07661290322580645,
          0.08064516129032258,
          0.08064516129032258,
          0.08266129032258064,
          0.08266129032258064,
          0.0846774193548387,
          0.0846774193548387,
          0.08669354838709678,
          0.08669354838709678,
          0.08870967741935484,
          0.08870967741935484,
          0.0907258064516129,
          0.0907258064516129,
          0.09274193548387097,
          0.09274193548387097,
          0.0967741935483871,
          0.0967741935483871,
          0.09879032258064516,
          0.09879032258064516,
          0.10080645161290322,
          0.10080645161290322,
          0.1028225806451613,
          0.1028225806451613,
          0.10483870967741936,
          0.10483870967741936,
          0.10887096774193548,
          0.10887096774193548,
          0.11088709677419355,
          0.11088709677419355,
          0.11491935483870967,
          0.11491935483870967,
          0.11693548387096774,
          0.11693548387096774,
          0.11895161290322581,
          0.11895161290322581,
          0.12096774193548387,
          0.12096774193548387,
          0.12298387096774194,
          0.12298387096774194,
          0.125,
          0.125,
          0.12903225806451613,
          0.12903225806451613,
          0.13306451612903225,
          0.13306451612903225,
          0.1350806451612903,
          0.1350806451612903,
          0.13709677419354838,
          0.13709677419354838,
          0.13911290322580644,
          0.13911290322580644,
          0.14112903225806453,
          0.14112903225806453,
          0.1431451612903226,
          0.1431451612903226,
          0.14516129032258066,
          0.14516129032258066,
          0.15120967741935484,
          0.15120967741935484,
          0.1532258064516129,
          0.1532258064516129,
          0.15524193548387097,
          0.15524193548387097,
          0.15725806451612903,
          0.15725806451612903,
          0.1592741935483871,
          0.1592741935483871,
          0.16532258064516128,
          0.16532258064516128,
          0.1693548387096774,
          0.1693548387096774,
          0.17137096774193547,
          0.17137096774193547,
          0.17338709677419356,
          0.17338709677419356,
          0.17540322580645162,
          0.17540322580645162,
          0.1774193548387097,
          0.1774193548387097,
          0.17943548387096775,
          0.17943548387096775,
          0.1814516129032258,
          0.1814516129032258,
          0.18548387096774194,
          0.18548387096774194,
          0.1875,
          0.1875,
          0.18951612903225806,
          0.18951612903225806,
          0.19153225806451613,
          0.19153225806451613,
          0.1935483870967742,
          0.1935483870967742,
          0.19556451612903225,
          0.19556451612903225,
          0.1975806451612903,
          0.1975806451612903,
          0.20362903225806453,
          0.20362903225806453,
          0.20766129032258066,
          0.20766129032258066,
          0.20967741935483872,
          0.20967741935483872,
          0.21169354838709678,
          0.21169354838709678,
          0.21370967741935484,
          0.21370967741935484,
          0.2157258064516129,
          0.2157258064516129,
          0.22580645161290322,
          0.22580645161290322,
          0.23588709677419356,
          0.23588709677419356,
          0.23790322580645162,
          0.23790322580645162,
          0.2399193548387097,
          0.2399193548387097,
          0.24193548387096775,
          0.24193548387096775,
          0.2439516129032258,
          0.2439516129032258,
          0.2540322580645161,
          0.2540322580645161,
          0.2560483870967742,
          0.2560483870967742,
          0.25806451612903225,
          0.25806451612903225,
          0.2600806451612903,
          0.2600806451612903,
          0.26411290322580644,
          0.26411290322580644,
          0.2701612903225806,
          0.2701612903225806,
          0.2721774193548387,
          0.2721774193548387,
          0.2762096774193548,
          0.2762096774193548,
          0.2842741935483871,
          0.2842741935483871,
          0.28830645161290325,
          0.28830645161290325,
          0.2903225806451613,
          0.2903225806451613,
          0.29838709677419356,
          0.29838709677419356,
          0.3004032258064516,
          0.3004032258064516,
          0.30443548387096775,
          0.30443548387096775,
          0.3084677419354839,
          0.3084677419354839,
          0.3286290322580645,
          0.3286290322580645,
          0.33064516129032256,
          0.33064516129032256,
          0.3326612903225806,
          0.3326612903225806,
          0.3346774193548387,
          0.3346774193548387,
          0.3387096774193548,
          0.3387096774193548,
          0.3467741935483871,
          0.3467741935483871,
          0.3528225806451613,
          0.3528225806451613,
          0.35685483870967744,
          0.35685483870967744,
          0.3588709677419355,
          0.3588709677419355,
          0.3629032258064516,
          0.3629032258064516,
          0.3649193548387097,
          0.3649193548387097,
          0.3709677419354839,
          0.3709677419354839,
          0.37298387096774194,
          0.37298387096774194,
          0.3790322580645161,
          0.3790322580645161,
          0.38306451612903225,
          0.38306451612903225,
          0.3850806451612903,
          0.3850806451612903,
          0.39314516129032256,
          0.39314516129032256,
          0.39919354838709675,
          0.39919354838709675,
          0.4032258064516129,
          0.4032258064516129,
          0.40524193548387094,
          0.40524193548387094,
          0.4092741935483871,
          0.4092741935483871,
          0.4254032258064516,
          0.4254032258064516,
          0.43951612903225806,
          0.43951612903225806,
          0.4415322580645161,
          0.4415322580645161,
          0.4495967741935484,
          0.4495967741935484,
          0.4576612903225806,
          0.4576612903225806,
          0.4637096774193548,
          0.4637096774193548,
          0.4657258064516129,
          0.4657258064516129,
          0.47580645161290325,
          0.47580645161290325,
          0.4838709677419355,
          0.4838709677419355,
          0.48588709677419356,
          0.48588709677419356,
          0.49193548387096775,
          0.49193548387096775,
          0.49798387096774194,
          0.49798387096774194,
          0.5040322580645161,
          0.5040322580645161,
          0.5100806451612904,
          0.5100806451612904,
          0.5161290322580645,
          0.5161290322580645,
          0.5181451612903226,
          0.5181451612903226,
          0.5241935483870968,
          0.5241935483870968,
          0.5282258064516129,
          0.5282258064516129,
          0.530241935483871,
          0.530241935483871,
          0.532258064516129,
          0.532258064516129,
          0.5342741935483871,
          0.5342741935483871,
          0.5463709677419355,
          0.5463709677419355,
          0.5645161290322581,
          0.5645161290322581,
          0.5705645161290323,
          0.5705645161290323,
          0.5725806451612904,
          0.5725806451612904,
          0.5766129032258065,
          0.5766129032258065,
          0.5786290322580645,
          0.5786290322580645,
          0.6008064516129032,
          0.6008064516129032,
          0.6068548387096774,
          0.6068548387096774,
          0.6129032258064516,
          0.6129032258064516,
          0.625,
          0.625,
          0.6350806451612904,
          0.6350806451612904,
          0.6370967741935484,
          0.6370967741935484,
          0.6391129032258065,
          0.6391129032258065,
          0.6471774193548387,
          0.6471774193548387,
          0.6592741935483871,
          0.6592741935483871,
          0.6653225806451613,
          0.6653225806451613,
          0.6673387096774194,
          0.6673387096774194,
          0.6774193548387096,
          0.6774193548387096,
          0.7096774193548387,
          0.7096774193548387,
          0.7258064516129032,
          0.7258064516129032,
          0.7278225806451613,
          0.7278225806451613,
          0.7439516129032258,
          0.7439516129032258,
          0.7479838709677419,
          0.7479838709677419,
          0.75,
          0.75,
          0.7701612903225806,
          0.7701612903225806,
          0.7762096774193549,
          0.7762096774193549,
          0.7782258064516129,
          0.7782258064516129,
          0.7983870967741935,
          0.7983870967741935,
          0.8004032258064516,
          0.8004032258064516,
          0.8125,
          0.8125,
          0.8185483870967742,
          0.8185483870967742,
          0.8366935483870968,
          0.8366935483870968,
          0.8508064516129032,
          0.8508064516129032,
          0.8548387096774194,
          0.8548387096774194,
          0.8629032258064516,
          0.8629032258064516,
          0.8689516129032258,
          0.8689516129032258,
          0.875,
          0.875,
          0.8790322580645161,
          0.8790322580645161,
          0.8850806451612904,
          0.8850806451612904,
          0.8870967741935484,
          0.8870967741935484,
          0.9455645161290323,
          0.9455645161290323,
          0.9536290322580645,
          0.9536290322580645,
          0.9637096774193549,
          0.9637096774193549,
          0.9979838709677419,
          0.9979838709677419,
          1,
          1
         ],
         "y": [
          0.0015313935681470138,
          0.022970903522205207,
          0.022970903522205207,
          0.07197549770290965,
          0.07197549770290965,
          0.07963246554364471,
          0.07963246554364471,
          0.09494640122511486,
          0.09494640122511486,
          0.10107197549770292,
          0.10107197549770292,
          0.11638591117917305,
          0.11638591117917305,
          0.13935681470137826,
          0.13935681470137826,
          0.1562021439509954,
          0.1562021439509954,
          0.19295558958652373,
          0.19295558958652373,
          0.21592649310872894,
          0.21592649310872894,
          0.21745788667687596,
          0.21745788667687596,
          0.23889739663093415,
          0.23889739663093415,
          0.26033690658499237,
          0.26033690658499237,
          0.28483920367534454,
          0.28483920367534454,
          0.2986217457886677,
          0.2986217457886677,
          0.30474732006125577,
          0.30474732006125577,
          0.3108728943338438,
          0.3108728943338438,
          0.3124042879019908,
          0.3124042879019908,
          0.32465543644716693,
          0.32465543644716693,
          0.32618683001531396,
          0.32618683001531396,
          0.32924961715160794,
          0.32924961715160794,
          0.3445635528330781,
          0.3445635528330781,
          0.3614088820826952,
          0.3614088820826952,
          0.36294027565084225,
          0.36294027565084225,
          0.38437978560490044,
          0.38437978560490044,
          0.39509954058192953,
          0.39509954058192953,
          0.39663093415007655,
          0.39663093415007655,
          0.4211332312404288,
          0.4211332312404288,
          0.4349157733537519,
          0.4349157733537519,
          0.43644716692189894,
          0.43644716692189894,
          0.43950995405819293,
          0.43950995405819293,
          0.445635528330781,
          0.445635528330781,
          0.44869831546707506,
          0.44869831546707506,
          0.4548238897396631,
          0.4548238897396631,
          0.45788667687595713,
          0.45788667687595713,
          0.46248085758039814,
          0.46248085758039814,
          0.4655436447166922,
          0.4655436447166922,
          0.47320061255742724,
          0.47320061255742724,
          0.4762633996937213,
          0.4762633996937213,
          0.4777947932618683,
          0.4777947932618683,
          0.4793261868300153,
          0.4793261868300153,
          0.4885145482388974,
          0.4885145482388974,
          0.5206738131699847,
          0.5206738131699847,
          0.5252679938744257,
          0.5252679938744257,
          0.5267993874425727,
          0.5267993874425727,
          0.5467075038284839,
          0.5467075038284839,
          0.5482388973966309,
          0.5482388973966309,
          0.5497702909647779,
          0.5497702909647779,
          0.554364471669219,
          0.554364471669219,
          0.55895865237366,
          0.55895865237366,
          0.5604900459418071,
          0.5604900459418071,
          0.5620214395099541,
          0.5620214395099541,
          0.5635528330781011,
          0.5635528330781011,
          0.5666156202143952,
          0.5666156202143952,
          0.5681470137825421,
          0.5681470137825421,
          0.5696784073506891,
          0.5696784073506891,
          0.5727411944869831,
          0.5727411944869831,
          0.5819295558958653,
          0.5819295558958653,
          0.5911179173047473,
          0.5911179173047473,
          0.5972434915773354,
          0.5972434915773354,
          0.6018376722817764,
          0.6018376722817764,
          0.6033690658499234,
          0.6033690658499234,
          0.6079632465543645,
          0.6079632465543645,
          0.6110260336906586,
          0.6110260336906586,
          0.6140888208269525,
          0.6140888208269525,
          0.6156202143950995,
          0.6156202143950995,
          0.6263399693721287,
          0.6263399693721287,
          0.6339969372128637,
          0.6339969372128637,
          0.6385911179173047,
          0.6385911179173047,
          0.6477794793261868,
          0.6477794793261868,
          0.6493108728943339,
          0.6493108728943339,
          0.6508422664624809,
          0.6508422664624809,
          0.655436447166922,
          0.655436447166922,
          0.6584992343032159,
          0.6584992343032159,
          0.666156202143951,
          0.666156202143951,
          0.667687595712098,
          0.667687595712098,
          0.669218989280245,
          0.669218989280245,
          0.6707503828483921,
          0.6707503828483921,
          0.6722817764165391,
          0.6722817764165391,
          0.6860643185298622,
          0.6860643185298622,
          0.6891271056661562,
          0.6891271056661562,
          0.6937212863705973,
          0.6937212863705973,
          0.6983154670750383,
          0.6983154670750383,
          0.7044410413476263,
          0.7044410413476263,
          0.7105666156202144,
          0.7105666156202144,
          0.7120980091883614,
          0.7120980091883614,
          0.7166921898928025,
          0.7166921898928025,
          0.7320061255742726,
          0.7320061255742726,
          0.7366003062787136,
          0.7366003062787136,
          0.7396630934150077,
          0.7396630934150077,
          0.7411944869831547,
          0.7411944869831547,
          0.7457886676875957,
          0.7457886676875957,
          0.7519142419601837,
          0.7519142419601837,
          0.7534456355283308,
          0.7534456355283308,
          0.7565084226646248,
          0.7565084226646248,
          0.7580398162327718,
          0.7580398162327718,
          0.7595712098009189,
          0.7595712098009189,
          0.7611026033690659,
          0.7611026033690659,
          0.7626339969372129,
          0.7626339969372129,
          0.7641653905053599,
          0.7641653905053599,
          0.7656967840735069,
          0.7656967840735069,
          0.777947932618683,
          0.777947932618683,
          0.781010719754977,
          0.781010719754977,
          0.7840735068912711,
          0.7840735068912711,
          0.7856049004594181,
          0.7856049004594181,
          0.7871362940275651,
          0.7871362940275651,
          0.7901990811638591,
          0.7901990811638591,
          0.7917304747320061,
          0.7917304747320061,
          0.7932618683001531,
          0.7932618683001531,
          0.7947932618683001,
          0.7947932618683001,
          0.8039816232771823,
          0.8039816232771823,
          0.8055130168453293,
          0.8055130168453293,
          0.8162327718223583,
          0.8162327718223583,
          0.8177641653905053,
          0.8177641653905053,
          0.8192955589586524,
          0.8192955589586524,
          0.8269525267993875,
          0.8269525267993875,
          0.8315467075038285,
          0.8315467075038285,
          0.8346094946401225,
          0.8346094946401225,
          0.8361408882082695,
          0.8361408882082695,
          0.8407350689127105,
          0.8407350689127105,
          0.8437978560490046,
          0.8437978560490046,
          0.8453292496171516,
          0.8453292496171516,
          0.8483920367534457,
          0.8483920367534457,
          0.8545176110260337,
          0.8545176110260337,
          0.8560490045941807,
          0.8560490045941807,
          0.8575803981623277,
          0.8575803981623277,
          0.8606431852986217,
          0.8606431852986217,
          0.8621745788667687,
          0.8621745788667687,
          0.8683001531393568,
          0.8683001531393568,
          0.8698315467075038,
          0.8698315467075038,
          0.8713629402756509,
          0.8713629402756509,
          0.8744257274119449,
          0.8744257274119449,
          0.8805513016845329,
          0.8805513016845329,
          0.8820826952526799,
          0.8820826952526799,
          0.8836140888208269,
          0.8836140888208269,
          0.885145482388974,
          0.885145482388974,
          0.886676875957121,
          0.886676875957121,
          0.888208269525268,
          0.888208269525268,
          0.891271056661562,
          0.891271056661562,
          0.892802450229709,
          0.892802450229709,
          0.8943338437978561,
          0.8943338437978561,
          0.8973966309341501,
          0.8973966309341501,
          0.8989280245022971,
          0.8989280245022971,
          0.900459418070444,
          0.900459418070444,
          0.9035222052067381,
          0.9035222052067381,
          0.9081163859111792,
          0.9081163859111792,
          0.9096477794793262,
          0.9096477794793262,
          0.9111791730474732,
          0.9111791730474732,
          0.9127105666156202,
          0.9127105666156202,
          0.9142419601837672,
          0.9142419601837672,
          0.9157733537519143,
          0.9157733537519143,
          0.9173047473200613,
          0.9173047473200613,
          0.9203675344563553,
          0.9203675344563553,
          0.9218989280245024,
          0.9218989280245024,
          0.9234303215926493,
          0.9234303215926493,
          0.9249617151607963,
          0.9249617151607963,
          0.9264931087289433,
          0.9264931087289433,
          0.9295558958652373,
          0.9295558958652373,
          0.9326186830015314,
          0.9326186830015314,
          0.9341500765696784,
          0.9341500765696784,
          0.9356814701378254,
          0.9356814701378254,
          0.9372128637059725,
          0.9372128637059725,
          0.9387442572741195,
          0.9387442572741195,
          0.9402756508422665,
          0.9402756508422665,
          0.9418070444104135,
          0.9418070444104135,
          0.9433384379785605,
          0.9433384379785605,
          0.9448698315467075,
          0.9448698315467075,
          0.9464012251148545,
          0.9464012251148545,
          0.9479326186830015,
          0.9479326186830015,
          0.9494640122511485,
          0.9494640122511485,
          0.9525267993874426,
          0.9525267993874426,
          0.9540581929555896,
          0.9540581929555896,
          0.9571209800918836,
          0.9571209800918836,
          0.9586523736600306,
          0.9586523736600306,
          0.9601837672281777,
          0.9601837672281777,
          0.9617151607963247,
          0.9617151607963247,
          0.9632465543644717,
          0.9632465543644717,
          0.9647779479326187,
          0.9647779479326187,
          0.9663093415007658,
          0.9663093415007658,
          0.9709035222052067,
          0.9709035222052067,
          0.9724349157733537,
          0.9724349157733537,
          0.9739663093415007,
          0.9739663093415007,
          0.9770290964777948,
          0.9770290964777948,
          0.9785604900459418,
          0.9785604900459418,
          0.9800918836140888,
          0.9800918836140888,
          0.9831546707503829,
          0.9831546707503829,
          0.9862174578866769,
          0.9862174578866769,
          0.9877488514548239,
          0.9877488514548239,
          0.9892802450229708,
          0.9892802450229708,
          0.9908116385911179,
          0.9908116385911179,
          0.9938744257274119,
          0.9938744257274119,
          0.998468606431853,
          0.998468606431853,
          1
         ]
        },
        {
         "line": {
          "color": "navy",
          "dash": "dash",
          "width": 2
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter",
         "uid": "3d187404-3396-11e9-8c97-67fc199b385a",
         "x": [
          0,
          1
         ],
         "y": [
          0,
          1
         ]
        },
        {
         "line": {
          "color": "cyan",
          "width": 2
         },
         "mode": "lines",
         "name": "H21 AUC(area = 0.86)",
         "type": "scatter",
         "uid": "3d1874fe-3396-11e9-8c97-67fc199b385a",
         "x": [
          0,
          0,
          0.002150537634408602,
          0.002150537634408602,
          0.004301075268817204,
          0.004301075268817204,
          0.0064516129032258064,
          0.0064516129032258064,
          0.008602150537634409,
          0.008602150537634409,
          0.010752688172043012,
          0.010752688172043012,
          0.012903225806451613,
          0.012903225806451613,
          0.015053763440860216,
          0.015053763440860216,
          0.017204301075268817,
          0.017204301075268817,
          0.01935483870967742,
          0.01935483870967742,
          0.021505376344086023,
          0.021505376344086023,
          0.023655913978494623,
          0.023655913978494623,
          0.025806451612903226,
          0.025806451612903226,
          0.02795698924731183,
          0.02795698924731183,
          0.030107526881720432,
          0.030107526881720432,
          0.03225806451612903,
          0.03225806451612903,
          0.034408602150537634,
          0.034408602150537634,
          0.043010752688172046,
          0.043010752688172046,
          0.04516129032258064,
          0.04516129032258064,
          0.04946236559139785,
          0.04946236559139785,
          0.05161290322580645,
          0.05161290322580645,
          0.053763440860215055,
          0.053763440860215055,
          0.05591397849462366,
          0.05591397849462366,
          0.05806451612903226,
          0.05806451612903226,
          0.06236559139784946,
          0.06236559139784946,
          0.06451612903225806,
          0.06451612903225806,
          0.06881720430107527,
          0.06881720430107527,
          0.07096774193548387,
          0.07096774193548387,
          0.07741935483870968,
          0.07741935483870968,
          0.08172043010752689,
          0.08172043010752689,
          0.08387096774193549,
          0.08387096774193549,
          0.08602150537634409,
          0.08602150537634409,
          0.09032258064516129,
          0.09032258064516129,
          0.09462365591397849,
          0.09462365591397849,
          0.0967741935483871,
          0.0967741935483871,
          0.0989247311827957,
          0.0989247311827957,
          0.1032258064516129,
          0.1032258064516129,
          0.1053763440860215,
          0.1053763440860215,
          0.10752688172043011,
          0.10752688172043011,
          0.12043010752688173,
          0.12043010752688173,
          0.12473118279569892,
          0.12473118279569892,
          0.13118279569892474,
          0.13118279569892474,
          0.13333333333333333,
          0.13333333333333333,
          0.13548387096774195,
          0.13548387096774195,
          0.13978494623655913,
          0.13978494623655913,
          0.14193548387096774,
          0.14193548387096774,
          0.14408602150537633,
          0.14408602150537633,
          0.14623655913978495,
          0.14623655913978495,
          0.15053763440860216,
          0.15053763440860216,
          0.15483870967741936,
          0.15483870967741936,
          0.15913978494623657,
          0.15913978494623657,
          0.16129032258064516,
          0.16129032258064516,
          0.16344086021505377,
          0.16344086021505377,
          0.16989247311827957,
          0.16989247311827957,
          0.17204301075268819,
          0.17204301075268819,
          0.17634408602150536,
          0.17634408602150536,
          0.17849462365591398,
          0.17849462365591398,
          0.1827956989247312,
          0.1827956989247312,
          0.1870967741935484,
          0.1870967741935484,
          0.18924731182795698,
          0.18924731182795698,
          0.1913978494623656,
          0.1913978494623656,
          0.1935483870967742,
          0.1935483870967742,
          0.1978494623655914,
          0.1978494623655914,
          0.2,
          0.2,
          0.20430107526881722,
          0.20430107526881722,
          0.2129032258064516,
          0.2129032258064516,
          0.221505376344086,
          0.221505376344086,
          0.22580645161290322,
          0.22580645161290322,
          0.22795698924731184,
          0.22795698924731184,
          0.24086021505376345,
          0.24086021505376345,
          0.24731182795698925,
          0.24731182795698925,
          0.24946236559139784,
          0.24946236559139784,
          0.2537634408602151,
          0.2537634408602151,
          0.25806451612903225,
          0.25806451612903225,
          0.2623655913978495,
          0.2623655913978495,
          0.2645161290322581,
          0.2645161290322581,
          0.2817204301075269,
          0.2817204301075269,
          0.2838709677419355,
          0.2838709677419355,
          0.2924731182795699,
          0.2924731182795699,
          0.2946236559139785,
          0.2946236559139785,
          0.2967741935483871,
          0.2967741935483871,
          0.3010752688172043,
          0.3010752688172043,
          0.3118279569892473,
          0.3118279569892473,
          0.31827956989247314,
          0.31827956989247314,
          0.3204301075268817,
          0.3204301075268817,
          0.3225806451612903,
          0.3225806451612903,
          0.32688172043010755,
          0.32688172043010755,
          0.33763440860215055,
          0.33763440860215055,
          0.34408602150537637,
          0.34408602150537637,
          0.35053763440860214,
          0.35053763440860214,
          0.3548387096774194,
          0.3548387096774194,
          0.35913978494623655,
          0.35913978494623655,
          0.3720430107526882,
          0.3720430107526882,
          0.3935483870967742,
          0.3935483870967742,
          0.3978494623655914,
          0.3978494623655914,
          0.432258064516129,
          0.432258064516129,
          0.443010752688172,
          0.443010752688172,
          0.46021505376344085,
          0.46021505376344085,
          0.4645161290322581,
          0.4645161290322581,
          0.46881720430107526,
          0.46881720430107526,
          0.4838709677419355,
          0.4838709677419355,
          0.5032258064516129,
          0.5032258064516129,
          0.5075268817204301,
          0.5075268817204301,
          0.5204301075268817,
          0.5204301075268817,
          0.5225806451612903,
          0.5225806451612903,
          0.5548387096774193,
          0.5548387096774193,
          0.578494623655914,
          0.578494623655914,
          0.5935483870967742,
          0.5935483870967742,
          0.610752688172043,
          0.610752688172043,
          0.6236559139784946,
          0.6236559139784946,
          0.6516129032258065,
          0.6516129032258065,
          0.6623655913978495,
          0.6623655913978495,
          0.6688172043010753,
          0.6688172043010753,
          0.6989247311827957,
          0.6989247311827957,
          0.7268817204301076,
          0.7268817204301076,
          0.7397849462365591,
          0.7397849462365591,
          0.7462365591397849,
          0.7462365591397849,
          0.7505376344086021,
          0.7505376344086021,
          0.7526881720430108,
          0.7526881720430108,
          0.7591397849462366,
          0.7591397849462366,
          0.7720430107526882,
          0.7720430107526882,
          0.7849462365591398,
          0.7849462365591398,
          0.8129032258064516,
          0.8129032258064516,
          0.832258064516129,
          0.832258064516129,
          0.8387096774193549,
          0.8387096774193549,
          0.8946236559139785,
          0.8946236559139785,
          0.9483870967741935,
          0.9483870967741935,
          1
         ],
         "y": [
          0.0024813895781637717,
          0.12406947890818859,
          0.12406947890818859,
          0.22084367245657568,
          0.22084367245657568,
          0.2555831265508685,
          0.2555831265508685,
          0.29280397022332505,
          0.29280397022332505,
          0.2977667493796526,
          0.2977667493796526,
          0.31265508684863524,
          0.31265508684863524,
          0.3250620347394541,
          0.3250620347394541,
          0.34739454094292804,
          0.34739454094292804,
          0.3598014888337469,
          0.3598014888337469,
          0.36228287841191065,
          0.36228287841191065,
          0.38213399503722084,
          0.38213399503722084,
          0.4044665012406948,
          0.4044665012406948,
          0.4218362282878412,
          0.4218362282878412,
          0.43672456575682383,
          0.43672456575682383,
          0.4466501240694789,
          0.4466501240694789,
          0.4491315136476427,
          0.4491315136476427,
          0.46153846153846156,
          0.46153846153846156,
          0.4640198511166253,
          0.4640198511166253,
          0.47146401985111663,
          0.47146401985111663,
          0.47642679900744417,
          0.47642679900744417,
          0.4838709677419355,
          0.4838709677419355,
          0.48883374689826303,
          0.48883374689826303,
          0.49627791563275436,
          0.49627791563275436,
          0.4987593052109181,
          0.4987593052109181,
          0.511166253101737,
          0.511166253101737,
          0.5161290322580645,
          0.5161290322580645,
          0.5260545905707196,
          0.5260545905707196,
          0.5310173697270472,
          0.5310173697270472,
          0.5558312655086849,
          0.5558312655086849,
          0.5632754342431762,
          0.5632754342431762,
          0.56575682382134,
          0.56575682382134,
          0.5732009925558312,
          0.5732009925558312,
          0.5831265508684863,
          0.5831265508684863,
          0.5856079404466501,
          0.5856079404466501,
          0.5955334987593052,
          0.5955334987593052,
          0.607940446650124,
          0.607940446650124,
          0.6104218362282878,
          0.6104218362282878,
          0.6129032258064516,
          0.6129032258064516,
          0.6203473945409429,
          0.6203473945409429,
          0.6228287841191067,
          0.6228287841191067,
          0.630272952853598,
          0.630272952853598,
          0.6327543424317618,
          0.6327543424317618,
          0.6352357320099256,
          0.6352357320099256,
          0.6377171215880894,
          0.6377171215880894,
          0.6426799007444168,
          0.6426799007444168,
          0.6451612903225806,
          0.6451612903225806,
          0.6550868486352357,
          0.6550868486352357,
          0.6600496277915633,
          0.6600496277915633,
          0.6674937965260546,
          0.6674937965260546,
          0.6799007444168734,
          0.6799007444168734,
          0.6823821339950372,
          0.6823821339950372,
          0.6898263027295285,
          0.6898263027295285,
          0.6923076923076923,
          0.6923076923076923,
          0.6947890818858561,
          0.6947890818858561,
          0.6997518610421837,
          0.6997518610421837,
          0.7121588089330024,
          0.7121588089330024,
          0.7146401985111662,
          0.7146401985111662,
          0.7220843672456576,
          0.7220843672456576,
          0.7270471464019851,
          0.7270471464019851,
          0.7320099255583127,
          0.7320099255583127,
          0.7344913151364765,
          0.7344913151364765,
          0.7543424317617866,
          0.7543424317617866,
          0.7568238213399504,
          0.7568238213399504,
          0.7692307692307693,
          0.7692307692307693,
          0.771712158808933,
          0.771712158808933,
          0.7766749379652605,
          0.7766749379652605,
          0.7791563275434243,
          0.7791563275434243,
          0.7915632754342432,
          0.7915632754342432,
          0.794044665012407,
          0.794044665012407,
          0.8014888337468983,
          0.8014888337468983,
          0.8064516129032258,
          0.8064516129032258,
          0.8138957816377171,
          0.8138957816377171,
          0.8163771712158809,
          0.8163771712158809,
          0.8188585607940446,
          0.8188585607940446,
          0.8213399503722084,
          0.8213399503722084,
          0.8238213399503722,
          0.8238213399503722,
          0.8287841191066998,
          0.8287841191066998,
          0.8312655086848635,
          0.8312655086848635,
          0.8337468982630273,
          0.8337468982630273,
          0.8387096774193549,
          0.8387096774193549,
          0.8411910669975186,
          0.8411910669975186,
          0.8436724565756824,
          0.8436724565756824,
          0.8486352357320099,
          0.8486352357320099,
          0.8535980148883374,
          0.8535980148883374,
          0.858560794044665,
          0.858560794044665,
          0.8635235732009926,
          0.8635235732009926,
          0.8660049627791563,
          0.8660049627791563,
          0.8684863523573201,
          0.8684863523573201,
          0.8759305210918115,
          0.8759305210918115,
          0.8808933002481389,
          0.8808933002481389,
          0.8858560794044665,
          0.8858560794044665,
          0.8957816377171216,
          0.8957816377171216,
          0.9007444168734491,
          0.9007444168734491,
          0.9081885856079405,
          0.9081885856079405,
          0.9106699751861043,
          0.9106699751861043,
          0.913151364764268,
          0.913151364764268,
          0.9156327543424317,
          0.9156327543424317,
          0.9181141439205955,
          0.9181141439205955,
          0.9205955334987593,
          0.9205955334987593,
          0.9230769230769231,
          0.9230769230769231,
          0.9255583126550868,
          0.9255583126550868,
          0.9330024813895782,
          0.9330024813895782,
          0.9379652605459057,
          0.9379652605459057,
          0.9404466501240695,
          0.9404466501240695,
          0.9429280397022333,
          0.9429280397022333,
          0.9454094292803971,
          0.9454094292803971,
          0.9478908188585607,
          0.9478908188585607,
          0.9503722084367245,
          0.9503722084367245,
          0.9553349875930521,
          0.9553349875930521,
          0.9578163771712159,
          0.9578163771712159,
          0.9602977667493796,
          0.9602977667493796,
          0.9627791563275434,
          0.9627791563275434,
          0.9652605459057072,
          0.9652605459057072,
          0.967741935483871,
          0.967741935483871,
          0.9702233250620348,
          0.9702233250620348,
          0.9727047146401985,
          0.9727047146401985,
          0.9751861042183623,
          0.9751861042183623,
          0.9776674937965261,
          0.9776674937965261,
          0.9801488833746899,
          0.9801488833746899,
          0.9826302729528535,
          0.9826302729528535,
          0.9851116625310173,
          0.9851116625310173,
          0.9875930521091811,
          0.9875930521091811,
          0.9900744416873449,
          0.9900744416873449,
          0.9925558312655087,
          0.9925558312655087,
          0.9950372208436724,
          0.9950372208436724,
          0.9975186104218362,
          0.9975186104218362,
          1,
          1
         ]
        },
        {
         "line": {
          "color": "green",
          "width": 2
         },
         "mode": "lines",
         "name": "H18 AUC(area = 0.52)",
         "type": "scatter",
         "uid": "3d1875d0-3396-11e9-8c97-67fc199b385a",
         "x": [
          0,
          0.00234192037470726,
          0.00234192037470726,
          0.00936768149882904,
          0.00936768149882904,
          0.0117096018735363,
          0.0117096018735363,
          0.01873536299765808,
          0.01873536299765808,
          0.02107728337236534,
          0.02107728337236534,
          0.0234192037470726,
          0.0234192037470726,
          0.02576112412177986,
          0.02576112412177986,
          0.02810304449648712,
          0.02810304449648712,
          0.0351288056206089,
          0.0351288056206089,
          0.03747072599531616,
          0.03747072599531616,
          0.04215456674473068,
          0.04215456674473068,
          0.05152224824355972,
          0.05152224824355972,
          0.053864168618266976,
          0.053864168618266976,
          0.05620608899297424,
          0.05620608899297424,
          0.0585480093676815,
          0.0585480093676815,
          0.06557377049180328,
          0.06557377049180328,
          0.06791569086651054,
          0.06791569086651054,
          0.0702576112412178,
          0.0702576112412178,
          0.07494145199063232,
          0.07494145199063232,
          0.08196721311475409,
          0.08196721311475409,
          0.08430913348946135,
          0.08430913348946135,
          0.08665105386416862,
          0.08665105386416862,
          0.09133489461358314,
          0.09133489461358314,
          0.10070257611241218,
          0.10070257611241218,
          0.10304449648711944,
          0.10304449648711944,
          0.1053864168618267,
          0.1053864168618267,
          0.117096018735363,
          0.117096018735363,
          0.12177985948477751,
          0.12177985948477751,
          0.1288056206088993,
          0.1288056206088993,
          0.13348946135831383,
          0.13348946135831383,
          0.1358313817330211,
          0.1358313817330211,
          0.1451990632318501,
          0.1451990632318501,
          0.1522248243559719,
          0.1522248243559719,
          0.15690866510538642,
          0.15690866510538642,
          0.16393442622950818,
          0.16393442622950818,
          0.16627634660421545,
          0.16627634660421545,
          0.17096018735362997,
          0.17096018735362997,
          0.1756440281030445,
          0.1756440281030445,
          0.18266978922716628,
          0.18266978922716628,
          0.1920374707259953,
          0.1920374707259953,
          0.19437939110070257,
          0.19437939110070257,
          0.19672131147540983,
          0.19672131147540983,
          0.20374707259953162,
          0.20374707259953162,
          0.20608899297423888,
          0.20608899297423888,
          0.20843091334894615,
          0.20843091334894615,
          0.21311475409836064,
          0.21311475409836064,
          0.2154566744730679,
          0.2154566744730679,
          0.21779859484777517,
          0.21779859484777517,
          0.22014051522248243,
          0.22014051522248243,
          0.2224824355971897,
          0.2224824355971897,
          0.22482435597189696,
          0.22482435597189696,
          0.22950819672131148,
          0.22950819672131148,
          0.23185011709601874,
          0.23185011709601874,
          0.234192037470726,
          0.234192037470726,
          0.23653395784543327,
          0.23653395784543327,
          0.2388758782201405,
          0.2388758782201405,
          0.2529274004683841,
          0.2529274004683841,
          0.25526932084309134,
          0.25526932084309134,
          0.25995316159250587,
          0.25995316159250587,
          0.2646370023419204,
          0.2646370023419204,
          0.26697892271662765,
          0.26697892271662765,
          0.2693208430913349,
          0.2693208430913349,
          0.27400468384074944,
          0.27400468384074944,
          0.27634660421545665,
          0.27634660421545665,
          0.2786885245901639,
          0.2786885245901639,
          0.28337236533957844,
          0.28337236533957844,
          0.2857142857142857,
          0.2857142857142857,
          0.2927400468384075,
          0.2927400468384075,
          0.29508196721311475,
          0.29508196721311475,
          0.297423887587822,
          0.297423887587822,
          0.30210772833723654,
          0.30210772833723654,
          0.3091334894613583,
          0.3091334894613583,
          0.32084309133489464,
          0.32084309133489464,
          0.33489461358313816,
          0.33489461358313816,
          0.3372365339578454,
          0.3372365339578454,
          0.3395784543325527,
          0.3395784543325527,
          0.3442622950819672,
          0.3442622950819672,
          0.34660421545667447,
          0.34660421545667447,
          0.35362997658079626,
          0.35362997658079626,
          0.3559718969555035,
          0.3559718969555035,
          0.36065573770491804,
          0.36065573770491804,
          0.3629976580796253,
          0.3629976580796253,
          0.36768149882903983,
          0.36768149882903983,
          0.37236533957845436,
          0.37236533957845436,
          0.3747072599531616,
          0.3747072599531616,
          0.3770491803278688,
          0.3770491803278688,
          0.3793911007025761,
          0.3793911007025761,
          0.3864168618266979,
          0.3864168618266979,
          0.38875878220140514,
          0.38875878220140514,
          0.39344262295081966,
          0.39344262295081966,
          1
         ],
         "y": [
          0,
          0,
          0.014778325123152709,
          0.014778325123152709,
          0.017241379310344827,
          0.017241379310344827,
          0.03201970443349754,
          0.03201970443349754,
          0.034482758620689655,
          0.034482758620689655,
          0.03694581280788178,
          0.03694581280788178,
          0.04187192118226601,
          0.04187192118226601,
          0.04433497536945813,
          0.04433497536945813,
          0.05172413793103448,
          0.05172413793103448,
          0.05665024630541872,
          0.05665024630541872,
          0.06403940886699508,
          0.06403940886699508,
          0.06896551724137931,
          0.06896551724137931,
          0.07142857142857142,
          0.07142857142857142,
          0.07881773399014778,
          0.07881773399014778,
          0.08374384236453201,
          0.08374384236453201,
          0.08866995073891626,
          0.08866995073891626,
          0.09113300492610837,
          0.09113300492610837,
          0.09852216748768473,
          0.09852216748768473,
          0.10098522167487685,
          0.10098522167487685,
          0.10344827586206896,
          0.10344827586206896,
          0.10837438423645321,
          0.10837438423645321,
          0.12315270935960591,
          0.12315270935960591,
          0.12561576354679804,
          0.12561576354679804,
          0.1330049261083744,
          0.1330049261083744,
          0.1354679802955665,
          0.1354679802955665,
          0.13793103448275862,
          0.13793103448275862,
          0.14285714285714285,
          0.14285714285714285,
          0.1477832512315271,
          0.1477832512315271,
          0.15024630541871922,
          0.15024630541871922,
          0.16009852216748768,
          0.16009852216748768,
          0.16748768472906403,
          0.16748768472906403,
          0.1748768472906404,
          0.1748768472906404,
          0.17733990147783252,
          0.17733990147783252,
          0.18226600985221675,
          0.18226600985221675,
          0.18472906403940886,
          0.18472906403940886,
          0.18719211822660098,
          0.18719211822660098,
          0.1896551724137931,
          0.1896551724137931,
          0.19704433497536947,
          0.19704433497536947,
          0.2019704433497537,
          0.2019704433497537,
          0.2044334975369458,
          0.2044334975369458,
          0.20689655172413793,
          0.20689655172413793,
          0.20935960591133004,
          0.20935960591133004,
          0.21182266009852216,
          0.21182266009852216,
          0.21428571428571427,
          0.21428571428571427,
          0.21921182266009853,
          0.21921182266009853,
          0.22413793103448276,
          0.22413793103448276,
          0.22660098522167488,
          0.22660098522167488,
          0.229064039408867,
          0.229064039408867,
          0.23399014778325122,
          0.23399014778325122,
          0.23645320197044334,
          0.23645320197044334,
          0.2413793103448276,
          0.2413793103448276,
          0.2438423645320197,
          0.2438423645320197,
          0.2536945812807882,
          0.2536945812807882,
          0.2561576354679803,
          0.2561576354679803,
          0.270935960591133,
          0.270935960591133,
          0.2733990147783251,
          0.2733990147783251,
          0.2881773399014778,
          0.2881773399014778,
          0.29064039408866993,
          0.29064039408866993,
          0.29310344827586204,
          0.29310344827586204,
          0.29802955665024633,
          0.29802955665024633,
          0.30049261083743845,
          0.30049261083743845,
          0.30295566502463056,
          0.30295566502463056,
          0.32019704433497537,
          0.32019704433497537,
          0.3226600985221675,
          0.3226600985221675,
          0.3251231527093596,
          0.3251231527093596,
          0.33004926108374383,
          0.33004926108374383,
          0.33251231527093594,
          0.33251231527093594,
          0.33497536945812806,
          0.33497536945812806,
          0.3374384236453202,
          0.3374384236453202,
          0.3399014778325123,
          0.3399014778325123,
          0.34236453201970446,
          0.34236453201970446,
          0.3448275862068966,
          0.3448275862068966,
          0.3472906403940887,
          0.3472906403940887,
          0.35467980295566504,
          0.35467980295566504,
          0.35714285714285715,
          0.35714285714285715,
          0.3645320197044335,
          0.3645320197044335,
          0.3669950738916256,
          0.3669950738916256,
          0.37192118226600984,
          0.37192118226600984,
          0.37438423645320196,
          0.37438423645320196,
          0.3793103448275862,
          0.3793103448275862,
          0.3842364532019704,
          0.3842364532019704,
          0.3866995073891626,
          0.3866995073891626,
          0.3891625615763547,
          0.3891625615763547,
          0.39901477832512317,
          0.39901477832512317,
          0.4039408866995074,
          0.4039408866995074,
          0.4064039408866995,
          0.4064039408866995,
          0.4088669950738916,
          0.4088669950738916,
          0.41133004926108374,
          0.41133004926108374,
          0.41379310344827586,
          0.41379310344827586,
          0.41625615763546797,
          0.41625615763546797,
          0.4187192118226601,
          1
         ]
        },
        {
         "line": {
          "color": "indigo",
          "width": 2
         },
         "mode": "lines",
         "name": "H8 AUC(area = 0.56)",
         "type": "scatter",
         "uid": "3d187698-3396-11e9-8c97-67fc199b385a",
         "x": [
          0,
          0.10593900481540931,
          0.10754414125200643,
          0.10754414125200643,
          0.10914927768860354,
          0.10914927768860354,
          0.11075441412520064,
          0.11075441412520064,
          0.11396468699839486,
          0.11396468699839486,
          0.11556982343499198,
          0.11556982343499198,
          0.1187800963081862,
          0.1187800963081862,
          0.12038523274478331,
          0.12038523274478331,
          0.12359550561797752,
          0.12359550561797752,
          0.12680577849117175,
          0.12680577849117175,
          0.12841091492776885,
          0.12841091492776885,
          0.13001605136436598,
          0.13001605136436598,
          0.13162118780096307,
          0.13162118780096307,
          0.1332263242375602,
          0.1332263242375602,
          0.13964686998394862,
          0.13964686998394862,
          0.14285714285714285,
          0.14285714285714285,
          0.14446227929373998,
          0.14446227929373998,
          0.14606741573033707,
          0.14606741573033707,
          0.1476725521669342,
          0.1476725521669342,
          0.1508828250401284,
          0.1508828250401284,
          0.15248796147672553,
          0.15248796147672553,
          0.15569823434991975,
          0.15569823434991975,
          0.15730337078651685,
          0.15730337078651685,
          0.16211878009630817,
          0.16211878009630817,
          0.1637239165329053,
          0.1637239165329053,
          0.16693418940609953,
          0.16693418940609953,
          0.16853932584269662,
          0.16853932584269662,
          0.17174959871589085,
          0.17174959871589085,
          0.17335473515248795,
          0.17335473515248795,
          0.17656500802568217,
          0.17656500802568217,
          0.1797752808988764,
          0.1797752808988764,
          0.18138041733547353,
          0.18138041733547353,
          0.18298555377207062,
          0.18298555377207062,
          0.18459069020866772,
          0.18459069020866772,
          0.18940609951845908,
          0.18940609951845908,
          0.19101123595505617,
          0.19101123595505617,
          0.1926163723916533,
          0.1926163723916533,
          0.19743178170144463,
          0.19743178170144463,
          0.19903691813804172,
          0.19903691813804172,
          0.20224719101123595,
          0.20224719101123595,
          0.20385232744783308,
          0.20385232744783308,
          0.20706260032102727,
          0.20706260032102727,
          0.2086677367576244,
          0.2086677367576244,
          0.21348314606741572,
          0.21348314606741572,
          0.21508828250401285,
          0.21508828250401285,
          0.21669341894060995,
          0.21669341894060995,
          0.21829855537720708,
          0.21829855537720708,
          0.21990369181380418,
          0.21990369181380418,
          0.22150882825040127,
          0.22150882825040127,
          0.2231139646869984,
          0.2231139646869984,
          0.2247191011235955,
          0.2247191011235955,
          0.22792937399678972,
          0.22792937399678972,
          0.23274478330658105,
          0.23274478330658105,
          0.23434991974317818,
          0.23434991974317818,
          0.23595505617977527,
          0.23595505617977527,
          0.2375601926163724,
          0.2375601926163724,
          0.2391653290529695,
          0.2391653290529695,
          0.24558587479935795,
          0.24558587479935795,
          0.2536115569823435,
          0.2536115569823435,
          0.25842696629213485,
          0.25842696629213485,
          0.26003210272873195,
          0.26003210272873195,
          0.26163723916532905,
          0.26163723916532905,
          0.2696629213483146,
          0.2696629213483146,
          0.27287319422150885,
          0.27287319422150885,
          0.27447833065810595,
          0.27447833065810595,
          0.27608346709470305,
          0.27608346709470305,
          0.27768860353130015,
          0.27768860353130015,
          0.27929373996789725,
          0.27929373996789725,
          0.2808988764044944,
          0.2808988764044944,
          0.2841091492776886,
          0.2841091492776886,
          0.28892455858747995,
          0.28892455858747995,
          0.29052969502407705,
          0.29052969502407705,
          0.29213483146067415,
          0.29213483146067415,
          0.2969502407704655,
          0.2969502407704655,
          0.30497592295345105,
          0.30497592295345105,
          0.30818619582664525,
          0.30818619582664525,
          0.3097913322632424,
          0.3097913322632424,
          0.3146067415730337,
          0.3146067415730337,
          0.3162118780096308,
          0.3162118780096308,
          0.32102728731942215,
          0.32102728731942215,
          0.32263242375601925,
          0.32263242375601925,
          0.3274478330658106,
          0.3274478330658106,
          0.3290529695024077,
          0.3290529695024077,
          0.33386837881219905,
          0.33386837881219905,
          0.33707865168539325,
          0.33707865168539325,
          0.3402889245585875,
          0.3402889245585875,
          0.3467094703049759,
          0.3467094703049759,
          0.34991974317817015,
          0.34991974317817015,
          0.35313001605136435,
          0.35313001605136435,
          0.3563402889245586,
          0.3563402889245586,
          0.3611556982343499,
          0.3611556982343499,
          0.36436597110754415,
          0.36436597110754415,
          0.36597110754414125,
          0.36597110754414125,
          0.3739967897271268,
          0.3739967897271268,
          0.37881219903691815,
          0.37881219903691815,
          0.38041733547351525,
          0.38041733547351525,
          0.38202247191011235,
          0.38202247191011235,
          0.38362760834670945,
          0.38362760834670945,
          0.3852327447833066,
          0.3852327447833066,
          0.3868378812199037,
          0.3868378812199037,
          0.3884430176565008,
          0.3884430176565008,
          0.39486356340288925,
          0.39486356340288925,
          0.39646869983948635,
          0.39646869983948635,
          0.3996789727126806,
          0.3996789727126806,
          0.4012841091492777,
          0.4012841091492777,
          0.4028892455858748,
          0.4028892455858748,
          0.406099518459069,
          0.406099518459069,
          0.40930979133226325,
          0.40930979133226325,
          0.41091492776886035,
          0.41091492776886035,
          0.41252006420545745,
          0.41252006420545745,
          0.4157303370786517,
          0.4157303370786517,
          0.420545746388443,
          0.420545746388443,
          0.42375601926163725,
          0.42375601926163725,
          0.42696629213483145,
          0.42696629213483145,
          0.4333868378812199,
          0.4333868378812199,
          0.434991974317817,
          0.434991974317817,
          0.43820224719101125,
          0.43820224719101125,
          0.4446227929373997,
          0.4446227929373997,
          0.45264847512038525,
          0.45264847512038525,
          0.45425361155698235,
          0.45425361155698235,
          0.45746388443017655,
          0.45746388443017655,
          0.4590690208667737,
          0.4590690208667737,
          0.4606741573033708,
          0.4606741573033708,
          0.4654895666131621,
          0.4654895666131621,
          0.47191011235955055,
          0.47191011235955055,
          0.47351524879614765,
          0.47351524879614765,
          0.4751203852327448,
          0.4751203852327448,
          0.4767255216693419,
          0.4767255216693419,
          0.48314606741573035,
          0.48314606741573035,
          0.4911717495987159,
          0.4911717495987159,
          0.492776886035313,
          0.492776886035313,
          0.49919743178170145,
          0.49919743178170145,
          0.5008025682182986,
          0.5008025682182986,
          0.5024077046548957,
          0.5024077046548957,
          0.5040128410914928,
          0.5040128410914928,
          0.507223113964687,
          0.507223113964687,
          0.5088282504012841,
          0.5088282504012841,
          0.5104333868378812,
          0.5104333868378812,
          0.5120385232744783,
          0.5120385232744783,
          0.5152487961476726,
          0.5152487961476726,
          0.5168539325842697,
          0.5168539325842697,
          0.5200642054574639,
          0.5200642054574639,
          0.521669341894061,
          0.521669341894061,
          0.5248796147672552,
          0.5248796147672552,
          0.5280898876404494,
          0.5280898876404494,
          0.5296950240770465,
          0.5296950240770465,
          0.5329052969502408,
          0.5329052969502408,
          0.5345104333868379,
          0.5345104333868379,
          0.536115569823435,
          0.536115569823435,
          0.5377207062600321,
          0.5377207062600321,
          0.5393258426966292,
          0.5393258426966292,
          0.5425361155698234,
          0.5425361155698234,
          0.5537720706260032,
          0.5537720706260032,
          0.5553772070626003,
          0.5553772070626003,
          0.5569823434991974,
          0.5569823434991974,
          0.5585874799357945,
          0.5585874799357945,
          0.5601926163723917,
          0.5601926163723917,
          0.565008025682183,
          0.565008025682183,
          0.5746388443017657,
          0.5746388443017657,
          0.5842696629213483,
          0.5842696629213483,
          0.5874799357945425,
          0.5874799357945425,
          0.5906902086677368,
          0.5906902086677368,
          0.5922953451043339,
          0.5922953451043339,
          0.5955056179775281,
          0.5955056179775281,
          0.5987158908507223,
          0.5987158908507223,
          0.6019261637239165,
          0.6019261637239165,
          0.6051364365971108,
          0.6051364365971108,
          0.6067415730337079,
          0.6067415730337079,
          0.608346709470305,
          0.608346709470305,
          0.6099518459069021,
          0.6099518459069021,
          0.6131621187800963,
          0.6131621187800963,
          0.6147672552166934,
          0.6147672552166934,
          0.6179775280898876,
          0.6179775280898876,
          0.6243980738362761,
          0.6243980738362761,
          0.6260032102728732,
          0.6260032102728732,
          0.6292134831460674,
          0.6292134831460674,
          0.6308186195826645,
          0.6308186195826645,
          0.6356340288924559,
          0.6356340288924559,
          0.637239165329053,
          0.637239165329053,
          0.6388443017656501,
          0.6388443017656501,
          0.6420545746388443,
          0.6420545746388443,
          0.6436597110754414,
          0.6436597110754414,
          0.6452648475120385,
          0.6452648475120385,
          0.6468699839486356,
          0.6468699839486356,
          0.6484751203852327,
          0.6484751203852327,
          0.6500802568218299,
          0.6500802568218299,
          0.6597110754414125,
          0.6597110754414125,
          0.6613162118780096,
          0.6613162118780096,
          0.6677367576243981,
          0.6677367576243981,
          0.6709470304975923,
          0.6709470304975923,
          0.6741573033707865,
          0.6741573033707865,
          0.6757624398073836,
          0.6757624398073836,
          0.6789727126805778,
          0.6789727126805778,
          0.6821829855537721,
          0.6821829855537721,
          0.6837881219903692,
          0.6837881219903692,
          0.6886035313001605,
          0.6886035313001605,
          0.6934189406099518,
          0.6934189406099518,
          0.695024077046549,
          0.695024077046549,
          0.6966292134831461,
          0.6966292134831461,
          0.7014446227929374,
          0.7014446227929374,
          0.7030497592295345,
          0.7030497592295345,
          0.7046548956661316,
          0.7046548956661316,
          0.7078651685393258,
          0.7078651685393258,
          0.7110754414125201,
          0.7110754414125201,
          0.7174959871589085,
          0.7174959871589085,
          0.7191011235955056,
          0.7191011235955056,
          0.7303370786516854,
          0.7303370786516854,
          0.7319422150882825,
          0.7319422150882825,
          0.7335473515248796,
          0.7335473515248796,
          0.7367576243980738,
          0.7367576243980738,
          0.7383627608346709,
          0.7383627608346709,
          0.7415730337078652,
          0.7415730337078652,
          0.7447833065810594,
          0.7447833065810594,
          0.7463884430176565,
          0.7463884430176565,
          0.7479935794542536,
          0.7479935794542536,
          0.7512038523274478,
          0.7512038523274478,
          0.7560192616372392,
          0.7560192616372392,
          0.7576243980738363,
          0.7576243980738363,
          0.7592295345104334,
          0.7592295345104334,
          0.7672552166934189,
          0.7672552166934189,
          0.7688603531300161,
          0.7688603531300161,
          0.7704654895666132,
          0.7704654895666132,
          0.7752808988764045,
          0.7752808988764045,
          0.7784911717495987,
          0.7784911717495987,
          0.78330658105939,
          0.78330658105939,
          0.7849117174959872,
          0.7849117174959872,
          0.7913322632423756,
          0.7913322632423756,
          0.7961476725521669,
          0.7961476725521669,
          0.8009630818619583,
          0.8009630818619583,
          0.8025682182985554,
          0.8025682182985554,
          0.8105939004815409,
          0.8105939004815409,
          0.812199036918138,
          0.812199036918138,
          0.8154093097913323,
          0.8154093097913323,
          0.8186195826645265,
          0.8186195826645265,
          0.8202247191011236,
          0.8202247191011236,
          0.8218298555377207,
          0.8218298555377207,
          0.8250401284109149,
          0.8250401284109149,
          0.826645264847512,
          0.826645264847512,
          0.8394863563402889,
          0.8394863563402889,
          0.8443017656500803,
          0.8443017656500803,
          0.8459069020866774,
          0.8459069020866774,
          0.8491171749598716,
          0.8491171749598716,
          0.8507223113964687,
          0.8507223113964687,
          0.8539325842696629,
          0.8539325842696629,
          0.8587479935794543,
          0.8587479935794543,
          0.8603531300160514,
          0.8603531300160514,
          0.8619582664526485,
          0.8619582664526485,
          0.8667736757624398,
          0.8667736757624398,
          0.8683788121990369,
          0.8683788121990369,
          0.869983948635634,
          0.869983948635634,
          0.8715890850722311,
          0.8715890850722311,
          0.8764044943820225,
          0.8764044943820225,
          0.8796147672552167,
          0.8796147672552167,
          0.8812199036918138,
          0.8812199036918138,
          0.8956661316211878,
          0.8956661316211878,
          0.9020866773675762,
          0.9020866773675762,
          0.9069020866773676,
          0.9069020866773676,
          0.9085072231139647,
          0.9085072231139647,
          0.9117174959871589,
          0.9117174959871589,
          0.9245585874799358,
          0.9245585874799358,
          0.9309791332263242,
          0.9309791332263242,
          0.942215088282504,
          0.942215088282504,
          0.9438202247191011,
          0.9438202247191011,
          0.9502407704654896,
          0.9502407704654896,
          0.9518459069020867,
          0.9518459069020867,
          0.9534510433386838,
          0.9534510433386838,
          0.956661316211878,
          0.956661316211878,
          0.9646869983948636,
          0.9646869983948636,
          1
         ],
         "y": [
          0,
          0.10278745644599303,
          0.10278745644599303,
          0.11672473867595819,
          0.11672473867595819,
          0.11846689895470383,
          0.11846689895470383,
          0.12020905923344948,
          0.12020905923344948,
          0.12195121951219512,
          0.12195121951219512,
          0.1289198606271777,
          0.1289198606271777,
          0.13066202090592335,
          0.13066202090592335,
          0.13588850174216027,
          0.13588850174216027,
          0.13763066202090593,
          0.13763066202090593,
          0.13937282229965156,
          0.13937282229965156,
          0.14111498257839722,
          0.14111498257839722,
          0.14634146341463414,
          0.14634146341463414,
          0.15331010452961671,
          0.15331010452961671,
          0.15853658536585366,
          0.15853658536585366,
          0.16202090592334495,
          0.16202090592334495,
          0.17247386759581881,
          0.17247386759581881,
          0.17770034843205576,
          0.17770034843205576,
          0.18292682926829268,
          0.18292682926829268,
          0.18466898954703834,
          0.18466898954703834,
          0.18641114982578397,
          0.18641114982578397,
          0.18815331010452963,
          0.18815331010452963,
          0.1916376306620209,
          0.1916376306620209,
          0.19686411149825783,
          0.19686411149825783,
          0.20209059233449478,
          0.20209059233449478,
          0.20557491289198607,
          0.20557491289198607,
          0.20905923344947736,
          0.20905923344947736,
          0.21254355400696864,
          0.21254355400696864,
          0.21602787456445993,
          0.21602787456445993,
          0.21951219512195122,
          0.21951219512195122,
          0.22125435540069685,
          0.22125435540069685,
          0.2229965156794425,
          0.2229965156794425,
          0.22996515679442509,
          0.22996515679442509,
          0.23519163763066203,
          0.23519163763066203,
          0.23693379790940766,
          0.23693379790940766,
          0.23867595818815332,
          0.23867595818815332,
          0.2421602787456446,
          0.2421602787456446,
          0.24390243902439024,
          0.24390243902439024,
          0.2456445993031359,
          0.2456445993031359,
          0.24912891986062718,
          0.24912891986062718,
          0.25261324041811845,
          0.25261324041811845,
          0.26306620209059234,
          0.26306620209059234,
          0.26480836236933797,
          0.26480836236933797,
          0.2665505226480836,
          0.2665505226480836,
          0.2682926829268293,
          0.2682926829268293,
          0.2700348432055749,
          0.2700348432055749,
          0.27526132404181186,
          0.27526132404181186,
          0.2770034843205575,
          0.2770034843205575,
          0.2787456445993031,
          0.2787456445993031,
          0.2804878048780488,
          0.2804878048780488,
          0.28222996515679444,
          0.28222996515679444,
          0.2857142857142857,
          0.2857142857142857,
          0.2874564459930314,
          0.2874564459930314,
          0.289198606271777,
          0.289198606271777,
          0.2926829268292683,
          0.2926829268292683,
          0.2979094076655052,
          0.2979094076655052,
          0.30139372822299654,
          0.30139372822299654,
          0.30313588850174217,
          0.30313588850174217,
          0.3048780487804878,
          0.3048780487804878,
          0.3083623693379791,
          0.3083623693379791,
          0.31010452961672474,
          0.31010452961672474,
          0.3153310104529617,
          0.3153310104529617,
          0.3205574912891986,
          0.3205574912891986,
          0.32752613240418116,
          0.32752613240418116,
          0.32926829268292684,
          0.32926829268292684,
          0.3344947735191638,
          0.3344947735191638,
          0.3362369337979094,
          0.3362369337979094,
          0.3397212543554007,
          0.3397212543554007,
          0.343205574912892,
          0.343205574912892,
          0.34668989547038326,
          0.34668989547038326,
          0.34843205574912894,
          0.34843205574912894,
          0.3501742160278746,
          0.3501742160278746,
          0.35714285714285715,
          0.35714285714285715,
          0.3588850174216028,
          0.3588850174216028,
          0.36585365853658536,
          0.36585365853658536,
          0.367595818815331,
          0.367595818815331,
          0.3710801393728223,
          0.3710801393728223,
          0.37282229965156793,
          0.37282229965156793,
          0.38153310104529614,
          0.38153310104529614,
          0.3832752613240418,
          0.3832752613240418,
          0.38501742160278746,
          0.38501742160278746,
          0.3867595818815331,
          0.3867595818815331,
          0.39372822299651566,
          0.39372822299651566,
          0.3989547038327526,
          0.3989547038327526,
          0.4024390243902439,
          0.4024390243902439,
          0.40418118466898956,
          0.40418118466898956,
          0.4076655052264808,
          0.4076655052264808,
          0.41114982578397213,
          0.41114982578397213,
          0.4146341463414634,
          0.4146341463414634,
          0.4163763066202091,
          0.4163763066202091,
          0.41986062717770034,
          0.41986062717770034,
          0.4337979094076655,
          0.4337979094076655,
          0.4355400696864111,
          0.4355400696864111,
          0.43902439024390244,
          0.43902439024390244,
          0.44076655052264807,
          0.44076655052264807,
          0.4425087108013937,
          0.4425087108013937,
          0.4442508710801394,
          0.4442508710801394,
          0.445993031358885,
          0.445993031358885,
          0.44773519163763065,
          0.44773519163763065,
          0.45121951219512196,
          0.45121951219512196,
          0.4547038327526132,
          0.4547038327526132,
          0.45993031358885017,
          0.45993031358885017,
          0.4651567944250871,
          0.4651567944250871,
          0.4721254355400697,
          0.4721254355400697,
          0.4738675958188153,
          0.4738675958188153,
          0.47909407665505227,
          0.47909407665505227,
          0.48257839721254353,
          0.48257839721254353,
          0.4843205574912892,
          0.4843205574912892,
          0.48606271777003485,
          0.48606271777003485,
          0.4878048780487805,
          0.4878048780487805,
          0.4930313588850174,
          0.4930313588850174,
          0.4965156794425087,
          0.4965156794425087,
          0.5017421602787456,
          0.5017421602787456,
          0.5052264808362369,
          0.5052264808362369,
          0.5069686411149826,
          0.5069686411149826,
          0.5104529616724739,
          0.5104529616724739,
          0.5139372822299652,
          0.5139372822299652,
          0.5156794425087108,
          0.5156794425087108,
          0.519163763066202,
          0.519163763066202,
          0.5209059233449478,
          0.5209059233449478,
          0.524390243902439,
          0.524390243902439,
          0.5296167247386759,
          0.5296167247386759,
          0.5313588850174216,
          0.5313588850174216,
          0.5365853658536586,
          0.5365853658536586,
          0.5418118466898955,
          0.5418118466898955,
          0.5452961672473867,
          0.5452961672473867,
          0.5470383275261324,
          0.5470383275261324,
          0.5487804878048781,
          0.5487804878048781,
          0.554006968641115,
          0.554006968641115,
          0.5557491289198606,
          0.5557491289198606,
          0.5592334494773519,
          0.5592334494773519,
          0.5679442508710801,
          0.5679442508710801,
          0.5696864111498258,
          0.5696864111498258,
          0.5818815331010453,
          0.5818815331010453,
          0.5905923344947736,
          0.5905923344947736,
          0.5958188153310104,
          0.5958188153310104,
          0.6010452961672473,
          0.6010452961672473,
          0.6027874564459931,
          0.6027874564459931,
          0.6045296167247387,
          0.6045296167247387,
          0.60801393728223,
          0.60801393728223,
          0.6097560975609756,
          0.6097560975609756,
          0.6114982578397212,
          0.6114982578397212,
          0.6149825783972126,
          0.6149825783972126,
          0.6167247386759582,
          0.6167247386759582,
          0.6184668989547039,
          0.6184668989547039,
          0.6219512195121951,
          0.6219512195121951,
          0.6236933797909407,
          0.6236933797909407,
          0.6254355400696864,
          0.6254355400696864,
          0.627177700348432,
          0.627177700348432,
          0.6289198606271778,
          0.6289198606271778,
          0.632404181184669,
          0.632404181184669,
          0.6341463414634146,
          0.6341463414634146,
          0.6411149825783972,
          0.6411149825783972,
          0.6428571428571429,
          0.6428571428571429,
          0.6445993031358885,
          0.6445993031358885,
          0.6463414634146342,
          0.6463414634146342,
          0.6498257839721254,
          0.6498257839721254,
          0.6550522648083623,
          0.6550522648083623,
          0.6567944250871081,
          0.6567944250871081,
          0.6655052264808362,
          0.6655052264808362,
          0.6689895470383276,
          0.6689895470383276,
          0.6724738675958188,
          0.6724738675958188,
          0.6759581881533101,
          0.6759581881533101,
          0.6777003484320557,
          0.6777003484320557,
          0.681184668989547,
          0.681184668989547,
          0.6846689895470384,
          0.6846689895470384,
          0.686411149825784,
          0.686411149825784,
          0.6881533101045296,
          0.6881533101045296,
          0.6933797909407665,
          0.6933797909407665,
          0.6951219512195121,
          0.6951219512195121,
          0.6986062717770035,
          0.6986062717770035,
          0.7003484320557491,
          0.7003484320557491,
          0.7020905923344948,
          0.7020905923344948,
          0.7038327526132404,
          0.7038327526132404,
          0.7090592334494773,
          0.7090592334494773,
          0.710801393728223,
          0.710801393728223,
          0.7125435540069687,
          0.7125435540069687,
          0.7142857142857143,
          0.7142857142857143,
          0.7160278745644599,
          0.7160278745644599,
          0.7195121951219512,
          0.7195121951219512,
          0.7247386759581882,
          0.7247386759581882,
          0.7264808362369338,
          0.7264808362369338,
          0.7299651567944251,
          0.7299651567944251,
          0.7317073170731707,
          0.7317073170731707,
          0.7334494773519163,
          0.7334494773519163,
          0.7369337979094077,
          0.7369337979094077,
          0.7421602787456446,
          0.7421602787456446,
          0.7508710801393729,
          0.7508710801393729,
          0.7526132404181185,
          0.7526132404181185,
          0.7543554006968641,
          0.7543554006968641,
          0.7578397212543554,
          0.7578397212543554,
          0.7613240418118467,
          0.7613240418118467,
          0.764808362369338,
          0.764808362369338,
          0.7700348432055749,
          0.7700348432055749,
          0.7717770034843205,
          0.7717770034843205,
          0.7735191637630662,
          0.7735191637630662,
          0.7752613240418118,
          0.7752613240418118,
          0.7787456445993032,
          0.7787456445993032,
          0.7822299651567944,
          0.7822299651567944,
          0.7839721254355401,
          0.7839721254355401,
          0.7926829268292683,
          0.7926829268292683,
          0.794425087108014,
          0.794425087108014,
          0.7979094076655052,
          0.7979094076655052,
          0.8013937282229965,
          0.8013937282229965,
          0.8048780487804879,
          0.8048780487804879,
          0.8066202090592335,
          0.8066202090592335,
          0.8205574912891986,
          0.8205574912891986,
          0.8222996515679443,
          0.8222996515679443,
          0.8275261324041812,
          0.8275261324041812,
          0.8362369337979094,
          0.8362369337979094,
          0.8397212543554007,
          0.8397212543554007,
          0.8432055749128919,
          0.8432055749128919,
          0.8449477351916377,
          0.8449477351916377,
          0.8466898954703833,
          0.8466898954703833,
          0.8484320557491289,
          0.8484320557491289,
          0.8519163763066202,
          0.8519163763066202,
          0.8536585365853658,
          0.8536585365853658,
          0.8554006968641115,
          0.8554006968641115,
          0.8571428571428571,
          0.8571428571428571,
          0.8623693379790941,
          0.8623693379790941,
          0.867595818815331,
          0.867595818815331,
          0.8693379790940766,
          0.8693379790940766,
          0.8710801393728222,
          0.8710801393728222,
          0.8745644599303136,
          0.8745644599303136,
          0.8763066202090593,
          0.8763066202090593,
          0.8780487804878049,
          0.8780487804878049,
          0.8797909407665505,
          0.8797909407665505,
          0.8815331010452961,
          0.8815331010452961,
          0.8832752613240418,
          0.8832752613240418,
          0.8850174216027874,
          0.8850174216027874,
          0.8885017421602788,
          0.8885017421602788,
          0.8902439024390244,
          0.8902439024390244,
          0.89198606271777,
          0.89198606271777,
          0.8937282229965157,
          0.8937282229965157,
          0.8954703832752613,
          0.8954703832752613,
          0.8972125435540069,
          0.8972125435540069,
          0.9024390243902439,
          0.9024390243902439,
          0.9041811846689896,
          0.9041811846689896,
          0.9094076655052264,
          0.9094076655052264,
          0.9146341463414634,
          0.9146341463414634,
          0.9163763066202091,
          0.9163763066202091,
          0.9198606271777003,
          0.9198606271777003,
          0.921602787456446,
          0.921602787456446,
          0.9250871080139372,
          0.9250871080139372,
          0.926829268292683,
          0.926829268292683,
          0.9285714285714286,
          0.9285714285714286,
          0.9303135888501742,
          0.9303135888501742,
          0.9355400696864111,
          0.9355400696864111,
          0.9372822299651568,
          0.9372822299651568,
          0.9425087108013938,
          0.9425087108013938,
          0.9442508710801394,
          0.9442508710801394,
          0.9477351916376306,
          0.9477351916376306,
          0.9529616724738676,
          0.9529616724738676,
          0.9547038327526133,
          0.9547038327526133,
          0.9564459930313589,
          0.9564459930313589,
          0.9599303135888502,
          0.9599303135888502,
          0.9616724738675958,
          0.9616724738675958,
          0.9651567944250871,
          0.9651567944250871,
          0.9668989547038328,
          0.9668989547038328,
          0.9703832752613241,
          0.9703832752613241,
          0.9721254355400697,
          0.9721254355400697,
          0.9738675958188153,
          0.9738675958188153,
          0.980836236933798,
          0.980836236933798,
          0.9825783972125436,
          0.9825783972125436,
          0.9843205574912892,
          0.9843205574912892,
          0.9895470383275261,
          0.9895470383275261,
          0.9947735191637631,
          0.9947735191637631,
          0.9982578397212544,
          0.9982578397212544,
          1,
          1
         ]
        }
       ],
       "layout": {
        "title": "Hourly ROC Comparisons",
        "xaxis": {
         "title": "False Positive Rate"
        },
        "yaxis": {
         "title": "Sensitivity"
        }
       }
      },
      "text/html": [
       "<div id=\"e1fa3374-ed34-416d-b913-b1b4427366a0\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "        Plotly.plot(\n",
       "            'e1fa3374-ed34-416d-b913-b1b4427366a0',\n",
       "            [{\"line\": {\"color\": \"darkorange\", \"width\": 2}, \"mode\": \"lines\", \"name\": \"H23 AUC(area = 0.81)\", \"x\": [0.0, 0.0, 0.0020161290322580645, 0.0020161290322580645, 0.004032258064516129, 0.004032258064516129, 0.006048387096774193, 0.006048387096774193, 0.008064516129032258, 0.008064516129032258, 0.010080645161290322, 0.010080645161290322, 0.012096774193548387, 0.012096774193548387, 0.016129032258064516, 0.016129032258064516, 0.018145161290322582, 0.018145161290322582, 0.020161290322580645, 0.020161290322580645, 0.02217741935483871, 0.02217741935483871, 0.024193548387096774, 0.024193548387096774, 0.02620967741935484, 0.02620967741935484, 0.028225806451612902, 0.028225806451612902, 0.03024193548387097, 0.03024193548387097, 0.03225806451612903, 0.03225806451612903, 0.034274193548387094, 0.034274193548387094, 0.036290322580645164, 0.036290322580645164, 0.038306451612903226, 0.038306451612903226, 0.04032258064516129, 0.04032258064516129, 0.04233870967741935, 0.04233870967741935, 0.04435483870967742, 0.04435483870967742, 0.04838709677419355, 0.04838709677419355, 0.05040322580645161, 0.05040322580645161, 0.05443548387096774, 0.05443548387096774, 0.05846774193548387, 0.05846774193548387, 0.06451612903225806, 0.06451612903225806, 0.06854838709677419, 0.06854838709677419, 0.07258064516129033, 0.07258064516129033, 0.07459677419354839, 0.07459677419354839, 0.07661290322580645, 0.07661290322580645, 0.08064516129032258, 0.08064516129032258, 0.08266129032258064, 0.08266129032258064, 0.0846774193548387, 0.0846774193548387, 0.08669354838709678, 0.08669354838709678, 0.08870967741935484, 0.08870967741935484, 0.0907258064516129, 0.0907258064516129, 0.09274193548387097, 0.09274193548387097, 0.0967741935483871, 0.0967741935483871, 0.09879032258064516, 0.09879032258064516, 0.10080645161290322, 0.10080645161290322, 0.1028225806451613, 0.1028225806451613, 0.10483870967741936, 0.10483870967741936, 0.10887096774193548, 0.10887096774193548, 0.11088709677419355, 0.11088709677419355, 0.11491935483870967, 0.11491935483870967, 0.11693548387096774, 0.11693548387096774, 0.11895161290322581, 0.11895161290322581, 0.12096774193548387, 0.12096774193548387, 0.12298387096774194, 0.12298387096774194, 0.125, 0.125, 0.12903225806451613, 0.12903225806451613, 0.13306451612903225, 0.13306451612903225, 0.1350806451612903, 0.1350806451612903, 0.13709677419354838, 0.13709677419354838, 0.13911290322580644, 0.13911290322580644, 0.14112903225806453, 0.14112903225806453, 0.1431451612903226, 0.1431451612903226, 0.14516129032258066, 0.14516129032258066, 0.15120967741935484, 0.15120967741935484, 0.1532258064516129, 0.1532258064516129, 0.15524193548387097, 0.15524193548387097, 0.15725806451612903, 0.15725806451612903, 0.1592741935483871, 0.1592741935483871, 0.16532258064516128, 0.16532258064516128, 0.1693548387096774, 0.1693548387096774, 0.17137096774193547, 0.17137096774193547, 0.17338709677419356, 0.17338709677419356, 0.17540322580645162, 0.17540322580645162, 0.1774193548387097, 0.1774193548387097, 0.17943548387096775, 0.17943548387096775, 0.1814516129032258, 0.1814516129032258, 0.18548387096774194, 0.18548387096774194, 0.1875, 0.1875, 0.18951612903225806, 0.18951612903225806, 0.19153225806451613, 0.19153225806451613, 0.1935483870967742, 0.1935483870967742, 0.19556451612903225, 0.19556451612903225, 0.1975806451612903, 0.1975806451612903, 0.20362903225806453, 0.20362903225806453, 0.20766129032258066, 0.20766129032258066, 0.20967741935483872, 0.20967741935483872, 0.21169354838709678, 0.21169354838709678, 0.21370967741935484, 0.21370967741935484, 0.2157258064516129, 0.2157258064516129, 0.22580645161290322, 0.22580645161290322, 0.23588709677419356, 0.23588709677419356, 0.23790322580645162, 0.23790322580645162, 0.2399193548387097, 0.2399193548387097, 0.24193548387096775, 0.24193548387096775, 0.2439516129032258, 0.2439516129032258, 0.2540322580645161, 0.2540322580645161, 0.2560483870967742, 0.2560483870967742, 0.25806451612903225, 0.25806451612903225, 0.2600806451612903, 0.2600806451612903, 0.26411290322580644, 0.26411290322580644, 0.2701612903225806, 0.2701612903225806, 0.2721774193548387, 0.2721774193548387, 0.2762096774193548, 0.2762096774193548, 0.2842741935483871, 0.2842741935483871, 0.28830645161290325, 0.28830645161290325, 0.2903225806451613, 0.2903225806451613, 0.29838709677419356, 0.29838709677419356, 0.3004032258064516, 0.3004032258064516, 0.30443548387096775, 0.30443548387096775, 0.3084677419354839, 0.3084677419354839, 0.3286290322580645, 0.3286290322580645, 0.33064516129032256, 0.33064516129032256, 0.3326612903225806, 0.3326612903225806, 0.3346774193548387, 0.3346774193548387, 0.3387096774193548, 0.3387096774193548, 0.3467741935483871, 0.3467741935483871, 0.3528225806451613, 0.3528225806451613, 0.35685483870967744, 0.35685483870967744, 0.3588709677419355, 0.3588709677419355, 0.3629032258064516, 0.3629032258064516, 0.3649193548387097, 0.3649193548387097, 0.3709677419354839, 0.3709677419354839, 0.37298387096774194, 0.37298387096774194, 0.3790322580645161, 0.3790322580645161, 0.38306451612903225, 0.38306451612903225, 0.3850806451612903, 0.3850806451612903, 0.39314516129032256, 0.39314516129032256, 0.39919354838709675, 0.39919354838709675, 0.4032258064516129, 0.4032258064516129, 0.40524193548387094, 0.40524193548387094, 0.4092741935483871, 0.4092741935483871, 0.4254032258064516, 0.4254032258064516, 0.43951612903225806, 0.43951612903225806, 0.4415322580645161, 0.4415322580645161, 0.4495967741935484, 0.4495967741935484, 0.4576612903225806, 0.4576612903225806, 0.4637096774193548, 0.4637096774193548, 0.4657258064516129, 0.4657258064516129, 0.47580645161290325, 0.47580645161290325, 0.4838709677419355, 0.4838709677419355, 0.48588709677419356, 0.48588709677419356, 0.49193548387096775, 0.49193548387096775, 0.49798387096774194, 0.49798387096774194, 0.5040322580645161, 0.5040322580645161, 0.5100806451612904, 0.5100806451612904, 0.5161290322580645, 0.5161290322580645, 0.5181451612903226, 0.5181451612903226, 0.5241935483870968, 0.5241935483870968, 0.5282258064516129, 0.5282258064516129, 0.530241935483871, 0.530241935483871, 0.532258064516129, 0.532258064516129, 0.5342741935483871, 0.5342741935483871, 0.5463709677419355, 0.5463709677419355, 0.5645161290322581, 0.5645161290322581, 0.5705645161290323, 0.5705645161290323, 0.5725806451612904, 0.5725806451612904, 0.5766129032258065, 0.5766129032258065, 0.5786290322580645, 0.5786290322580645, 0.6008064516129032, 0.6008064516129032, 0.6068548387096774, 0.6068548387096774, 0.6129032258064516, 0.6129032258064516, 0.625, 0.625, 0.6350806451612904, 0.6350806451612904, 0.6370967741935484, 0.6370967741935484, 0.6391129032258065, 0.6391129032258065, 0.6471774193548387, 0.6471774193548387, 0.6592741935483871, 0.6592741935483871, 0.6653225806451613, 0.6653225806451613, 0.6673387096774194, 0.6673387096774194, 0.6774193548387096, 0.6774193548387096, 0.7096774193548387, 0.7096774193548387, 0.7258064516129032, 0.7258064516129032, 0.7278225806451613, 0.7278225806451613, 0.7439516129032258, 0.7439516129032258, 0.7479838709677419, 0.7479838709677419, 0.75, 0.75, 0.7701612903225806, 0.7701612903225806, 0.7762096774193549, 0.7762096774193549, 0.7782258064516129, 0.7782258064516129, 0.7983870967741935, 0.7983870967741935, 0.8004032258064516, 0.8004032258064516, 0.8125, 0.8125, 0.8185483870967742, 0.8185483870967742, 0.8366935483870968, 0.8366935483870968, 0.8508064516129032, 0.8508064516129032, 0.8548387096774194, 0.8548387096774194, 0.8629032258064516, 0.8629032258064516, 0.8689516129032258, 0.8689516129032258, 0.875, 0.875, 0.8790322580645161, 0.8790322580645161, 0.8850806451612904, 0.8850806451612904, 0.8870967741935484, 0.8870967741935484, 0.9455645161290323, 0.9455645161290323, 0.9536290322580645, 0.9536290322580645, 0.9637096774193549, 0.9637096774193549, 0.9979838709677419, 0.9979838709677419, 1.0, 1.0], \"y\": [0.0015313935681470138, 0.022970903522205207, 0.022970903522205207, 0.07197549770290965, 0.07197549770290965, 0.07963246554364471, 0.07963246554364471, 0.09494640122511486, 0.09494640122511486, 0.10107197549770292, 0.10107197549770292, 0.11638591117917305, 0.11638591117917305, 0.13935681470137826, 0.13935681470137826, 0.1562021439509954, 0.1562021439509954, 0.19295558958652373, 0.19295558958652373, 0.21592649310872894, 0.21592649310872894, 0.21745788667687596, 0.21745788667687596, 0.23889739663093415, 0.23889739663093415, 0.26033690658499237, 0.26033690658499237, 0.28483920367534454, 0.28483920367534454, 0.2986217457886677, 0.2986217457886677, 0.30474732006125577, 0.30474732006125577, 0.3108728943338438, 0.3108728943338438, 0.3124042879019908, 0.3124042879019908, 0.32465543644716693, 0.32465543644716693, 0.32618683001531396, 0.32618683001531396, 0.32924961715160794, 0.32924961715160794, 0.3445635528330781, 0.3445635528330781, 0.3614088820826952, 0.3614088820826952, 0.36294027565084225, 0.36294027565084225, 0.38437978560490044, 0.38437978560490044, 0.39509954058192953, 0.39509954058192953, 0.39663093415007655, 0.39663093415007655, 0.4211332312404288, 0.4211332312404288, 0.4349157733537519, 0.4349157733537519, 0.43644716692189894, 0.43644716692189894, 0.43950995405819293, 0.43950995405819293, 0.445635528330781, 0.445635528330781, 0.44869831546707506, 0.44869831546707506, 0.4548238897396631, 0.4548238897396631, 0.45788667687595713, 0.45788667687595713, 0.46248085758039814, 0.46248085758039814, 0.4655436447166922, 0.4655436447166922, 0.47320061255742724, 0.47320061255742724, 0.4762633996937213, 0.4762633996937213, 0.4777947932618683, 0.4777947932618683, 0.4793261868300153, 0.4793261868300153, 0.4885145482388974, 0.4885145482388974, 0.5206738131699847, 0.5206738131699847, 0.5252679938744257, 0.5252679938744257, 0.5267993874425727, 0.5267993874425727, 0.5467075038284839, 0.5467075038284839, 0.5482388973966309, 0.5482388973966309, 0.5497702909647779, 0.5497702909647779, 0.554364471669219, 0.554364471669219, 0.55895865237366, 0.55895865237366, 0.5604900459418071, 0.5604900459418071, 0.5620214395099541, 0.5620214395099541, 0.5635528330781011, 0.5635528330781011, 0.5666156202143952, 0.5666156202143952, 0.5681470137825421, 0.5681470137825421, 0.5696784073506891, 0.5696784073506891, 0.5727411944869831, 0.5727411944869831, 0.5819295558958653, 0.5819295558958653, 0.5911179173047473, 0.5911179173047473, 0.5972434915773354, 0.5972434915773354, 0.6018376722817764, 0.6018376722817764, 0.6033690658499234, 0.6033690658499234, 0.6079632465543645, 0.6079632465543645, 0.6110260336906586, 0.6110260336906586, 0.6140888208269525, 0.6140888208269525, 0.6156202143950995, 0.6156202143950995, 0.6263399693721287, 0.6263399693721287, 0.6339969372128637, 0.6339969372128637, 0.6385911179173047, 0.6385911179173047, 0.6477794793261868, 0.6477794793261868, 0.6493108728943339, 0.6493108728943339, 0.6508422664624809, 0.6508422664624809, 0.655436447166922, 0.655436447166922, 0.6584992343032159, 0.6584992343032159, 0.666156202143951, 0.666156202143951, 0.667687595712098, 0.667687595712098, 0.669218989280245, 0.669218989280245, 0.6707503828483921, 0.6707503828483921, 0.6722817764165391, 0.6722817764165391, 0.6860643185298622, 0.6860643185298622, 0.6891271056661562, 0.6891271056661562, 0.6937212863705973, 0.6937212863705973, 0.6983154670750383, 0.6983154670750383, 0.7044410413476263, 0.7044410413476263, 0.7105666156202144, 0.7105666156202144, 0.7120980091883614, 0.7120980091883614, 0.7166921898928025, 0.7166921898928025, 0.7320061255742726, 0.7320061255742726, 0.7366003062787136, 0.7366003062787136, 0.7396630934150077, 0.7396630934150077, 0.7411944869831547, 0.7411944869831547, 0.7457886676875957, 0.7457886676875957, 0.7519142419601837, 0.7519142419601837, 0.7534456355283308, 0.7534456355283308, 0.7565084226646248, 0.7565084226646248, 0.7580398162327718, 0.7580398162327718, 0.7595712098009189, 0.7595712098009189, 0.7611026033690659, 0.7611026033690659, 0.7626339969372129, 0.7626339969372129, 0.7641653905053599, 0.7641653905053599, 0.7656967840735069, 0.7656967840735069, 0.777947932618683, 0.777947932618683, 0.781010719754977, 0.781010719754977, 0.7840735068912711, 0.7840735068912711, 0.7856049004594181, 0.7856049004594181, 0.7871362940275651, 0.7871362940275651, 0.7901990811638591, 0.7901990811638591, 0.7917304747320061, 0.7917304747320061, 0.7932618683001531, 0.7932618683001531, 0.7947932618683001, 0.7947932618683001, 0.8039816232771823, 0.8039816232771823, 0.8055130168453293, 0.8055130168453293, 0.8162327718223583, 0.8162327718223583, 0.8177641653905053, 0.8177641653905053, 0.8192955589586524, 0.8192955589586524, 0.8269525267993875, 0.8269525267993875, 0.8315467075038285, 0.8315467075038285, 0.8346094946401225, 0.8346094946401225, 0.8361408882082695, 0.8361408882082695, 0.8407350689127105, 0.8407350689127105, 0.8437978560490046, 0.8437978560490046, 0.8453292496171516, 0.8453292496171516, 0.8483920367534457, 0.8483920367534457, 0.8545176110260337, 0.8545176110260337, 0.8560490045941807, 0.8560490045941807, 0.8575803981623277, 0.8575803981623277, 0.8606431852986217, 0.8606431852986217, 0.8621745788667687, 0.8621745788667687, 0.8683001531393568, 0.8683001531393568, 0.8698315467075038, 0.8698315467075038, 0.8713629402756509, 0.8713629402756509, 0.8744257274119449, 0.8744257274119449, 0.8805513016845329, 0.8805513016845329, 0.8820826952526799, 0.8820826952526799, 0.8836140888208269, 0.8836140888208269, 0.885145482388974, 0.885145482388974, 0.886676875957121, 0.886676875957121, 0.888208269525268, 0.888208269525268, 0.891271056661562, 0.891271056661562, 0.892802450229709, 0.892802450229709, 0.8943338437978561, 0.8943338437978561, 0.8973966309341501, 0.8973966309341501, 0.8989280245022971, 0.8989280245022971, 0.900459418070444, 0.900459418070444, 0.9035222052067381, 0.9035222052067381, 0.9081163859111792, 0.9081163859111792, 0.9096477794793262, 0.9096477794793262, 0.9111791730474732, 0.9111791730474732, 0.9127105666156202, 0.9127105666156202, 0.9142419601837672, 0.9142419601837672, 0.9157733537519143, 0.9157733537519143, 0.9173047473200613, 0.9173047473200613, 0.9203675344563553, 0.9203675344563553, 0.9218989280245024, 0.9218989280245024, 0.9234303215926493, 0.9234303215926493, 0.9249617151607963, 0.9249617151607963, 0.9264931087289433, 0.9264931087289433, 0.9295558958652373, 0.9295558958652373, 0.9326186830015314, 0.9326186830015314, 0.9341500765696784, 0.9341500765696784, 0.9356814701378254, 0.9356814701378254, 0.9372128637059725, 0.9372128637059725, 0.9387442572741195, 0.9387442572741195, 0.9402756508422665, 0.9402756508422665, 0.9418070444104135, 0.9418070444104135, 0.9433384379785605, 0.9433384379785605, 0.9448698315467075, 0.9448698315467075, 0.9464012251148545, 0.9464012251148545, 0.9479326186830015, 0.9479326186830015, 0.9494640122511485, 0.9494640122511485, 0.9525267993874426, 0.9525267993874426, 0.9540581929555896, 0.9540581929555896, 0.9571209800918836, 0.9571209800918836, 0.9586523736600306, 0.9586523736600306, 0.9601837672281777, 0.9601837672281777, 0.9617151607963247, 0.9617151607963247, 0.9632465543644717, 0.9632465543644717, 0.9647779479326187, 0.9647779479326187, 0.9663093415007658, 0.9663093415007658, 0.9709035222052067, 0.9709035222052067, 0.9724349157733537, 0.9724349157733537, 0.9739663093415007, 0.9739663093415007, 0.9770290964777948, 0.9770290964777948, 0.9785604900459418, 0.9785604900459418, 0.9800918836140888, 0.9800918836140888, 0.9831546707503829, 0.9831546707503829, 0.9862174578866769, 0.9862174578866769, 0.9877488514548239, 0.9877488514548239, 0.9892802450229708, 0.9892802450229708, 0.9908116385911179, 0.9908116385911179, 0.9938744257274119, 0.9938744257274119, 0.998468606431853, 0.998468606431853, 1.0], \"type\": \"scatter\", \"uid\": \"3d187242-3396-11e9-8c97-67fc199b385a\"}, {\"line\": {\"color\": \"navy\", \"dash\": \"dash\", \"width\": 2}, \"mode\": \"lines\", \"showlegend\": false, \"x\": [0, 1], \"y\": [0, 1], \"type\": \"scatter\", \"uid\": \"3d187404-3396-11e9-8c97-67fc199b385a\"}, {\"line\": {\"color\": \"cyan\", \"width\": 2}, \"mode\": \"lines\", \"name\": \"H21 AUC(area = 0.86)\", \"x\": [0.0, 0.0, 0.002150537634408602, 0.002150537634408602, 0.004301075268817204, 0.004301075268817204, 0.0064516129032258064, 0.0064516129032258064, 0.008602150537634409, 0.008602150537634409, 0.010752688172043012, 0.010752688172043012, 0.012903225806451613, 0.012903225806451613, 0.015053763440860216, 0.015053763440860216, 0.017204301075268817, 0.017204301075268817, 0.01935483870967742, 0.01935483870967742, 0.021505376344086023, 0.021505376344086023, 0.023655913978494623, 0.023655913978494623, 0.025806451612903226, 0.025806451612903226, 0.02795698924731183, 0.02795698924731183, 0.030107526881720432, 0.030107526881720432, 0.03225806451612903, 0.03225806451612903, 0.034408602150537634, 0.034408602150537634, 0.043010752688172046, 0.043010752688172046, 0.04516129032258064, 0.04516129032258064, 0.04946236559139785, 0.04946236559139785, 0.05161290322580645, 0.05161290322580645, 0.053763440860215055, 0.053763440860215055, 0.05591397849462366, 0.05591397849462366, 0.05806451612903226, 0.05806451612903226, 0.06236559139784946, 0.06236559139784946, 0.06451612903225806, 0.06451612903225806, 0.06881720430107527, 0.06881720430107527, 0.07096774193548387, 0.07096774193548387, 0.07741935483870968, 0.07741935483870968, 0.08172043010752689, 0.08172043010752689, 0.08387096774193549, 0.08387096774193549, 0.08602150537634409, 0.08602150537634409, 0.09032258064516129, 0.09032258064516129, 0.09462365591397849, 0.09462365591397849, 0.0967741935483871, 0.0967741935483871, 0.0989247311827957, 0.0989247311827957, 0.1032258064516129, 0.1032258064516129, 0.1053763440860215, 0.1053763440860215, 0.10752688172043011, 0.10752688172043011, 0.12043010752688173, 0.12043010752688173, 0.12473118279569892, 0.12473118279569892, 0.13118279569892474, 0.13118279569892474, 0.13333333333333333, 0.13333333333333333, 0.13548387096774195, 0.13548387096774195, 0.13978494623655913, 0.13978494623655913, 0.14193548387096774, 0.14193548387096774, 0.14408602150537633, 0.14408602150537633, 0.14623655913978495, 0.14623655913978495, 0.15053763440860216, 0.15053763440860216, 0.15483870967741936, 0.15483870967741936, 0.15913978494623657, 0.15913978494623657, 0.16129032258064516, 0.16129032258064516, 0.16344086021505377, 0.16344086021505377, 0.16989247311827957, 0.16989247311827957, 0.17204301075268819, 0.17204301075268819, 0.17634408602150536, 0.17634408602150536, 0.17849462365591398, 0.17849462365591398, 0.1827956989247312, 0.1827956989247312, 0.1870967741935484, 0.1870967741935484, 0.18924731182795698, 0.18924731182795698, 0.1913978494623656, 0.1913978494623656, 0.1935483870967742, 0.1935483870967742, 0.1978494623655914, 0.1978494623655914, 0.2, 0.2, 0.20430107526881722, 0.20430107526881722, 0.2129032258064516, 0.2129032258064516, 0.221505376344086, 0.221505376344086, 0.22580645161290322, 0.22580645161290322, 0.22795698924731184, 0.22795698924731184, 0.24086021505376345, 0.24086021505376345, 0.24731182795698925, 0.24731182795698925, 0.24946236559139784, 0.24946236559139784, 0.2537634408602151, 0.2537634408602151, 0.25806451612903225, 0.25806451612903225, 0.2623655913978495, 0.2623655913978495, 0.2645161290322581, 0.2645161290322581, 0.2817204301075269, 0.2817204301075269, 0.2838709677419355, 0.2838709677419355, 0.2924731182795699, 0.2924731182795699, 0.2946236559139785, 0.2946236559139785, 0.2967741935483871, 0.2967741935483871, 0.3010752688172043, 0.3010752688172043, 0.3118279569892473, 0.3118279569892473, 0.31827956989247314, 0.31827956989247314, 0.3204301075268817, 0.3204301075268817, 0.3225806451612903, 0.3225806451612903, 0.32688172043010755, 0.32688172043010755, 0.33763440860215055, 0.33763440860215055, 0.34408602150537637, 0.34408602150537637, 0.35053763440860214, 0.35053763440860214, 0.3548387096774194, 0.3548387096774194, 0.35913978494623655, 0.35913978494623655, 0.3720430107526882, 0.3720430107526882, 0.3935483870967742, 0.3935483870967742, 0.3978494623655914, 0.3978494623655914, 0.432258064516129, 0.432258064516129, 0.443010752688172, 0.443010752688172, 0.46021505376344085, 0.46021505376344085, 0.4645161290322581, 0.4645161290322581, 0.46881720430107526, 0.46881720430107526, 0.4838709677419355, 0.4838709677419355, 0.5032258064516129, 0.5032258064516129, 0.5075268817204301, 0.5075268817204301, 0.5204301075268817, 0.5204301075268817, 0.5225806451612903, 0.5225806451612903, 0.5548387096774193, 0.5548387096774193, 0.578494623655914, 0.578494623655914, 0.5935483870967742, 0.5935483870967742, 0.610752688172043, 0.610752688172043, 0.6236559139784946, 0.6236559139784946, 0.6516129032258065, 0.6516129032258065, 0.6623655913978495, 0.6623655913978495, 0.6688172043010753, 0.6688172043010753, 0.6989247311827957, 0.6989247311827957, 0.7268817204301076, 0.7268817204301076, 0.7397849462365591, 0.7397849462365591, 0.7462365591397849, 0.7462365591397849, 0.7505376344086021, 0.7505376344086021, 0.7526881720430108, 0.7526881720430108, 0.7591397849462366, 0.7591397849462366, 0.7720430107526882, 0.7720430107526882, 0.7849462365591398, 0.7849462365591398, 0.8129032258064516, 0.8129032258064516, 0.832258064516129, 0.832258064516129, 0.8387096774193549, 0.8387096774193549, 0.8946236559139785, 0.8946236559139785, 0.9483870967741935, 0.9483870967741935, 1.0], \"y\": [0.0024813895781637717, 0.12406947890818859, 0.12406947890818859, 0.22084367245657568, 0.22084367245657568, 0.2555831265508685, 0.2555831265508685, 0.29280397022332505, 0.29280397022332505, 0.2977667493796526, 0.2977667493796526, 0.31265508684863524, 0.31265508684863524, 0.3250620347394541, 0.3250620347394541, 0.34739454094292804, 0.34739454094292804, 0.3598014888337469, 0.3598014888337469, 0.36228287841191065, 0.36228287841191065, 0.38213399503722084, 0.38213399503722084, 0.4044665012406948, 0.4044665012406948, 0.4218362282878412, 0.4218362282878412, 0.43672456575682383, 0.43672456575682383, 0.4466501240694789, 0.4466501240694789, 0.4491315136476427, 0.4491315136476427, 0.46153846153846156, 0.46153846153846156, 0.4640198511166253, 0.4640198511166253, 0.47146401985111663, 0.47146401985111663, 0.47642679900744417, 0.47642679900744417, 0.4838709677419355, 0.4838709677419355, 0.48883374689826303, 0.48883374689826303, 0.49627791563275436, 0.49627791563275436, 0.4987593052109181, 0.4987593052109181, 0.511166253101737, 0.511166253101737, 0.5161290322580645, 0.5161290322580645, 0.5260545905707196, 0.5260545905707196, 0.5310173697270472, 0.5310173697270472, 0.5558312655086849, 0.5558312655086849, 0.5632754342431762, 0.5632754342431762, 0.56575682382134, 0.56575682382134, 0.5732009925558312, 0.5732009925558312, 0.5831265508684863, 0.5831265508684863, 0.5856079404466501, 0.5856079404466501, 0.5955334987593052, 0.5955334987593052, 0.607940446650124, 0.607940446650124, 0.6104218362282878, 0.6104218362282878, 0.6129032258064516, 0.6129032258064516, 0.6203473945409429, 0.6203473945409429, 0.6228287841191067, 0.6228287841191067, 0.630272952853598, 0.630272952853598, 0.6327543424317618, 0.6327543424317618, 0.6352357320099256, 0.6352357320099256, 0.6377171215880894, 0.6377171215880894, 0.6426799007444168, 0.6426799007444168, 0.6451612903225806, 0.6451612903225806, 0.6550868486352357, 0.6550868486352357, 0.6600496277915633, 0.6600496277915633, 0.6674937965260546, 0.6674937965260546, 0.6799007444168734, 0.6799007444168734, 0.6823821339950372, 0.6823821339950372, 0.6898263027295285, 0.6898263027295285, 0.6923076923076923, 0.6923076923076923, 0.6947890818858561, 0.6947890818858561, 0.6997518610421837, 0.6997518610421837, 0.7121588089330024, 0.7121588089330024, 0.7146401985111662, 0.7146401985111662, 0.7220843672456576, 0.7220843672456576, 0.7270471464019851, 0.7270471464019851, 0.7320099255583127, 0.7320099255583127, 0.7344913151364765, 0.7344913151364765, 0.7543424317617866, 0.7543424317617866, 0.7568238213399504, 0.7568238213399504, 0.7692307692307693, 0.7692307692307693, 0.771712158808933, 0.771712158808933, 0.7766749379652605, 0.7766749379652605, 0.7791563275434243, 0.7791563275434243, 0.7915632754342432, 0.7915632754342432, 0.794044665012407, 0.794044665012407, 0.8014888337468983, 0.8014888337468983, 0.8064516129032258, 0.8064516129032258, 0.8138957816377171, 0.8138957816377171, 0.8163771712158809, 0.8163771712158809, 0.8188585607940446, 0.8188585607940446, 0.8213399503722084, 0.8213399503722084, 0.8238213399503722, 0.8238213399503722, 0.8287841191066998, 0.8287841191066998, 0.8312655086848635, 0.8312655086848635, 0.8337468982630273, 0.8337468982630273, 0.8387096774193549, 0.8387096774193549, 0.8411910669975186, 0.8411910669975186, 0.8436724565756824, 0.8436724565756824, 0.8486352357320099, 0.8486352357320099, 0.8535980148883374, 0.8535980148883374, 0.858560794044665, 0.858560794044665, 0.8635235732009926, 0.8635235732009926, 0.8660049627791563, 0.8660049627791563, 0.8684863523573201, 0.8684863523573201, 0.8759305210918115, 0.8759305210918115, 0.8808933002481389, 0.8808933002481389, 0.8858560794044665, 0.8858560794044665, 0.8957816377171216, 0.8957816377171216, 0.9007444168734491, 0.9007444168734491, 0.9081885856079405, 0.9081885856079405, 0.9106699751861043, 0.9106699751861043, 0.913151364764268, 0.913151364764268, 0.9156327543424317, 0.9156327543424317, 0.9181141439205955, 0.9181141439205955, 0.9205955334987593, 0.9205955334987593, 0.9230769230769231, 0.9230769230769231, 0.9255583126550868, 0.9255583126550868, 0.9330024813895782, 0.9330024813895782, 0.9379652605459057, 0.9379652605459057, 0.9404466501240695, 0.9404466501240695, 0.9429280397022333, 0.9429280397022333, 0.9454094292803971, 0.9454094292803971, 0.9478908188585607, 0.9478908188585607, 0.9503722084367245, 0.9503722084367245, 0.9553349875930521, 0.9553349875930521, 0.9578163771712159, 0.9578163771712159, 0.9602977667493796, 0.9602977667493796, 0.9627791563275434, 0.9627791563275434, 0.9652605459057072, 0.9652605459057072, 0.967741935483871, 0.967741935483871, 0.9702233250620348, 0.9702233250620348, 0.9727047146401985, 0.9727047146401985, 0.9751861042183623, 0.9751861042183623, 0.9776674937965261, 0.9776674937965261, 0.9801488833746899, 0.9801488833746899, 0.9826302729528535, 0.9826302729528535, 0.9851116625310173, 0.9851116625310173, 0.9875930521091811, 0.9875930521091811, 0.9900744416873449, 0.9900744416873449, 0.9925558312655087, 0.9925558312655087, 0.9950372208436724, 0.9950372208436724, 0.9975186104218362, 0.9975186104218362, 1.0, 1.0], \"type\": \"scatter\", \"uid\": \"3d1874fe-3396-11e9-8c97-67fc199b385a\"}, {\"line\": {\"color\": \"green\", \"width\": 2}, \"mode\": \"lines\", \"name\": \"H18 AUC(area = 0.52)\", \"x\": [0.0, 0.00234192037470726, 0.00234192037470726, 0.00936768149882904, 0.00936768149882904, 0.0117096018735363, 0.0117096018735363, 0.01873536299765808, 0.01873536299765808, 0.02107728337236534, 0.02107728337236534, 0.0234192037470726, 0.0234192037470726, 0.02576112412177986, 0.02576112412177986, 0.02810304449648712, 0.02810304449648712, 0.0351288056206089, 0.0351288056206089, 0.03747072599531616, 0.03747072599531616, 0.04215456674473068, 0.04215456674473068, 0.05152224824355972, 0.05152224824355972, 0.053864168618266976, 0.053864168618266976, 0.05620608899297424, 0.05620608899297424, 0.0585480093676815, 0.0585480093676815, 0.06557377049180328, 0.06557377049180328, 0.06791569086651054, 0.06791569086651054, 0.0702576112412178, 0.0702576112412178, 0.07494145199063232, 0.07494145199063232, 0.08196721311475409, 0.08196721311475409, 0.08430913348946135, 0.08430913348946135, 0.08665105386416862, 0.08665105386416862, 0.09133489461358314, 0.09133489461358314, 0.10070257611241218, 0.10070257611241218, 0.10304449648711944, 0.10304449648711944, 0.1053864168618267, 0.1053864168618267, 0.117096018735363, 0.117096018735363, 0.12177985948477751, 0.12177985948477751, 0.1288056206088993, 0.1288056206088993, 0.13348946135831383, 0.13348946135831383, 0.1358313817330211, 0.1358313817330211, 0.1451990632318501, 0.1451990632318501, 0.1522248243559719, 0.1522248243559719, 0.15690866510538642, 0.15690866510538642, 0.16393442622950818, 0.16393442622950818, 0.16627634660421545, 0.16627634660421545, 0.17096018735362997, 0.17096018735362997, 0.1756440281030445, 0.1756440281030445, 0.18266978922716628, 0.18266978922716628, 0.1920374707259953, 0.1920374707259953, 0.19437939110070257, 0.19437939110070257, 0.19672131147540983, 0.19672131147540983, 0.20374707259953162, 0.20374707259953162, 0.20608899297423888, 0.20608899297423888, 0.20843091334894615, 0.20843091334894615, 0.21311475409836064, 0.21311475409836064, 0.2154566744730679, 0.2154566744730679, 0.21779859484777517, 0.21779859484777517, 0.22014051522248243, 0.22014051522248243, 0.2224824355971897, 0.2224824355971897, 0.22482435597189696, 0.22482435597189696, 0.22950819672131148, 0.22950819672131148, 0.23185011709601874, 0.23185011709601874, 0.234192037470726, 0.234192037470726, 0.23653395784543327, 0.23653395784543327, 0.2388758782201405, 0.2388758782201405, 0.2529274004683841, 0.2529274004683841, 0.25526932084309134, 0.25526932084309134, 0.25995316159250587, 0.25995316159250587, 0.2646370023419204, 0.2646370023419204, 0.26697892271662765, 0.26697892271662765, 0.2693208430913349, 0.2693208430913349, 0.27400468384074944, 0.27400468384074944, 0.27634660421545665, 0.27634660421545665, 0.2786885245901639, 0.2786885245901639, 0.28337236533957844, 0.28337236533957844, 0.2857142857142857, 0.2857142857142857, 0.2927400468384075, 0.2927400468384075, 0.29508196721311475, 0.29508196721311475, 0.297423887587822, 0.297423887587822, 0.30210772833723654, 0.30210772833723654, 0.3091334894613583, 0.3091334894613583, 0.32084309133489464, 0.32084309133489464, 0.33489461358313816, 0.33489461358313816, 0.3372365339578454, 0.3372365339578454, 0.3395784543325527, 0.3395784543325527, 0.3442622950819672, 0.3442622950819672, 0.34660421545667447, 0.34660421545667447, 0.35362997658079626, 0.35362997658079626, 0.3559718969555035, 0.3559718969555035, 0.36065573770491804, 0.36065573770491804, 0.3629976580796253, 0.3629976580796253, 0.36768149882903983, 0.36768149882903983, 0.37236533957845436, 0.37236533957845436, 0.3747072599531616, 0.3747072599531616, 0.3770491803278688, 0.3770491803278688, 0.3793911007025761, 0.3793911007025761, 0.3864168618266979, 0.3864168618266979, 0.38875878220140514, 0.38875878220140514, 0.39344262295081966, 0.39344262295081966, 1.0], \"y\": [0.0, 0.0, 0.014778325123152709, 0.014778325123152709, 0.017241379310344827, 0.017241379310344827, 0.03201970443349754, 0.03201970443349754, 0.034482758620689655, 0.034482758620689655, 0.03694581280788178, 0.03694581280788178, 0.04187192118226601, 0.04187192118226601, 0.04433497536945813, 0.04433497536945813, 0.05172413793103448, 0.05172413793103448, 0.05665024630541872, 0.05665024630541872, 0.06403940886699508, 0.06403940886699508, 0.06896551724137931, 0.06896551724137931, 0.07142857142857142, 0.07142857142857142, 0.07881773399014778, 0.07881773399014778, 0.08374384236453201, 0.08374384236453201, 0.08866995073891626, 0.08866995073891626, 0.09113300492610837, 0.09113300492610837, 0.09852216748768473, 0.09852216748768473, 0.10098522167487685, 0.10098522167487685, 0.10344827586206896, 0.10344827586206896, 0.10837438423645321, 0.10837438423645321, 0.12315270935960591, 0.12315270935960591, 0.12561576354679804, 0.12561576354679804, 0.1330049261083744, 0.1330049261083744, 0.1354679802955665, 0.1354679802955665, 0.13793103448275862, 0.13793103448275862, 0.14285714285714285, 0.14285714285714285, 0.1477832512315271, 0.1477832512315271, 0.15024630541871922, 0.15024630541871922, 0.16009852216748768, 0.16009852216748768, 0.16748768472906403, 0.16748768472906403, 0.1748768472906404, 0.1748768472906404, 0.17733990147783252, 0.17733990147783252, 0.18226600985221675, 0.18226600985221675, 0.18472906403940886, 0.18472906403940886, 0.18719211822660098, 0.18719211822660098, 0.1896551724137931, 0.1896551724137931, 0.19704433497536947, 0.19704433497536947, 0.2019704433497537, 0.2019704433497537, 0.2044334975369458, 0.2044334975369458, 0.20689655172413793, 0.20689655172413793, 0.20935960591133004, 0.20935960591133004, 0.21182266009852216, 0.21182266009852216, 0.21428571428571427, 0.21428571428571427, 0.21921182266009853, 0.21921182266009853, 0.22413793103448276, 0.22413793103448276, 0.22660098522167488, 0.22660098522167488, 0.229064039408867, 0.229064039408867, 0.23399014778325122, 0.23399014778325122, 0.23645320197044334, 0.23645320197044334, 0.2413793103448276, 0.2413793103448276, 0.2438423645320197, 0.2438423645320197, 0.2536945812807882, 0.2536945812807882, 0.2561576354679803, 0.2561576354679803, 0.270935960591133, 0.270935960591133, 0.2733990147783251, 0.2733990147783251, 0.2881773399014778, 0.2881773399014778, 0.29064039408866993, 0.29064039408866993, 0.29310344827586204, 0.29310344827586204, 0.29802955665024633, 0.29802955665024633, 0.30049261083743845, 0.30049261083743845, 0.30295566502463056, 0.30295566502463056, 0.32019704433497537, 0.32019704433497537, 0.3226600985221675, 0.3226600985221675, 0.3251231527093596, 0.3251231527093596, 0.33004926108374383, 0.33004926108374383, 0.33251231527093594, 0.33251231527093594, 0.33497536945812806, 0.33497536945812806, 0.3374384236453202, 0.3374384236453202, 0.3399014778325123, 0.3399014778325123, 0.34236453201970446, 0.34236453201970446, 0.3448275862068966, 0.3448275862068966, 0.3472906403940887, 0.3472906403940887, 0.35467980295566504, 0.35467980295566504, 0.35714285714285715, 0.35714285714285715, 0.3645320197044335, 0.3645320197044335, 0.3669950738916256, 0.3669950738916256, 0.37192118226600984, 0.37192118226600984, 0.37438423645320196, 0.37438423645320196, 0.3793103448275862, 0.3793103448275862, 0.3842364532019704, 0.3842364532019704, 0.3866995073891626, 0.3866995073891626, 0.3891625615763547, 0.3891625615763547, 0.39901477832512317, 0.39901477832512317, 0.4039408866995074, 0.4039408866995074, 0.4064039408866995, 0.4064039408866995, 0.4088669950738916, 0.4088669950738916, 0.41133004926108374, 0.41133004926108374, 0.41379310344827586, 0.41379310344827586, 0.41625615763546797, 0.41625615763546797, 0.4187192118226601, 1.0], \"type\": \"scatter\", \"uid\": \"3d1875d0-3396-11e9-8c97-67fc199b385a\"}, {\"line\": {\"color\": \"indigo\", \"width\": 2}, \"mode\": \"lines\", \"name\": \"H8 AUC(area = 0.56)\", \"x\": [0.0, 0.10593900481540931, 0.10754414125200643, 0.10754414125200643, 0.10914927768860354, 0.10914927768860354, 0.11075441412520064, 0.11075441412520064, 0.11396468699839486, 0.11396468699839486, 0.11556982343499198, 0.11556982343499198, 0.1187800963081862, 0.1187800963081862, 0.12038523274478331, 0.12038523274478331, 0.12359550561797752, 0.12359550561797752, 0.12680577849117175, 0.12680577849117175, 0.12841091492776885, 0.12841091492776885, 0.13001605136436598, 0.13001605136436598, 0.13162118780096307, 0.13162118780096307, 0.1332263242375602, 0.1332263242375602, 0.13964686998394862, 0.13964686998394862, 0.14285714285714285, 0.14285714285714285, 0.14446227929373998, 0.14446227929373998, 0.14606741573033707, 0.14606741573033707, 0.1476725521669342, 0.1476725521669342, 0.1508828250401284, 0.1508828250401284, 0.15248796147672553, 0.15248796147672553, 0.15569823434991975, 0.15569823434991975, 0.15730337078651685, 0.15730337078651685, 0.16211878009630817, 0.16211878009630817, 0.1637239165329053, 0.1637239165329053, 0.16693418940609953, 0.16693418940609953, 0.16853932584269662, 0.16853932584269662, 0.17174959871589085, 0.17174959871589085, 0.17335473515248795, 0.17335473515248795, 0.17656500802568217, 0.17656500802568217, 0.1797752808988764, 0.1797752808988764, 0.18138041733547353, 0.18138041733547353, 0.18298555377207062, 0.18298555377207062, 0.18459069020866772, 0.18459069020866772, 0.18940609951845908, 0.18940609951845908, 0.19101123595505617, 0.19101123595505617, 0.1926163723916533, 0.1926163723916533, 0.19743178170144463, 0.19743178170144463, 0.19903691813804172, 0.19903691813804172, 0.20224719101123595, 0.20224719101123595, 0.20385232744783308, 0.20385232744783308, 0.20706260032102727, 0.20706260032102727, 0.2086677367576244, 0.2086677367576244, 0.21348314606741572, 0.21348314606741572, 0.21508828250401285, 0.21508828250401285, 0.21669341894060995, 0.21669341894060995, 0.21829855537720708, 0.21829855537720708, 0.21990369181380418, 0.21990369181380418, 0.22150882825040127, 0.22150882825040127, 0.2231139646869984, 0.2231139646869984, 0.2247191011235955, 0.2247191011235955, 0.22792937399678972, 0.22792937399678972, 0.23274478330658105, 0.23274478330658105, 0.23434991974317818, 0.23434991974317818, 0.23595505617977527, 0.23595505617977527, 0.2375601926163724, 0.2375601926163724, 0.2391653290529695, 0.2391653290529695, 0.24558587479935795, 0.24558587479935795, 0.2536115569823435, 0.2536115569823435, 0.25842696629213485, 0.25842696629213485, 0.26003210272873195, 0.26003210272873195, 0.26163723916532905, 0.26163723916532905, 0.2696629213483146, 0.2696629213483146, 0.27287319422150885, 0.27287319422150885, 0.27447833065810595, 0.27447833065810595, 0.27608346709470305, 0.27608346709470305, 0.27768860353130015, 0.27768860353130015, 0.27929373996789725, 0.27929373996789725, 0.2808988764044944, 0.2808988764044944, 0.2841091492776886, 0.2841091492776886, 0.28892455858747995, 0.28892455858747995, 0.29052969502407705, 0.29052969502407705, 0.29213483146067415, 0.29213483146067415, 0.2969502407704655, 0.2969502407704655, 0.30497592295345105, 0.30497592295345105, 0.30818619582664525, 0.30818619582664525, 0.3097913322632424, 0.3097913322632424, 0.3146067415730337, 0.3146067415730337, 0.3162118780096308, 0.3162118780096308, 0.32102728731942215, 0.32102728731942215, 0.32263242375601925, 0.32263242375601925, 0.3274478330658106, 0.3274478330658106, 0.3290529695024077, 0.3290529695024077, 0.33386837881219905, 0.33386837881219905, 0.33707865168539325, 0.33707865168539325, 0.3402889245585875, 0.3402889245585875, 0.3467094703049759, 0.3467094703049759, 0.34991974317817015, 0.34991974317817015, 0.35313001605136435, 0.35313001605136435, 0.3563402889245586, 0.3563402889245586, 0.3611556982343499, 0.3611556982343499, 0.36436597110754415, 0.36436597110754415, 0.36597110754414125, 0.36597110754414125, 0.3739967897271268, 0.3739967897271268, 0.37881219903691815, 0.37881219903691815, 0.38041733547351525, 0.38041733547351525, 0.38202247191011235, 0.38202247191011235, 0.38362760834670945, 0.38362760834670945, 0.3852327447833066, 0.3852327447833066, 0.3868378812199037, 0.3868378812199037, 0.3884430176565008, 0.3884430176565008, 0.39486356340288925, 0.39486356340288925, 0.39646869983948635, 0.39646869983948635, 0.3996789727126806, 0.3996789727126806, 0.4012841091492777, 0.4012841091492777, 0.4028892455858748, 0.4028892455858748, 0.406099518459069, 0.406099518459069, 0.40930979133226325, 0.40930979133226325, 0.41091492776886035, 0.41091492776886035, 0.41252006420545745, 0.41252006420545745, 0.4157303370786517, 0.4157303370786517, 0.420545746388443, 0.420545746388443, 0.42375601926163725, 0.42375601926163725, 0.42696629213483145, 0.42696629213483145, 0.4333868378812199, 0.4333868378812199, 0.434991974317817, 0.434991974317817, 0.43820224719101125, 0.43820224719101125, 0.4446227929373997, 0.4446227929373997, 0.45264847512038525, 0.45264847512038525, 0.45425361155698235, 0.45425361155698235, 0.45746388443017655, 0.45746388443017655, 0.4590690208667737, 0.4590690208667737, 0.4606741573033708, 0.4606741573033708, 0.4654895666131621, 0.4654895666131621, 0.47191011235955055, 0.47191011235955055, 0.47351524879614765, 0.47351524879614765, 0.4751203852327448, 0.4751203852327448, 0.4767255216693419, 0.4767255216693419, 0.48314606741573035, 0.48314606741573035, 0.4911717495987159, 0.4911717495987159, 0.492776886035313, 0.492776886035313, 0.49919743178170145, 0.49919743178170145, 0.5008025682182986, 0.5008025682182986, 0.5024077046548957, 0.5024077046548957, 0.5040128410914928, 0.5040128410914928, 0.507223113964687, 0.507223113964687, 0.5088282504012841, 0.5088282504012841, 0.5104333868378812, 0.5104333868378812, 0.5120385232744783, 0.5120385232744783, 0.5152487961476726, 0.5152487961476726, 0.5168539325842697, 0.5168539325842697, 0.5200642054574639, 0.5200642054574639, 0.521669341894061, 0.521669341894061, 0.5248796147672552, 0.5248796147672552, 0.5280898876404494, 0.5280898876404494, 0.5296950240770465, 0.5296950240770465, 0.5329052969502408, 0.5329052969502408, 0.5345104333868379, 0.5345104333868379, 0.536115569823435, 0.536115569823435, 0.5377207062600321, 0.5377207062600321, 0.5393258426966292, 0.5393258426966292, 0.5425361155698234, 0.5425361155698234, 0.5537720706260032, 0.5537720706260032, 0.5553772070626003, 0.5553772070626003, 0.5569823434991974, 0.5569823434991974, 0.5585874799357945, 0.5585874799357945, 0.5601926163723917, 0.5601926163723917, 0.565008025682183, 0.565008025682183, 0.5746388443017657, 0.5746388443017657, 0.5842696629213483, 0.5842696629213483, 0.5874799357945425, 0.5874799357945425, 0.5906902086677368, 0.5906902086677368, 0.5922953451043339, 0.5922953451043339, 0.5955056179775281, 0.5955056179775281, 0.5987158908507223, 0.5987158908507223, 0.6019261637239165, 0.6019261637239165, 0.6051364365971108, 0.6051364365971108, 0.6067415730337079, 0.6067415730337079, 0.608346709470305, 0.608346709470305, 0.6099518459069021, 0.6099518459069021, 0.6131621187800963, 0.6131621187800963, 0.6147672552166934, 0.6147672552166934, 0.6179775280898876, 0.6179775280898876, 0.6243980738362761, 0.6243980738362761, 0.6260032102728732, 0.6260032102728732, 0.6292134831460674, 0.6292134831460674, 0.6308186195826645, 0.6308186195826645, 0.6356340288924559, 0.6356340288924559, 0.637239165329053, 0.637239165329053, 0.6388443017656501, 0.6388443017656501, 0.6420545746388443, 0.6420545746388443, 0.6436597110754414, 0.6436597110754414, 0.6452648475120385, 0.6452648475120385, 0.6468699839486356, 0.6468699839486356, 0.6484751203852327, 0.6484751203852327, 0.6500802568218299, 0.6500802568218299, 0.6597110754414125, 0.6597110754414125, 0.6613162118780096, 0.6613162118780096, 0.6677367576243981, 0.6677367576243981, 0.6709470304975923, 0.6709470304975923, 0.6741573033707865, 0.6741573033707865, 0.6757624398073836, 0.6757624398073836, 0.6789727126805778, 0.6789727126805778, 0.6821829855537721, 0.6821829855537721, 0.6837881219903692, 0.6837881219903692, 0.6886035313001605, 0.6886035313001605, 0.6934189406099518, 0.6934189406099518, 0.695024077046549, 0.695024077046549, 0.6966292134831461, 0.6966292134831461, 0.7014446227929374, 0.7014446227929374, 0.7030497592295345, 0.7030497592295345, 0.7046548956661316, 0.7046548956661316, 0.7078651685393258, 0.7078651685393258, 0.7110754414125201, 0.7110754414125201, 0.7174959871589085, 0.7174959871589085, 0.7191011235955056, 0.7191011235955056, 0.7303370786516854, 0.7303370786516854, 0.7319422150882825, 0.7319422150882825, 0.7335473515248796, 0.7335473515248796, 0.7367576243980738, 0.7367576243980738, 0.7383627608346709, 0.7383627608346709, 0.7415730337078652, 0.7415730337078652, 0.7447833065810594, 0.7447833065810594, 0.7463884430176565, 0.7463884430176565, 0.7479935794542536, 0.7479935794542536, 0.7512038523274478, 0.7512038523274478, 0.7560192616372392, 0.7560192616372392, 0.7576243980738363, 0.7576243980738363, 0.7592295345104334, 0.7592295345104334, 0.7672552166934189, 0.7672552166934189, 0.7688603531300161, 0.7688603531300161, 0.7704654895666132, 0.7704654895666132, 0.7752808988764045, 0.7752808988764045, 0.7784911717495987, 0.7784911717495987, 0.78330658105939, 0.78330658105939, 0.7849117174959872, 0.7849117174959872, 0.7913322632423756, 0.7913322632423756, 0.7961476725521669, 0.7961476725521669, 0.8009630818619583, 0.8009630818619583, 0.8025682182985554, 0.8025682182985554, 0.8105939004815409, 0.8105939004815409, 0.812199036918138, 0.812199036918138, 0.8154093097913323, 0.8154093097913323, 0.8186195826645265, 0.8186195826645265, 0.8202247191011236, 0.8202247191011236, 0.8218298555377207, 0.8218298555377207, 0.8250401284109149, 0.8250401284109149, 0.826645264847512, 0.826645264847512, 0.8394863563402889, 0.8394863563402889, 0.8443017656500803, 0.8443017656500803, 0.8459069020866774, 0.8459069020866774, 0.8491171749598716, 0.8491171749598716, 0.8507223113964687, 0.8507223113964687, 0.8539325842696629, 0.8539325842696629, 0.8587479935794543, 0.8587479935794543, 0.8603531300160514, 0.8603531300160514, 0.8619582664526485, 0.8619582664526485, 0.8667736757624398, 0.8667736757624398, 0.8683788121990369, 0.8683788121990369, 0.869983948635634, 0.869983948635634, 0.8715890850722311, 0.8715890850722311, 0.8764044943820225, 0.8764044943820225, 0.8796147672552167, 0.8796147672552167, 0.8812199036918138, 0.8812199036918138, 0.8956661316211878, 0.8956661316211878, 0.9020866773675762, 0.9020866773675762, 0.9069020866773676, 0.9069020866773676, 0.9085072231139647, 0.9085072231139647, 0.9117174959871589, 0.9117174959871589, 0.9245585874799358, 0.9245585874799358, 0.9309791332263242, 0.9309791332263242, 0.942215088282504, 0.942215088282504, 0.9438202247191011, 0.9438202247191011, 0.9502407704654896, 0.9502407704654896, 0.9518459069020867, 0.9518459069020867, 0.9534510433386838, 0.9534510433386838, 0.956661316211878, 0.956661316211878, 0.9646869983948636, 0.9646869983948636, 1.0], \"y\": [0.0, 0.10278745644599303, 0.10278745644599303, 0.11672473867595819, 0.11672473867595819, 0.11846689895470383, 0.11846689895470383, 0.12020905923344948, 0.12020905923344948, 0.12195121951219512, 0.12195121951219512, 0.1289198606271777, 0.1289198606271777, 0.13066202090592335, 0.13066202090592335, 0.13588850174216027, 0.13588850174216027, 0.13763066202090593, 0.13763066202090593, 0.13937282229965156, 0.13937282229965156, 0.14111498257839722, 0.14111498257839722, 0.14634146341463414, 0.14634146341463414, 0.15331010452961671, 0.15331010452961671, 0.15853658536585366, 0.15853658536585366, 0.16202090592334495, 0.16202090592334495, 0.17247386759581881, 0.17247386759581881, 0.17770034843205576, 0.17770034843205576, 0.18292682926829268, 0.18292682926829268, 0.18466898954703834, 0.18466898954703834, 0.18641114982578397, 0.18641114982578397, 0.18815331010452963, 0.18815331010452963, 0.1916376306620209, 0.1916376306620209, 0.19686411149825783, 0.19686411149825783, 0.20209059233449478, 0.20209059233449478, 0.20557491289198607, 0.20557491289198607, 0.20905923344947736, 0.20905923344947736, 0.21254355400696864, 0.21254355400696864, 0.21602787456445993, 0.21602787456445993, 0.21951219512195122, 0.21951219512195122, 0.22125435540069685, 0.22125435540069685, 0.2229965156794425, 0.2229965156794425, 0.22996515679442509, 0.22996515679442509, 0.23519163763066203, 0.23519163763066203, 0.23693379790940766, 0.23693379790940766, 0.23867595818815332, 0.23867595818815332, 0.2421602787456446, 0.2421602787456446, 0.24390243902439024, 0.24390243902439024, 0.2456445993031359, 0.2456445993031359, 0.24912891986062718, 0.24912891986062718, 0.25261324041811845, 0.25261324041811845, 0.26306620209059234, 0.26306620209059234, 0.26480836236933797, 0.26480836236933797, 0.2665505226480836, 0.2665505226480836, 0.2682926829268293, 0.2682926829268293, 0.2700348432055749, 0.2700348432055749, 0.27526132404181186, 0.27526132404181186, 0.2770034843205575, 0.2770034843205575, 0.2787456445993031, 0.2787456445993031, 0.2804878048780488, 0.2804878048780488, 0.28222996515679444, 0.28222996515679444, 0.2857142857142857, 0.2857142857142857, 0.2874564459930314, 0.2874564459930314, 0.289198606271777, 0.289198606271777, 0.2926829268292683, 0.2926829268292683, 0.2979094076655052, 0.2979094076655052, 0.30139372822299654, 0.30139372822299654, 0.30313588850174217, 0.30313588850174217, 0.3048780487804878, 0.3048780487804878, 0.3083623693379791, 0.3083623693379791, 0.31010452961672474, 0.31010452961672474, 0.3153310104529617, 0.3153310104529617, 0.3205574912891986, 0.3205574912891986, 0.32752613240418116, 0.32752613240418116, 0.32926829268292684, 0.32926829268292684, 0.3344947735191638, 0.3344947735191638, 0.3362369337979094, 0.3362369337979094, 0.3397212543554007, 0.3397212543554007, 0.343205574912892, 0.343205574912892, 0.34668989547038326, 0.34668989547038326, 0.34843205574912894, 0.34843205574912894, 0.3501742160278746, 0.3501742160278746, 0.35714285714285715, 0.35714285714285715, 0.3588850174216028, 0.3588850174216028, 0.36585365853658536, 0.36585365853658536, 0.367595818815331, 0.367595818815331, 0.3710801393728223, 0.3710801393728223, 0.37282229965156793, 0.37282229965156793, 0.38153310104529614, 0.38153310104529614, 0.3832752613240418, 0.3832752613240418, 0.38501742160278746, 0.38501742160278746, 0.3867595818815331, 0.3867595818815331, 0.39372822299651566, 0.39372822299651566, 0.3989547038327526, 0.3989547038327526, 0.4024390243902439, 0.4024390243902439, 0.40418118466898956, 0.40418118466898956, 0.4076655052264808, 0.4076655052264808, 0.41114982578397213, 0.41114982578397213, 0.4146341463414634, 0.4146341463414634, 0.4163763066202091, 0.4163763066202091, 0.41986062717770034, 0.41986062717770034, 0.4337979094076655, 0.4337979094076655, 0.4355400696864111, 0.4355400696864111, 0.43902439024390244, 0.43902439024390244, 0.44076655052264807, 0.44076655052264807, 0.4425087108013937, 0.4425087108013937, 0.4442508710801394, 0.4442508710801394, 0.445993031358885, 0.445993031358885, 0.44773519163763065, 0.44773519163763065, 0.45121951219512196, 0.45121951219512196, 0.4547038327526132, 0.4547038327526132, 0.45993031358885017, 0.45993031358885017, 0.4651567944250871, 0.4651567944250871, 0.4721254355400697, 0.4721254355400697, 0.4738675958188153, 0.4738675958188153, 0.47909407665505227, 0.47909407665505227, 0.48257839721254353, 0.48257839721254353, 0.4843205574912892, 0.4843205574912892, 0.48606271777003485, 0.48606271777003485, 0.4878048780487805, 0.4878048780487805, 0.4930313588850174, 0.4930313588850174, 0.4965156794425087, 0.4965156794425087, 0.5017421602787456, 0.5017421602787456, 0.5052264808362369, 0.5052264808362369, 0.5069686411149826, 0.5069686411149826, 0.5104529616724739, 0.5104529616724739, 0.5139372822299652, 0.5139372822299652, 0.5156794425087108, 0.5156794425087108, 0.519163763066202, 0.519163763066202, 0.5209059233449478, 0.5209059233449478, 0.524390243902439, 0.524390243902439, 0.5296167247386759, 0.5296167247386759, 0.5313588850174216, 0.5313588850174216, 0.5365853658536586, 0.5365853658536586, 0.5418118466898955, 0.5418118466898955, 0.5452961672473867, 0.5452961672473867, 0.5470383275261324, 0.5470383275261324, 0.5487804878048781, 0.5487804878048781, 0.554006968641115, 0.554006968641115, 0.5557491289198606, 0.5557491289198606, 0.5592334494773519, 0.5592334494773519, 0.5679442508710801, 0.5679442508710801, 0.5696864111498258, 0.5696864111498258, 0.5818815331010453, 0.5818815331010453, 0.5905923344947736, 0.5905923344947736, 0.5958188153310104, 0.5958188153310104, 0.6010452961672473, 0.6010452961672473, 0.6027874564459931, 0.6027874564459931, 0.6045296167247387, 0.6045296167247387, 0.60801393728223, 0.60801393728223, 0.6097560975609756, 0.6097560975609756, 0.6114982578397212, 0.6114982578397212, 0.6149825783972126, 0.6149825783972126, 0.6167247386759582, 0.6167247386759582, 0.6184668989547039, 0.6184668989547039, 0.6219512195121951, 0.6219512195121951, 0.6236933797909407, 0.6236933797909407, 0.6254355400696864, 0.6254355400696864, 0.627177700348432, 0.627177700348432, 0.6289198606271778, 0.6289198606271778, 0.632404181184669, 0.632404181184669, 0.6341463414634146, 0.6341463414634146, 0.6411149825783972, 0.6411149825783972, 0.6428571428571429, 0.6428571428571429, 0.6445993031358885, 0.6445993031358885, 0.6463414634146342, 0.6463414634146342, 0.6498257839721254, 0.6498257839721254, 0.6550522648083623, 0.6550522648083623, 0.6567944250871081, 0.6567944250871081, 0.6655052264808362, 0.6655052264808362, 0.6689895470383276, 0.6689895470383276, 0.6724738675958188, 0.6724738675958188, 0.6759581881533101, 0.6759581881533101, 0.6777003484320557, 0.6777003484320557, 0.681184668989547, 0.681184668989547, 0.6846689895470384, 0.6846689895470384, 0.686411149825784, 0.686411149825784, 0.6881533101045296, 0.6881533101045296, 0.6933797909407665, 0.6933797909407665, 0.6951219512195121, 0.6951219512195121, 0.6986062717770035, 0.6986062717770035, 0.7003484320557491, 0.7003484320557491, 0.7020905923344948, 0.7020905923344948, 0.7038327526132404, 0.7038327526132404, 0.7090592334494773, 0.7090592334494773, 0.710801393728223, 0.710801393728223, 0.7125435540069687, 0.7125435540069687, 0.7142857142857143, 0.7142857142857143, 0.7160278745644599, 0.7160278745644599, 0.7195121951219512, 0.7195121951219512, 0.7247386759581882, 0.7247386759581882, 0.7264808362369338, 0.7264808362369338, 0.7299651567944251, 0.7299651567944251, 0.7317073170731707, 0.7317073170731707, 0.7334494773519163, 0.7334494773519163, 0.7369337979094077, 0.7369337979094077, 0.7421602787456446, 0.7421602787456446, 0.7508710801393729, 0.7508710801393729, 0.7526132404181185, 0.7526132404181185, 0.7543554006968641, 0.7543554006968641, 0.7578397212543554, 0.7578397212543554, 0.7613240418118467, 0.7613240418118467, 0.764808362369338, 0.764808362369338, 0.7700348432055749, 0.7700348432055749, 0.7717770034843205, 0.7717770034843205, 0.7735191637630662, 0.7735191637630662, 0.7752613240418118, 0.7752613240418118, 0.7787456445993032, 0.7787456445993032, 0.7822299651567944, 0.7822299651567944, 0.7839721254355401, 0.7839721254355401, 0.7926829268292683, 0.7926829268292683, 0.794425087108014, 0.794425087108014, 0.7979094076655052, 0.7979094076655052, 0.8013937282229965, 0.8013937282229965, 0.8048780487804879, 0.8048780487804879, 0.8066202090592335, 0.8066202090592335, 0.8205574912891986, 0.8205574912891986, 0.8222996515679443, 0.8222996515679443, 0.8275261324041812, 0.8275261324041812, 0.8362369337979094, 0.8362369337979094, 0.8397212543554007, 0.8397212543554007, 0.8432055749128919, 0.8432055749128919, 0.8449477351916377, 0.8449477351916377, 0.8466898954703833, 0.8466898954703833, 0.8484320557491289, 0.8484320557491289, 0.8519163763066202, 0.8519163763066202, 0.8536585365853658, 0.8536585365853658, 0.8554006968641115, 0.8554006968641115, 0.8571428571428571, 0.8571428571428571, 0.8623693379790941, 0.8623693379790941, 0.867595818815331, 0.867595818815331, 0.8693379790940766, 0.8693379790940766, 0.8710801393728222, 0.8710801393728222, 0.8745644599303136, 0.8745644599303136, 0.8763066202090593, 0.8763066202090593, 0.8780487804878049, 0.8780487804878049, 0.8797909407665505, 0.8797909407665505, 0.8815331010452961, 0.8815331010452961, 0.8832752613240418, 0.8832752613240418, 0.8850174216027874, 0.8850174216027874, 0.8885017421602788, 0.8885017421602788, 0.8902439024390244, 0.8902439024390244, 0.89198606271777, 0.89198606271777, 0.8937282229965157, 0.8937282229965157, 0.8954703832752613, 0.8954703832752613, 0.8972125435540069, 0.8972125435540069, 0.9024390243902439, 0.9024390243902439, 0.9041811846689896, 0.9041811846689896, 0.9094076655052264, 0.9094076655052264, 0.9146341463414634, 0.9146341463414634, 0.9163763066202091, 0.9163763066202091, 0.9198606271777003, 0.9198606271777003, 0.921602787456446, 0.921602787456446, 0.9250871080139372, 0.9250871080139372, 0.926829268292683, 0.926829268292683, 0.9285714285714286, 0.9285714285714286, 0.9303135888501742, 0.9303135888501742, 0.9355400696864111, 0.9355400696864111, 0.9372822299651568, 0.9372822299651568, 0.9425087108013938, 0.9425087108013938, 0.9442508710801394, 0.9442508710801394, 0.9477351916376306, 0.9477351916376306, 0.9529616724738676, 0.9529616724738676, 0.9547038327526133, 0.9547038327526133, 0.9564459930313589, 0.9564459930313589, 0.9599303135888502, 0.9599303135888502, 0.9616724738675958, 0.9616724738675958, 0.9651567944250871, 0.9651567944250871, 0.9668989547038328, 0.9668989547038328, 0.9703832752613241, 0.9703832752613241, 0.9721254355400697, 0.9721254355400697, 0.9738675958188153, 0.9738675958188153, 0.980836236933798, 0.980836236933798, 0.9825783972125436, 0.9825783972125436, 0.9843205574912892, 0.9843205574912892, 0.9895470383275261, 0.9895470383275261, 0.9947735191637631, 0.9947735191637631, 0.9982578397212544, 0.9982578397212544, 1.0, 1.0], \"type\": \"scatter\", \"uid\": \"3d187698-3396-11e9-8c97-67fc199b385a\"}],\n",
       "            {\"title\": \"Hourly ROC Comparisons\", \"xaxis\": {\"title\": \"False Positive Rate\"}, \"yaxis\": {\"title\": \"Sensitivity\"}},\n",
       "            {\"showLink\": false, \"linkText\": \"Export to plot.ly\"}\n",
       "        ).then(function () {return Plotly.addFrames('e1fa3374-ed34-416d-b913-b1b4427366a0',{});}).then(function(){Plotly.animate('e1fa3374-ed34-416d-b913-b1b4427366a0');})\n",
       "        });</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"e1fa3374-ed34-416d-b913-b1b4427366a0\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "        Plotly.plot(\n",
       "            'e1fa3374-ed34-416d-b913-b1b4427366a0',\n",
       "            [{\"line\": {\"color\": \"darkorange\", \"width\": 2}, \"mode\": \"lines\", \"name\": \"H23 AUC(area = 0.81)\", \"x\": [0.0, 0.0, 0.0020161290322580645, 0.0020161290322580645, 0.004032258064516129, 0.004032258064516129, 0.006048387096774193, 0.006048387096774193, 0.008064516129032258, 0.008064516129032258, 0.010080645161290322, 0.010080645161290322, 0.012096774193548387, 0.012096774193548387, 0.016129032258064516, 0.016129032258064516, 0.018145161290322582, 0.018145161290322582, 0.020161290322580645, 0.020161290322580645, 0.02217741935483871, 0.02217741935483871, 0.024193548387096774, 0.024193548387096774, 0.02620967741935484, 0.02620967741935484, 0.028225806451612902, 0.028225806451612902, 0.03024193548387097, 0.03024193548387097, 0.03225806451612903, 0.03225806451612903, 0.034274193548387094, 0.034274193548387094, 0.036290322580645164, 0.036290322580645164, 0.038306451612903226, 0.038306451612903226, 0.04032258064516129, 0.04032258064516129, 0.04233870967741935, 0.04233870967741935, 0.04435483870967742, 0.04435483870967742, 0.04838709677419355, 0.04838709677419355, 0.05040322580645161, 0.05040322580645161, 0.05443548387096774, 0.05443548387096774, 0.05846774193548387, 0.05846774193548387, 0.06451612903225806, 0.06451612903225806, 0.06854838709677419, 0.06854838709677419, 0.07258064516129033, 0.07258064516129033, 0.07459677419354839, 0.07459677419354839, 0.07661290322580645, 0.07661290322580645, 0.08064516129032258, 0.08064516129032258, 0.08266129032258064, 0.08266129032258064, 0.0846774193548387, 0.0846774193548387, 0.08669354838709678, 0.08669354838709678, 0.08870967741935484, 0.08870967741935484, 0.0907258064516129, 0.0907258064516129, 0.09274193548387097, 0.09274193548387097, 0.0967741935483871, 0.0967741935483871, 0.09879032258064516, 0.09879032258064516, 0.10080645161290322, 0.10080645161290322, 0.1028225806451613, 0.1028225806451613, 0.10483870967741936, 0.10483870967741936, 0.10887096774193548, 0.10887096774193548, 0.11088709677419355, 0.11088709677419355, 0.11491935483870967, 0.11491935483870967, 0.11693548387096774, 0.11693548387096774, 0.11895161290322581, 0.11895161290322581, 0.12096774193548387, 0.12096774193548387, 0.12298387096774194, 0.12298387096774194, 0.125, 0.125, 0.12903225806451613, 0.12903225806451613, 0.13306451612903225, 0.13306451612903225, 0.1350806451612903, 0.1350806451612903, 0.13709677419354838, 0.13709677419354838, 0.13911290322580644, 0.13911290322580644, 0.14112903225806453, 0.14112903225806453, 0.1431451612903226, 0.1431451612903226, 0.14516129032258066, 0.14516129032258066, 0.15120967741935484, 0.15120967741935484, 0.1532258064516129, 0.1532258064516129, 0.15524193548387097, 0.15524193548387097, 0.15725806451612903, 0.15725806451612903, 0.1592741935483871, 0.1592741935483871, 0.16532258064516128, 0.16532258064516128, 0.1693548387096774, 0.1693548387096774, 0.17137096774193547, 0.17137096774193547, 0.17338709677419356, 0.17338709677419356, 0.17540322580645162, 0.17540322580645162, 0.1774193548387097, 0.1774193548387097, 0.17943548387096775, 0.17943548387096775, 0.1814516129032258, 0.1814516129032258, 0.18548387096774194, 0.18548387096774194, 0.1875, 0.1875, 0.18951612903225806, 0.18951612903225806, 0.19153225806451613, 0.19153225806451613, 0.1935483870967742, 0.1935483870967742, 0.19556451612903225, 0.19556451612903225, 0.1975806451612903, 0.1975806451612903, 0.20362903225806453, 0.20362903225806453, 0.20766129032258066, 0.20766129032258066, 0.20967741935483872, 0.20967741935483872, 0.21169354838709678, 0.21169354838709678, 0.21370967741935484, 0.21370967741935484, 0.2157258064516129, 0.2157258064516129, 0.22580645161290322, 0.22580645161290322, 0.23588709677419356, 0.23588709677419356, 0.23790322580645162, 0.23790322580645162, 0.2399193548387097, 0.2399193548387097, 0.24193548387096775, 0.24193548387096775, 0.2439516129032258, 0.2439516129032258, 0.2540322580645161, 0.2540322580645161, 0.2560483870967742, 0.2560483870967742, 0.25806451612903225, 0.25806451612903225, 0.2600806451612903, 0.2600806451612903, 0.26411290322580644, 0.26411290322580644, 0.2701612903225806, 0.2701612903225806, 0.2721774193548387, 0.2721774193548387, 0.2762096774193548, 0.2762096774193548, 0.2842741935483871, 0.2842741935483871, 0.28830645161290325, 0.28830645161290325, 0.2903225806451613, 0.2903225806451613, 0.29838709677419356, 0.29838709677419356, 0.3004032258064516, 0.3004032258064516, 0.30443548387096775, 0.30443548387096775, 0.3084677419354839, 0.3084677419354839, 0.3286290322580645, 0.3286290322580645, 0.33064516129032256, 0.33064516129032256, 0.3326612903225806, 0.3326612903225806, 0.3346774193548387, 0.3346774193548387, 0.3387096774193548, 0.3387096774193548, 0.3467741935483871, 0.3467741935483871, 0.3528225806451613, 0.3528225806451613, 0.35685483870967744, 0.35685483870967744, 0.3588709677419355, 0.3588709677419355, 0.3629032258064516, 0.3629032258064516, 0.3649193548387097, 0.3649193548387097, 0.3709677419354839, 0.3709677419354839, 0.37298387096774194, 0.37298387096774194, 0.3790322580645161, 0.3790322580645161, 0.38306451612903225, 0.38306451612903225, 0.3850806451612903, 0.3850806451612903, 0.39314516129032256, 0.39314516129032256, 0.39919354838709675, 0.39919354838709675, 0.4032258064516129, 0.4032258064516129, 0.40524193548387094, 0.40524193548387094, 0.4092741935483871, 0.4092741935483871, 0.4254032258064516, 0.4254032258064516, 0.43951612903225806, 0.43951612903225806, 0.4415322580645161, 0.4415322580645161, 0.4495967741935484, 0.4495967741935484, 0.4576612903225806, 0.4576612903225806, 0.4637096774193548, 0.4637096774193548, 0.4657258064516129, 0.4657258064516129, 0.47580645161290325, 0.47580645161290325, 0.4838709677419355, 0.4838709677419355, 0.48588709677419356, 0.48588709677419356, 0.49193548387096775, 0.49193548387096775, 0.49798387096774194, 0.49798387096774194, 0.5040322580645161, 0.5040322580645161, 0.5100806451612904, 0.5100806451612904, 0.5161290322580645, 0.5161290322580645, 0.5181451612903226, 0.5181451612903226, 0.5241935483870968, 0.5241935483870968, 0.5282258064516129, 0.5282258064516129, 0.530241935483871, 0.530241935483871, 0.532258064516129, 0.532258064516129, 0.5342741935483871, 0.5342741935483871, 0.5463709677419355, 0.5463709677419355, 0.5645161290322581, 0.5645161290322581, 0.5705645161290323, 0.5705645161290323, 0.5725806451612904, 0.5725806451612904, 0.5766129032258065, 0.5766129032258065, 0.5786290322580645, 0.5786290322580645, 0.6008064516129032, 0.6008064516129032, 0.6068548387096774, 0.6068548387096774, 0.6129032258064516, 0.6129032258064516, 0.625, 0.625, 0.6350806451612904, 0.6350806451612904, 0.6370967741935484, 0.6370967741935484, 0.6391129032258065, 0.6391129032258065, 0.6471774193548387, 0.6471774193548387, 0.6592741935483871, 0.6592741935483871, 0.6653225806451613, 0.6653225806451613, 0.6673387096774194, 0.6673387096774194, 0.6774193548387096, 0.6774193548387096, 0.7096774193548387, 0.7096774193548387, 0.7258064516129032, 0.7258064516129032, 0.7278225806451613, 0.7278225806451613, 0.7439516129032258, 0.7439516129032258, 0.7479838709677419, 0.7479838709677419, 0.75, 0.75, 0.7701612903225806, 0.7701612903225806, 0.7762096774193549, 0.7762096774193549, 0.7782258064516129, 0.7782258064516129, 0.7983870967741935, 0.7983870967741935, 0.8004032258064516, 0.8004032258064516, 0.8125, 0.8125, 0.8185483870967742, 0.8185483870967742, 0.8366935483870968, 0.8366935483870968, 0.8508064516129032, 0.8508064516129032, 0.8548387096774194, 0.8548387096774194, 0.8629032258064516, 0.8629032258064516, 0.8689516129032258, 0.8689516129032258, 0.875, 0.875, 0.8790322580645161, 0.8790322580645161, 0.8850806451612904, 0.8850806451612904, 0.8870967741935484, 0.8870967741935484, 0.9455645161290323, 0.9455645161290323, 0.9536290322580645, 0.9536290322580645, 0.9637096774193549, 0.9637096774193549, 0.9979838709677419, 0.9979838709677419, 1.0, 1.0], \"y\": [0.0015313935681470138, 0.022970903522205207, 0.022970903522205207, 0.07197549770290965, 0.07197549770290965, 0.07963246554364471, 0.07963246554364471, 0.09494640122511486, 0.09494640122511486, 0.10107197549770292, 0.10107197549770292, 0.11638591117917305, 0.11638591117917305, 0.13935681470137826, 0.13935681470137826, 0.1562021439509954, 0.1562021439509954, 0.19295558958652373, 0.19295558958652373, 0.21592649310872894, 0.21592649310872894, 0.21745788667687596, 0.21745788667687596, 0.23889739663093415, 0.23889739663093415, 0.26033690658499237, 0.26033690658499237, 0.28483920367534454, 0.28483920367534454, 0.2986217457886677, 0.2986217457886677, 0.30474732006125577, 0.30474732006125577, 0.3108728943338438, 0.3108728943338438, 0.3124042879019908, 0.3124042879019908, 0.32465543644716693, 0.32465543644716693, 0.32618683001531396, 0.32618683001531396, 0.32924961715160794, 0.32924961715160794, 0.3445635528330781, 0.3445635528330781, 0.3614088820826952, 0.3614088820826952, 0.36294027565084225, 0.36294027565084225, 0.38437978560490044, 0.38437978560490044, 0.39509954058192953, 0.39509954058192953, 0.39663093415007655, 0.39663093415007655, 0.4211332312404288, 0.4211332312404288, 0.4349157733537519, 0.4349157733537519, 0.43644716692189894, 0.43644716692189894, 0.43950995405819293, 0.43950995405819293, 0.445635528330781, 0.445635528330781, 0.44869831546707506, 0.44869831546707506, 0.4548238897396631, 0.4548238897396631, 0.45788667687595713, 0.45788667687595713, 0.46248085758039814, 0.46248085758039814, 0.4655436447166922, 0.4655436447166922, 0.47320061255742724, 0.47320061255742724, 0.4762633996937213, 0.4762633996937213, 0.4777947932618683, 0.4777947932618683, 0.4793261868300153, 0.4793261868300153, 0.4885145482388974, 0.4885145482388974, 0.5206738131699847, 0.5206738131699847, 0.5252679938744257, 0.5252679938744257, 0.5267993874425727, 0.5267993874425727, 0.5467075038284839, 0.5467075038284839, 0.5482388973966309, 0.5482388973966309, 0.5497702909647779, 0.5497702909647779, 0.554364471669219, 0.554364471669219, 0.55895865237366, 0.55895865237366, 0.5604900459418071, 0.5604900459418071, 0.5620214395099541, 0.5620214395099541, 0.5635528330781011, 0.5635528330781011, 0.5666156202143952, 0.5666156202143952, 0.5681470137825421, 0.5681470137825421, 0.5696784073506891, 0.5696784073506891, 0.5727411944869831, 0.5727411944869831, 0.5819295558958653, 0.5819295558958653, 0.5911179173047473, 0.5911179173047473, 0.5972434915773354, 0.5972434915773354, 0.6018376722817764, 0.6018376722817764, 0.6033690658499234, 0.6033690658499234, 0.6079632465543645, 0.6079632465543645, 0.6110260336906586, 0.6110260336906586, 0.6140888208269525, 0.6140888208269525, 0.6156202143950995, 0.6156202143950995, 0.6263399693721287, 0.6263399693721287, 0.6339969372128637, 0.6339969372128637, 0.6385911179173047, 0.6385911179173047, 0.6477794793261868, 0.6477794793261868, 0.6493108728943339, 0.6493108728943339, 0.6508422664624809, 0.6508422664624809, 0.655436447166922, 0.655436447166922, 0.6584992343032159, 0.6584992343032159, 0.666156202143951, 0.666156202143951, 0.667687595712098, 0.667687595712098, 0.669218989280245, 0.669218989280245, 0.6707503828483921, 0.6707503828483921, 0.6722817764165391, 0.6722817764165391, 0.6860643185298622, 0.6860643185298622, 0.6891271056661562, 0.6891271056661562, 0.6937212863705973, 0.6937212863705973, 0.6983154670750383, 0.6983154670750383, 0.7044410413476263, 0.7044410413476263, 0.7105666156202144, 0.7105666156202144, 0.7120980091883614, 0.7120980091883614, 0.7166921898928025, 0.7166921898928025, 0.7320061255742726, 0.7320061255742726, 0.7366003062787136, 0.7366003062787136, 0.7396630934150077, 0.7396630934150077, 0.7411944869831547, 0.7411944869831547, 0.7457886676875957, 0.7457886676875957, 0.7519142419601837, 0.7519142419601837, 0.7534456355283308, 0.7534456355283308, 0.7565084226646248, 0.7565084226646248, 0.7580398162327718, 0.7580398162327718, 0.7595712098009189, 0.7595712098009189, 0.7611026033690659, 0.7611026033690659, 0.7626339969372129, 0.7626339969372129, 0.7641653905053599, 0.7641653905053599, 0.7656967840735069, 0.7656967840735069, 0.777947932618683, 0.777947932618683, 0.781010719754977, 0.781010719754977, 0.7840735068912711, 0.7840735068912711, 0.7856049004594181, 0.7856049004594181, 0.7871362940275651, 0.7871362940275651, 0.7901990811638591, 0.7901990811638591, 0.7917304747320061, 0.7917304747320061, 0.7932618683001531, 0.7932618683001531, 0.7947932618683001, 0.7947932618683001, 0.8039816232771823, 0.8039816232771823, 0.8055130168453293, 0.8055130168453293, 0.8162327718223583, 0.8162327718223583, 0.8177641653905053, 0.8177641653905053, 0.8192955589586524, 0.8192955589586524, 0.8269525267993875, 0.8269525267993875, 0.8315467075038285, 0.8315467075038285, 0.8346094946401225, 0.8346094946401225, 0.8361408882082695, 0.8361408882082695, 0.8407350689127105, 0.8407350689127105, 0.8437978560490046, 0.8437978560490046, 0.8453292496171516, 0.8453292496171516, 0.8483920367534457, 0.8483920367534457, 0.8545176110260337, 0.8545176110260337, 0.8560490045941807, 0.8560490045941807, 0.8575803981623277, 0.8575803981623277, 0.8606431852986217, 0.8606431852986217, 0.8621745788667687, 0.8621745788667687, 0.8683001531393568, 0.8683001531393568, 0.8698315467075038, 0.8698315467075038, 0.8713629402756509, 0.8713629402756509, 0.8744257274119449, 0.8744257274119449, 0.8805513016845329, 0.8805513016845329, 0.8820826952526799, 0.8820826952526799, 0.8836140888208269, 0.8836140888208269, 0.885145482388974, 0.885145482388974, 0.886676875957121, 0.886676875957121, 0.888208269525268, 0.888208269525268, 0.891271056661562, 0.891271056661562, 0.892802450229709, 0.892802450229709, 0.8943338437978561, 0.8943338437978561, 0.8973966309341501, 0.8973966309341501, 0.8989280245022971, 0.8989280245022971, 0.900459418070444, 0.900459418070444, 0.9035222052067381, 0.9035222052067381, 0.9081163859111792, 0.9081163859111792, 0.9096477794793262, 0.9096477794793262, 0.9111791730474732, 0.9111791730474732, 0.9127105666156202, 0.9127105666156202, 0.9142419601837672, 0.9142419601837672, 0.9157733537519143, 0.9157733537519143, 0.9173047473200613, 0.9173047473200613, 0.9203675344563553, 0.9203675344563553, 0.9218989280245024, 0.9218989280245024, 0.9234303215926493, 0.9234303215926493, 0.9249617151607963, 0.9249617151607963, 0.9264931087289433, 0.9264931087289433, 0.9295558958652373, 0.9295558958652373, 0.9326186830015314, 0.9326186830015314, 0.9341500765696784, 0.9341500765696784, 0.9356814701378254, 0.9356814701378254, 0.9372128637059725, 0.9372128637059725, 0.9387442572741195, 0.9387442572741195, 0.9402756508422665, 0.9402756508422665, 0.9418070444104135, 0.9418070444104135, 0.9433384379785605, 0.9433384379785605, 0.9448698315467075, 0.9448698315467075, 0.9464012251148545, 0.9464012251148545, 0.9479326186830015, 0.9479326186830015, 0.9494640122511485, 0.9494640122511485, 0.9525267993874426, 0.9525267993874426, 0.9540581929555896, 0.9540581929555896, 0.9571209800918836, 0.9571209800918836, 0.9586523736600306, 0.9586523736600306, 0.9601837672281777, 0.9601837672281777, 0.9617151607963247, 0.9617151607963247, 0.9632465543644717, 0.9632465543644717, 0.9647779479326187, 0.9647779479326187, 0.9663093415007658, 0.9663093415007658, 0.9709035222052067, 0.9709035222052067, 0.9724349157733537, 0.9724349157733537, 0.9739663093415007, 0.9739663093415007, 0.9770290964777948, 0.9770290964777948, 0.9785604900459418, 0.9785604900459418, 0.9800918836140888, 0.9800918836140888, 0.9831546707503829, 0.9831546707503829, 0.9862174578866769, 0.9862174578866769, 0.9877488514548239, 0.9877488514548239, 0.9892802450229708, 0.9892802450229708, 0.9908116385911179, 0.9908116385911179, 0.9938744257274119, 0.9938744257274119, 0.998468606431853, 0.998468606431853, 1.0], \"type\": \"scatter\", \"uid\": \"3d187242-3396-11e9-8c97-67fc199b385a\"}, {\"line\": {\"color\": \"navy\", \"dash\": \"dash\", \"width\": 2}, \"mode\": \"lines\", \"showlegend\": false, \"x\": [0, 1], \"y\": [0, 1], \"type\": \"scatter\", \"uid\": \"3d187404-3396-11e9-8c97-67fc199b385a\"}, {\"line\": {\"color\": \"cyan\", \"width\": 2}, \"mode\": \"lines\", \"name\": \"H21 AUC(area = 0.86)\", \"x\": [0.0, 0.0, 0.002150537634408602, 0.002150537634408602, 0.004301075268817204, 0.004301075268817204, 0.0064516129032258064, 0.0064516129032258064, 0.008602150537634409, 0.008602150537634409, 0.010752688172043012, 0.010752688172043012, 0.012903225806451613, 0.012903225806451613, 0.015053763440860216, 0.015053763440860216, 0.017204301075268817, 0.017204301075268817, 0.01935483870967742, 0.01935483870967742, 0.021505376344086023, 0.021505376344086023, 0.023655913978494623, 0.023655913978494623, 0.025806451612903226, 0.025806451612903226, 0.02795698924731183, 0.02795698924731183, 0.030107526881720432, 0.030107526881720432, 0.03225806451612903, 0.03225806451612903, 0.034408602150537634, 0.034408602150537634, 0.043010752688172046, 0.043010752688172046, 0.04516129032258064, 0.04516129032258064, 0.04946236559139785, 0.04946236559139785, 0.05161290322580645, 0.05161290322580645, 0.053763440860215055, 0.053763440860215055, 0.05591397849462366, 0.05591397849462366, 0.05806451612903226, 0.05806451612903226, 0.06236559139784946, 0.06236559139784946, 0.06451612903225806, 0.06451612903225806, 0.06881720430107527, 0.06881720430107527, 0.07096774193548387, 0.07096774193548387, 0.07741935483870968, 0.07741935483870968, 0.08172043010752689, 0.08172043010752689, 0.08387096774193549, 0.08387096774193549, 0.08602150537634409, 0.08602150537634409, 0.09032258064516129, 0.09032258064516129, 0.09462365591397849, 0.09462365591397849, 0.0967741935483871, 0.0967741935483871, 0.0989247311827957, 0.0989247311827957, 0.1032258064516129, 0.1032258064516129, 0.1053763440860215, 0.1053763440860215, 0.10752688172043011, 0.10752688172043011, 0.12043010752688173, 0.12043010752688173, 0.12473118279569892, 0.12473118279569892, 0.13118279569892474, 0.13118279569892474, 0.13333333333333333, 0.13333333333333333, 0.13548387096774195, 0.13548387096774195, 0.13978494623655913, 0.13978494623655913, 0.14193548387096774, 0.14193548387096774, 0.14408602150537633, 0.14408602150537633, 0.14623655913978495, 0.14623655913978495, 0.15053763440860216, 0.15053763440860216, 0.15483870967741936, 0.15483870967741936, 0.15913978494623657, 0.15913978494623657, 0.16129032258064516, 0.16129032258064516, 0.16344086021505377, 0.16344086021505377, 0.16989247311827957, 0.16989247311827957, 0.17204301075268819, 0.17204301075268819, 0.17634408602150536, 0.17634408602150536, 0.17849462365591398, 0.17849462365591398, 0.1827956989247312, 0.1827956989247312, 0.1870967741935484, 0.1870967741935484, 0.18924731182795698, 0.18924731182795698, 0.1913978494623656, 0.1913978494623656, 0.1935483870967742, 0.1935483870967742, 0.1978494623655914, 0.1978494623655914, 0.2, 0.2, 0.20430107526881722, 0.20430107526881722, 0.2129032258064516, 0.2129032258064516, 0.221505376344086, 0.221505376344086, 0.22580645161290322, 0.22580645161290322, 0.22795698924731184, 0.22795698924731184, 0.24086021505376345, 0.24086021505376345, 0.24731182795698925, 0.24731182795698925, 0.24946236559139784, 0.24946236559139784, 0.2537634408602151, 0.2537634408602151, 0.25806451612903225, 0.25806451612903225, 0.2623655913978495, 0.2623655913978495, 0.2645161290322581, 0.2645161290322581, 0.2817204301075269, 0.2817204301075269, 0.2838709677419355, 0.2838709677419355, 0.2924731182795699, 0.2924731182795699, 0.2946236559139785, 0.2946236559139785, 0.2967741935483871, 0.2967741935483871, 0.3010752688172043, 0.3010752688172043, 0.3118279569892473, 0.3118279569892473, 0.31827956989247314, 0.31827956989247314, 0.3204301075268817, 0.3204301075268817, 0.3225806451612903, 0.3225806451612903, 0.32688172043010755, 0.32688172043010755, 0.33763440860215055, 0.33763440860215055, 0.34408602150537637, 0.34408602150537637, 0.35053763440860214, 0.35053763440860214, 0.3548387096774194, 0.3548387096774194, 0.35913978494623655, 0.35913978494623655, 0.3720430107526882, 0.3720430107526882, 0.3935483870967742, 0.3935483870967742, 0.3978494623655914, 0.3978494623655914, 0.432258064516129, 0.432258064516129, 0.443010752688172, 0.443010752688172, 0.46021505376344085, 0.46021505376344085, 0.4645161290322581, 0.4645161290322581, 0.46881720430107526, 0.46881720430107526, 0.4838709677419355, 0.4838709677419355, 0.5032258064516129, 0.5032258064516129, 0.5075268817204301, 0.5075268817204301, 0.5204301075268817, 0.5204301075268817, 0.5225806451612903, 0.5225806451612903, 0.5548387096774193, 0.5548387096774193, 0.578494623655914, 0.578494623655914, 0.5935483870967742, 0.5935483870967742, 0.610752688172043, 0.610752688172043, 0.6236559139784946, 0.6236559139784946, 0.6516129032258065, 0.6516129032258065, 0.6623655913978495, 0.6623655913978495, 0.6688172043010753, 0.6688172043010753, 0.6989247311827957, 0.6989247311827957, 0.7268817204301076, 0.7268817204301076, 0.7397849462365591, 0.7397849462365591, 0.7462365591397849, 0.7462365591397849, 0.7505376344086021, 0.7505376344086021, 0.7526881720430108, 0.7526881720430108, 0.7591397849462366, 0.7591397849462366, 0.7720430107526882, 0.7720430107526882, 0.7849462365591398, 0.7849462365591398, 0.8129032258064516, 0.8129032258064516, 0.832258064516129, 0.832258064516129, 0.8387096774193549, 0.8387096774193549, 0.8946236559139785, 0.8946236559139785, 0.9483870967741935, 0.9483870967741935, 1.0], \"y\": [0.0024813895781637717, 0.12406947890818859, 0.12406947890818859, 0.22084367245657568, 0.22084367245657568, 0.2555831265508685, 0.2555831265508685, 0.29280397022332505, 0.29280397022332505, 0.2977667493796526, 0.2977667493796526, 0.31265508684863524, 0.31265508684863524, 0.3250620347394541, 0.3250620347394541, 0.34739454094292804, 0.34739454094292804, 0.3598014888337469, 0.3598014888337469, 0.36228287841191065, 0.36228287841191065, 0.38213399503722084, 0.38213399503722084, 0.4044665012406948, 0.4044665012406948, 0.4218362282878412, 0.4218362282878412, 0.43672456575682383, 0.43672456575682383, 0.4466501240694789, 0.4466501240694789, 0.4491315136476427, 0.4491315136476427, 0.46153846153846156, 0.46153846153846156, 0.4640198511166253, 0.4640198511166253, 0.47146401985111663, 0.47146401985111663, 0.47642679900744417, 0.47642679900744417, 0.4838709677419355, 0.4838709677419355, 0.48883374689826303, 0.48883374689826303, 0.49627791563275436, 0.49627791563275436, 0.4987593052109181, 0.4987593052109181, 0.511166253101737, 0.511166253101737, 0.5161290322580645, 0.5161290322580645, 0.5260545905707196, 0.5260545905707196, 0.5310173697270472, 0.5310173697270472, 0.5558312655086849, 0.5558312655086849, 0.5632754342431762, 0.5632754342431762, 0.56575682382134, 0.56575682382134, 0.5732009925558312, 0.5732009925558312, 0.5831265508684863, 0.5831265508684863, 0.5856079404466501, 0.5856079404466501, 0.5955334987593052, 0.5955334987593052, 0.607940446650124, 0.607940446650124, 0.6104218362282878, 0.6104218362282878, 0.6129032258064516, 0.6129032258064516, 0.6203473945409429, 0.6203473945409429, 0.6228287841191067, 0.6228287841191067, 0.630272952853598, 0.630272952853598, 0.6327543424317618, 0.6327543424317618, 0.6352357320099256, 0.6352357320099256, 0.6377171215880894, 0.6377171215880894, 0.6426799007444168, 0.6426799007444168, 0.6451612903225806, 0.6451612903225806, 0.6550868486352357, 0.6550868486352357, 0.6600496277915633, 0.6600496277915633, 0.6674937965260546, 0.6674937965260546, 0.6799007444168734, 0.6799007444168734, 0.6823821339950372, 0.6823821339950372, 0.6898263027295285, 0.6898263027295285, 0.6923076923076923, 0.6923076923076923, 0.6947890818858561, 0.6947890818858561, 0.6997518610421837, 0.6997518610421837, 0.7121588089330024, 0.7121588089330024, 0.7146401985111662, 0.7146401985111662, 0.7220843672456576, 0.7220843672456576, 0.7270471464019851, 0.7270471464019851, 0.7320099255583127, 0.7320099255583127, 0.7344913151364765, 0.7344913151364765, 0.7543424317617866, 0.7543424317617866, 0.7568238213399504, 0.7568238213399504, 0.7692307692307693, 0.7692307692307693, 0.771712158808933, 0.771712158808933, 0.7766749379652605, 0.7766749379652605, 0.7791563275434243, 0.7791563275434243, 0.7915632754342432, 0.7915632754342432, 0.794044665012407, 0.794044665012407, 0.8014888337468983, 0.8014888337468983, 0.8064516129032258, 0.8064516129032258, 0.8138957816377171, 0.8138957816377171, 0.8163771712158809, 0.8163771712158809, 0.8188585607940446, 0.8188585607940446, 0.8213399503722084, 0.8213399503722084, 0.8238213399503722, 0.8238213399503722, 0.8287841191066998, 0.8287841191066998, 0.8312655086848635, 0.8312655086848635, 0.8337468982630273, 0.8337468982630273, 0.8387096774193549, 0.8387096774193549, 0.8411910669975186, 0.8411910669975186, 0.8436724565756824, 0.8436724565756824, 0.8486352357320099, 0.8486352357320099, 0.8535980148883374, 0.8535980148883374, 0.858560794044665, 0.858560794044665, 0.8635235732009926, 0.8635235732009926, 0.8660049627791563, 0.8660049627791563, 0.8684863523573201, 0.8684863523573201, 0.8759305210918115, 0.8759305210918115, 0.8808933002481389, 0.8808933002481389, 0.8858560794044665, 0.8858560794044665, 0.8957816377171216, 0.8957816377171216, 0.9007444168734491, 0.9007444168734491, 0.9081885856079405, 0.9081885856079405, 0.9106699751861043, 0.9106699751861043, 0.913151364764268, 0.913151364764268, 0.9156327543424317, 0.9156327543424317, 0.9181141439205955, 0.9181141439205955, 0.9205955334987593, 0.9205955334987593, 0.9230769230769231, 0.9230769230769231, 0.9255583126550868, 0.9255583126550868, 0.9330024813895782, 0.9330024813895782, 0.9379652605459057, 0.9379652605459057, 0.9404466501240695, 0.9404466501240695, 0.9429280397022333, 0.9429280397022333, 0.9454094292803971, 0.9454094292803971, 0.9478908188585607, 0.9478908188585607, 0.9503722084367245, 0.9503722084367245, 0.9553349875930521, 0.9553349875930521, 0.9578163771712159, 0.9578163771712159, 0.9602977667493796, 0.9602977667493796, 0.9627791563275434, 0.9627791563275434, 0.9652605459057072, 0.9652605459057072, 0.967741935483871, 0.967741935483871, 0.9702233250620348, 0.9702233250620348, 0.9727047146401985, 0.9727047146401985, 0.9751861042183623, 0.9751861042183623, 0.9776674937965261, 0.9776674937965261, 0.9801488833746899, 0.9801488833746899, 0.9826302729528535, 0.9826302729528535, 0.9851116625310173, 0.9851116625310173, 0.9875930521091811, 0.9875930521091811, 0.9900744416873449, 0.9900744416873449, 0.9925558312655087, 0.9925558312655087, 0.9950372208436724, 0.9950372208436724, 0.9975186104218362, 0.9975186104218362, 1.0, 1.0], \"type\": \"scatter\", \"uid\": \"3d1874fe-3396-11e9-8c97-67fc199b385a\"}, {\"line\": {\"color\": \"green\", \"width\": 2}, \"mode\": \"lines\", \"name\": \"H18 AUC(area = 0.52)\", \"x\": [0.0, 0.00234192037470726, 0.00234192037470726, 0.00936768149882904, 0.00936768149882904, 0.0117096018735363, 0.0117096018735363, 0.01873536299765808, 0.01873536299765808, 0.02107728337236534, 0.02107728337236534, 0.0234192037470726, 0.0234192037470726, 0.02576112412177986, 0.02576112412177986, 0.02810304449648712, 0.02810304449648712, 0.0351288056206089, 0.0351288056206089, 0.03747072599531616, 0.03747072599531616, 0.04215456674473068, 0.04215456674473068, 0.05152224824355972, 0.05152224824355972, 0.053864168618266976, 0.053864168618266976, 0.05620608899297424, 0.05620608899297424, 0.0585480093676815, 0.0585480093676815, 0.06557377049180328, 0.06557377049180328, 0.06791569086651054, 0.06791569086651054, 0.0702576112412178, 0.0702576112412178, 0.07494145199063232, 0.07494145199063232, 0.08196721311475409, 0.08196721311475409, 0.08430913348946135, 0.08430913348946135, 0.08665105386416862, 0.08665105386416862, 0.09133489461358314, 0.09133489461358314, 0.10070257611241218, 0.10070257611241218, 0.10304449648711944, 0.10304449648711944, 0.1053864168618267, 0.1053864168618267, 0.117096018735363, 0.117096018735363, 0.12177985948477751, 0.12177985948477751, 0.1288056206088993, 0.1288056206088993, 0.13348946135831383, 0.13348946135831383, 0.1358313817330211, 0.1358313817330211, 0.1451990632318501, 0.1451990632318501, 0.1522248243559719, 0.1522248243559719, 0.15690866510538642, 0.15690866510538642, 0.16393442622950818, 0.16393442622950818, 0.16627634660421545, 0.16627634660421545, 0.17096018735362997, 0.17096018735362997, 0.1756440281030445, 0.1756440281030445, 0.18266978922716628, 0.18266978922716628, 0.1920374707259953, 0.1920374707259953, 0.19437939110070257, 0.19437939110070257, 0.19672131147540983, 0.19672131147540983, 0.20374707259953162, 0.20374707259953162, 0.20608899297423888, 0.20608899297423888, 0.20843091334894615, 0.20843091334894615, 0.21311475409836064, 0.21311475409836064, 0.2154566744730679, 0.2154566744730679, 0.21779859484777517, 0.21779859484777517, 0.22014051522248243, 0.22014051522248243, 0.2224824355971897, 0.2224824355971897, 0.22482435597189696, 0.22482435597189696, 0.22950819672131148, 0.22950819672131148, 0.23185011709601874, 0.23185011709601874, 0.234192037470726, 0.234192037470726, 0.23653395784543327, 0.23653395784543327, 0.2388758782201405, 0.2388758782201405, 0.2529274004683841, 0.2529274004683841, 0.25526932084309134, 0.25526932084309134, 0.25995316159250587, 0.25995316159250587, 0.2646370023419204, 0.2646370023419204, 0.26697892271662765, 0.26697892271662765, 0.2693208430913349, 0.2693208430913349, 0.27400468384074944, 0.27400468384074944, 0.27634660421545665, 0.27634660421545665, 0.2786885245901639, 0.2786885245901639, 0.28337236533957844, 0.28337236533957844, 0.2857142857142857, 0.2857142857142857, 0.2927400468384075, 0.2927400468384075, 0.29508196721311475, 0.29508196721311475, 0.297423887587822, 0.297423887587822, 0.30210772833723654, 0.30210772833723654, 0.3091334894613583, 0.3091334894613583, 0.32084309133489464, 0.32084309133489464, 0.33489461358313816, 0.33489461358313816, 0.3372365339578454, 0.3372365339578454, 0.3395784543325527, 0.3395784543325527, 0.3442622950819672, 0.3442622950819672, 0.34660421545667447, 0.34660421545667447, 0.35362997658079626, 0.35362997658079626, 0.3559718969555035, 0.3559718969555035, 0.36065573770491804, 0.36065573770491804, 0.3629976580796253, 0.3629976580796253, 0.36768149882903983, 0.36768149882903983, 0.37236533957845436, 0.37236533957845436, 0.3747072599531616, 0.3747072599531616, 0.3770491803278688, 0.3770491803278688, 0.3793911007025761, 0.3793911007025761, 0.3864168618266979, 0.3864168618266979, 0.38875878220140514, 0.38875878220140514, 0.39344262295081966, 0.39344262295081966, 1.0], \"y\": [0.0, 0.0, 0.014778325123152709, 0.014778325123152709, 0.017241379310344827, 0.017241379310344827, 0.03201970443349754, 0.03201970443349754, 0.034482758620689655, 0.034482758620689655, 0.03694581280788178, 0.03694581280788178, 0.04187192118226601, 0.04187192118226601, 0.04433497536945813, 0.04433497536945813, 0.05172413793103448, 0.05172413793103448, 0.05665024630541872, 0.05665024630541872, 0.06403940886699508, 0.06403940886699508, 0.06896551724137931, 0.06896551724137931, 0.07142857142857142, 0.07142857142857142, 0.07881773399014778, 0.07881773399014778, 0.08374384236453201, 0.08374384236453201, 0.08866995073891626, 0.08866995073891626, 0.09113300492610837, 0.09113300492610837, 0.09852216748768473, 0.09852216748768473, 0.10098522167487685, 0.10098522167487685, 0.10344827586206896, 0.10344827586206896, 0.10837438423645321, 0.10837438423645321, 0.12315270935960591, 0.12315270935960591, 0.12561576354679804, 0.12561576354679804, 0.1330049261083744, 0.1330049261083744, 0.1354679802955665, 0.1354679802955665, 0.13793103448275862, 0.13793103448275862, 0.14285714285714285, 0.14285714285714285, 0.1477832512315271, 0.1477832512315271, 0.15024630541871922, 0.15024630541871922, 0.16009852216748768, 0.16009852216748768, 0.16748768472906403, 0.16748768472906403, 0.1748768472906404, 0.1748768472906404, 0.17733990147783252, 0.17733990147783252, 0.18226600985221675, 0.18226600985221675, 0.18472906403940886, 0.18472906403940886, 0.18719211822660098, 0.18719211822660098, 0.1896551724137931, 0.1896551724137931, 0.19704433497536947, 0.19704433497536947, 0.2019704433497537, 0.2019704433497537, 0.2044334975369458, 0.2044334975369458, 0.20689655172413793, 0.20689655172413793, 0.20935960591133004, 0.20935960591133004, 0.21182266009852216, 0.21182266009852216, 0.21428571428571427, 0.21428571428571427, 0.21921182266009853, 0.21921182266009853, 0.22413793103448276, 0.22413793103448276, 0.22660098522167488, 0.22660098522167488, 0.229064039408867, 0.229064039408867, 0.23399014778325122, 0.23399014778325122, 0.23645320197044334, 0.23645320197044334, 0.2413793103448276, 0.2413793103448276, 0.2438423645320197, 0.2438423645320197, 0.2536945812807882, 0.2536945812807882, 0.2561576354679803, 0.2561576354679803, 0.270935960591133, 0.270935960591133, 0.2733990147783251, 0.2733990147783251, 0.2881773399014778, 0.2881773399014778, 0.29064039408866993, 0.29064039408866993, 0.29310344827586204, 0.29310344827586204, 0.29802955665024633, 0.29802955665024633, 0.30049261083743845, 0.30049261083743845, 0.30295566502463056, 0.30295566502463056, 0.32019704433497537, 0.32019704433497537, 0.3226600985221675, 0.3226600985221675, 0.3251231527093596, 0.3251231527093596, 0.33004926108374383, 0.33004926108374383, 0.33251231527093594, 0.33251231527093594, 0.33497536945812806, 0.33497536945812806, 0.3374384236453202, 0.3374384236453202, 0.3399014778325123, 0.3399014778325123, 0.34236453201970446, 0.34236453201970446, 0.3448275862068966, 0.3448275862068966, 0.3472906403940887, 0.3472906403940887, 0.35467980295566504, 0.35467980295566504, 0.35714285714285715, 0.35714285714285715, 0.3645320197044335, 0.3645320197044335, 0.3669950738916256, 0.3669950738916256, 0.37192118226600984, 0.37192118226600984, 0.37438423645320196, 0.37438423645320196, 0.3793103448275862, 0.3793103448275862, 0.3842364532019704, 0.3842364532019704, 0.3866995073891626, 0.3866995073891626, 0.3891625615763547, 0.3891625615763547, 0.39901477832512317, 0.39901477832512317, 0.4039408866995074, 0.4039408866995074, 0.4064039408866995, 0.4064039408866995, 0.4088669950738916, 0.4088669950738916, 0.41133004926108374, 0.41133004926108374, 0.41379310344827586, 0.41379310344827586, 0.41625615763546797, 0.41625615763546797, 0.4187192118226601, 1.0], \"type\": \"scatter\", \"uid\": \"3d1875d0-3396-11e9-8c97-67fc199b385a\"}, {\"line\": {\"color\": \"indigo\", \"width\": 2}, \"mode\": \"lines\", \"name\": \"H8 AUC(area = 0.56)\", \"x\": [0.0, 0.10593900481540931, 0.10754414125200643, 0.10754414125200643, 0.10914927768860354, 0.10914927768860354, 0.11075441412520064, 0.11075441412520064, 0.11396468699839486, 0.11396468699839486, 0.11556982343499198, 0.11556982343499198, 0.1187800963081862, 0.1187800963081862, 0.12038523274478331, 0.12038523274478331, 0.12359550561797752, 0.12359550561797752, 0.12680577849117175, 0.12680577849117175, 0.12841091492776885, 0.12841091492776885, 0.13001605136436598, 0.13001605136436598, 0.13162118780096307, 0.13162118780096307, 0.1332263242375602, 0.1332263242375602, 0.13964686998394862, 0.13964686998394862, 0.14285714285714285, 0.14285714285714285, 0.14446227929373998, 0.14446227929373998, 0.14606741573033707, 0.14606741573033707, 0.1476725521669342, 0.1476725521669342, 0.1508828250401284, 0.1508828250401284, 0.15248796147672553, 0.15248796147672553, 0.15569823434991975, 0.15569823434991975, 0.15730337078651685, 0.15730337078651685, 0.16211878009630817, 0.16211878009630817, 0.1637239165329053, 0.1637239165329053, 0.16693418940609953, 0.16693418940609953, 0.16853932584269662, 0.16853932584269662, 0.17174959871589085, 0.17174959871589085, 0.17335473515248795, 0.17335473515248795, 0.17656500802568217, 0.17656500802568217, 0.1797752808988764, 0.1797752808988764, 0.18138041733547353, 0.18138041733547353, 0.18298555377207062, 0.18298555377207062, 0.18459069020866772, 0.18459069020866772, 0.18940609951845908, 0.18940609951845908, 0.19101123595505617, 0.19101123595505617, 0.1926163723916533, 0.1926163723916533, 0.19743178170144463, 0.19743178170144463, 0.19903691813804172, 0.19903691813804172, 0.20224719101123595, 0.20224719101123595, 0.20385232744783308, 0.20385232744783308, 0.20706260032102727, 0.20706260032102727, 0.2086677367576244, 0.2086677367576244, 0.21348314606741572, 0.21348314606741572, 0.21508828250401285, 0.21508828250401285, 0.21669341894060995, 0.21669341894060995, 0.21829855537720708, 0.21829855537720708, 0.21990369181380418, 0.21990369181380418, 0.22150882825040127, 0.22150882825040127, 0.2231139646869984, 0.2231139646869984, 0.2247191011235955, 0.2247191011235955, 0.22792937399678972, 0.22792937399678972, 0.23274478330658105, 0.23274478330658105, 0.23434991974317818, 0.23434991974317818, 0.23595505617977527, 0.23595505617977527, 0.2375601926163724, 0.2375601926163724, 0.2391653290529695, 0.2391653290529695, 0.24558587479935795, 0.24558587479935795, 0.2536115569823435, 0.2536115569823435, 0.25842696629213485, 0.25842696629213485, 0.26003210272873195, 0.26003210272873195, 0.26163723916532905, 0.26163723916532905, 0.2696629213483146, 0.2696629213483146, 0.27287319422150885, 0.27287319422150885, 0.27447833065810595, 0.27447833065810595, 0.27608346709470305, 0.27608346709470305, 0.27768860353130015, 0.27768860353130015, 0.27929373996789725, 0.27929373996789725, 0.2808988764044944, 0.2808988764044944, 0.2841091492776886, 0.2841091492776886, 0.28892455858747995, 0.28892455858747995, 0.29052969502407705, 0.29052969502407705, 0.29213483146067415, 0.29213483146067415, 0.2969502407704655, 0.2969502407704655, 0.30497592295345105, 0.30497592295345105, 0.30818619582664525, 0.30818619582664525, 0.3097913322632424, 0.3097913322632424, 0.3146067415730337, 0.3146067415730337, 0.3162118780096308, 0.3162118780096308, 0.32102728731942215, 0.32102728731942215, 0.32263242375601925, 0.32263242375601925, 0.3274478330658106, 0.3274478330658106, 0.3290529695024077, 0.3290529695024077, 0.33386837881219905, 0.33386837881219905, 0.33707865168539325, 0.33707865168539325, 0.3402889245585875, 0.3402889245585875, 0.3467094703049759, 0.3467094703049759, 0.34991974317817015, 0.34991974317817015, 0.35313001605136435, 0.35313001605136435, 0.3563402889245586, 0.3563402889245586, 0.3611556982343499, 0.3611556982343499, 0.36436597110754415, 0.36436597110754415, 0.36597110754414125, 0.36597110754414125, 0.3739967897271268, 0.3739967897271268, 0.37881219903691815, 0.37881219903691815, 0.38041733547351525, 0.38041733547351525, 0.38202247191011235, 0.38202247191011235, 0.38362760834670945, 0.38362760834670945, 0.3852327447833066, 0.3852327447833066, 0.3868378812199037, 0.3868378812199037, 0.3884430176565008, 0.3884430176565008, 0.39486356340288925, 0.39486356340288925, 0.39646869983948635, 0.39646869983948635, 0.3996789727126806, 0.3996789727126806, 0.4012841091492777, 0.4012841091492777, 0.4028892455858748, 0.4028892455858748, 0.406099518459069, 0.406099518459069, 0.40930979133226325, 0.40930979133226325, 0.41091492776886035, 0.41091492776886035, 0.41252006420545745, 0.41252006420545745, 0.4157303370786517, 0.4157303370786517, 0.420545746388443, 0.420545746388443, 0.42375601926163725, 0.42375601926163725, 0.42696629213483145, 0.42696629213483145, 0.4333868378812199, 0.4333868378812199, 0.434991974317817, 0.434991974317817, 0.43820224719101125, 0.43820224719101125, 0.4446227929373997, 0.4446227929373997, 0.45264847512038525, 0.45264847512038525, 0.45425361155698235, 0.45425361155698235, 0.45746388443017655, 0.45746388443017655, 0.4590690208667737, 0.4590690208667737, 0.4606741573033708, 0.4606741573033708, 0.4654895666131621, 0.4654895666131621, 0.47191011235955055, 0.47191011235955055, 0.47351524879614765, 0.47351524879614765, 0.4751203852327448, 0.4751203852327448, 0.4767255216693419, 0.4767255216693419, 0.48314606741573035, 0.48314606741573035, 0.4911717495987159, 0.4911717495987159, 0.492776886035313, 0.492776886035313, 0.49919743178170145, 0.49919743178170145, 0.5008025682182986, 0.5008025682182986, 0.5024077046548957, 0.5024077046548957, 0.5040128410914928, 0.5040128410914928, 0.507223113964687, 0.507223113964687, 0.5088282504012841, 0.5088282504012841, 0.5104333868378812, 0.5104333868378812, 0.5120385232744783, 0.5120385232744783, 0.5152487961476726, 0.5152487961476726, 0.5168539325842697, 0.5168539325842697, 0.5200642054574639, 0.5200642054574639, 0.521669341894061, 0.521669341894061, 0.5248796147672552, 0.5248796147672552, 0.5280898876404494, 0.5280898876404494, 0.5296950240770465, 0.5296950240770465, 0.5329052969502408, 0.5329052969502408, 0.5345104333868379, 0.5345104333868379, 0.536115569823435, 0.536115569823435, 0.5377207062600321, 0.5377207062600321, 0.5393258426966292, 0.5393258426966292, 0.5425361155698234, 0.5425361155698234, 0.5537720706260032, 0.5537720706260032, 0.5553772070626003, 0.5553772070626003, 0.5569823434991974, 0.5569823434991974, 0.5585874799357945, 0.5585874799357945, 0.5601926163723917, 0.5601926163723917, 0.565008025682183, 0.565008025682183, 0.5746388443017657, 0.5746388443017657, 0.5842696629213483, 0.5842696629213483, 0.5874799357945425, 0.5874799357945425, 0.5906902086677368, 0.5906902086677368, 0.5922953451043339, 0.5922953451043339, 0.5955056179775281, 0.5955056179775281, 0.5987158908507223, 0.5987158908507223, 0.6019261637239165, 0.6019261637239165, 0.6051364365971108, 0.6051364365971108, 0.6067415730337079, 0.6067415730337079, 0.608346709470305, 0.608346709470305, 0.6099518459069021, 0.6099518459069021, 0.6131621187800963, 0.6131621187800963, 0.6147672552166934, 0.6147672552166934, 0.6179775280898876, 0.6179775280898876, 0.6243980738362761, 0.6243980738362761, 0.6260032102728732, 0.6260032102728732, 0.6292134831460674, 0.6292134831460674, 0.6308186195826645, 0.6308186195826645, 0.6356340288924559, 0.6356340288924559, 0.637239165329053, 0.637239165329053, 0.6388443017656501, 0.6388443017656501, 0.6420545746388443, 0.6420545746388443, 0.6436597110754414, 0.6436597110754414, 0.6452648475120385, 0.6452648475120385, 0.6468699839486356, 0.6468699839486356, 0.6484751203852327, 0.6484751203852327, 0.6500802568218299, 0.6500802568218299, 0.6597110754414125, 0.6597110754414125, 0.6613162118780096, 0.6613162118780096, 0.6677367576243981, 0.6677367576243981, 0.6709470304975923, 0.6709470304975923, 0.6741573033707865, 0.6741573033707865, 0.6757624398073836, 0.6757624398073836, 0.6789727126805778, 0.6789727126805778, 0.6821829855537721, 0.6821829855537721, 0.6837881219903692, 0.6837881219903692, 0.6886035313001605, 0.6886035313001605, 0.6934189406099518, 0.6934189406099518, 0.695024077046549, 0.695024077046549, 0.6966292134831461, 0.6966292134831461, 0.7014446227929374, 0.7014446227929374, 0.7030497592295345, 0.7030497592295345, 0.7046548956661316, 0.7046548956661316, 0.7078651685393258, 0.7078651685393258, 0.7110754414125201, 0.7110754414125201, 0.7174959871589085, 0.7174959871589085, 0.7191011235955056, 0.7191011235955056, 0.7303370786516854, 0.7303370786516854, 0.7319422150882825, 0.7319422150882825, 0.7335473515248796, 0.7335473515248796, 0.7367576243980738, 0.7367576243980738, 0.7383627608346709, 0.7383627608346709, 0.7415730337078652, 0.7415730337078652, 0.7447833065810594, 0.7447833065810594, 0.7463884430176565, 0.7463884430176565, 0.7479935794542536, 0.7479935794542536, 0.7512038523274478, 0.7512038523274478, 0.7560192616372392, 0.7560192616372392, 0.7576243980738363, 0.7576243980738363, 0.7592295345104334, 0.7592295345104334, 0.7672552166934189, 0.7672552166934189, 0.7688603531300161, 0.7688603531300161, 0.7704654895666132, 0.7704654895666132, 0.7752808988764045, 0.7752808988764045, 0.7784911717495987, 0.7784911717495987, 0.78330658105939, 0.78330658105939, 0.7849117174959872, 0.7849117174959872, 0.7913322632423756, 0.7913322632423756, 0.7961476725521669, 0.7961476725521669, 0.8009630818619583, 0.8009630818619583, 0.8025682182985554, 0.8025682182985554, 0.8105939004815409, 0.8105939004815409, 0.812199036918138, 0.812199036918138, 0.8154093097913323, 0.8154093097913323, 0.8186195826645265, 0.8186195826645265, 0.8202247191011236, 0.8202247191011236, 0.8218298555377207, 0.8218298555377207, 0.8250401284109149, 0.8250401284109149, 0.826645264847512, 0.826645264847512, 0.8394863563402889, 0.8394863563402889, 0.8443017656500803, 0.8443017656500803, 0.8459069020866774, 0.8459069020866774, 0.8491171749598716, 0.8491171749598716, 0.8507223113964687, 0.8507223113964687, 0.8539325842696629, 0.8539325842696629, 0.8587479935794543, 0.8587479935794543, 0.8603531300160514, 0.8603531300160514, 0.8619582664526485, 0.8619582664526485, 0.8667736757624398, 0.8667736757624398, 0.8683788121990369, 0.8683788121990369, 0.869983948635634, 0.869983948635634, 0.8715890850722311, 0.8715890850722311, 0.8764044943820225, 0.8764044943820225, 0.8796147672552167, 0.8796147672552167, 0.8812199036918138, 0.8812199036918138, 0.8956661316211878, 0.8956661316211878, 0.9020866773675762, 0.9020866773675762, 0.9069020866773676, 0.9069020866773676, 0.9085072231139647, 0.9085072231139647, 0.9117174959871589, 0.9117174959871589, 0.9245585874799358, 0.9245585874799358, 0.9309791332263242, 0.9309791332263242, 0.942215088282504, 0.942215088282504, 0.9438202247191011, 0.9438202247191011, 0.9502407704654896, 0.9502407704654896, 0.9518459069020867, 0.9518459069020867, 0.9534510433386838, 0.9534510433386838, 0.956661316211878, 0.956661316211878, 0.9646869983948636, 0.9646869983948636, 1.0], \"y\": [0.0, 0.10278745644599303, 0.10278745644599303, 0.11672473867595819, 0.11672473867595819, 0.11846689895470383, 0.11846689895470383, 0.12020905923344948, 0.12020905923344948, 0.12195121951219512, 0.12195121951219512, 0.1289198606271777, 0.1289198606271777, 0.13066202090592335, 0.13066202090592335, 0.13588850174216027, 0.13588850174216027, 0.13763066202090593, 0.13763066202090593, 0.13937282229965156, 0.13937282229965156, 0.14111498257839722, 0.14111498257839722, 0.14634146341463414, 0.14634146341463414, 0.15331010452961671, 0.15331010452961671, 0.15853658536585366, 0.15853658536585366, 0.16202090592334495, 0.16202090592334495, 0.17247386759581881, 0.17247386759581881, 0.17770034843205576, 0.17770034843205576, 0.18292682926829268, 0.18292682926829268, 0.18466898954703834, 0.18466898954703834, 0.18641114982578397, 0.18641114982578397, 0.18815331010452963, 0.18815331010452963, 0.1916376306620209, 0.1916376306620209, 0.19686411149825783, 0.19686411149825783, 0.20209059233449478, 0.20209059233449478, 0.20557491289198607, 0.20557491289198607, 0.20905923344947736, 0.20905923344947736, 0.21254355400696864, 0.21254355400696864, 0.21602787456445993, 0.21602787456445993, 0.21951219512195122, 0.21951219512195122, 0.22125435540069685, 0.22125435540069685, 0.2229965156794425, 0.2229965156794425, 0.22996515679442509, 0.22996515679442509, 0.23519163763066203, 0.23519163763066203, 0.23693379790940766, 0.23693379790940766, 0.23867595818815332, 0.23867595818815332, 0.2421602787456446, 0.2421602787456446, 0.24390243902439024, 0.24390243902439024, 0.2456445993031359, 0.2456445993031359, 0.24912891986062718, 0.24912891986062718, 0.25261324041811845, 0.25261324041811845, 0.26306620209059234, 0.26306620209059234, 0.26480836236933797, 0.26480836236933797, 0.2665505226480836, 0.2665505226480836, 0.2682926829268293, 0.2682926829268293, 0.2700348432055749, 0.2700348432055749, 0.27526132404181186, 0.27526132404181186, 0.2770034843205575, 0.2770034843205575, 0.2787456445993031, 0.2787456445993031, 0.2804878048780488, 0.2804878048780488, 0.28222996515679444, 0.28222996515679444, 0.2857142857142857, 0.2857142857142857, 0.2874564459930314, 0.2874564459930314, 0.289198606271777, 0.289198606271777, 0.2926829268292683, 0.2926829268292683, 0.2979094076655052, 0.2979094076655052, 0.30139372822299654, 0.30139372822299654, 0.30313588850174217, 0.30313588850174217, 0.3048780487804878, 0.3048780487804878, 0.3083623693379791, 0.3083623693379791, 0.31010452961672474, 0.31010452961672474, 0.3153310104529617, 0.3153310104529617, 0.3205574912891986, 0.3205574912891986, 0.32752613240418116, 0.32752613240418116, 0.32926829268292684, 0.32926829268292684, 0.3344947735191638, 0.3344947735191638, 0.3362369337979094, 0.3362369337979094, 0.3397212543554007, 0.3397212543554007, 0.343205574912892, 0.343205574912892, 0.34668989547038326, 0.34668989547038326, 0.34843205574912894, 0.34843205574912894, 0.3501742160278746, 0.3501742160278746, 0.35714285714285715, 0.35714285714285715, 0.3588850174216028, 0.3588850174216028, 0.36585365853658536, 0.36585365853658536, 0.367595818815331, 0.367595818815331, 0.3710801393728223, 0.3710801393728223, 0.37282229965156793, 0.37282229965156793, 0.38153310104529614, 0.38153310104529614, 0.3832752613240418, 0.3832752613240418, 0.38501742160278746, 0.38501742160278746, 0.3867595818815331, 0.3867595818815331, 0.39372822299651566, 0.39372822299651566, 0.3989547038327526, 0.3989547038327526, 0.4024390243902439, 0.4024390243902439, 0.40418118466898956, 0.40418118466898956, 0.4076655052264808, 0.4076655052264808, 0.41114982578397213, 0.41114982578397213, 0.4146341463414634, 0.4146341463414634, 0.4163763066202091, 0.4163763066202091, 0.41986062717770034, 0.41986062717770034, 0.4337979094076655, 0.4337979094076655, 0.4355400696864111, 0.4355400696864111, 0.43902439024390244, 0.43902439024390244, 0.44076655052264807, 0.44076655052264807, 0.4425087108013937, 0.4425087108013937, 0.4442508710801394, 0.4442508710801394, 0.445993031358885, 0.445993031358885, 0.44773519163763065, 0.44773519163763065, 0.45121951219512196, 0.45121951219512196, 0.4547038327526132, 0.4547038327526132, 0.45993031358885017, 0.45993031358885017, 0.4651567944250871, 0.4651567944250871, 0.4721254355400697, 0.4721254355400697, 0.4738675958188153, 0.4738675958188153, 0.47909407665505227, 0.47909407665505227, 0.48257839721254353, 0.48257839721254353, 0.4843205574912892, 0.4843205574912892, 0.48606271777003485, 0.48606271777003485, 0.4878048780487805, 0.4878048780487805, 0.4930313588850174, 0.4930313588850174, 0.4965156794425087, 0.4965156794425087, 0.5017421602787456, 0.5017421602787456, 0.5052264808362369, 0.5052264808362369, 0.5069686411149826, 0.5069686411149826, 0.5104529616724739, 0.5104529616724739, 0.5139372822299652, 0.5139372822299652, 0.5156794425087108, 0.5156794425087108, 0.519163763066202, 0.519163763066202, 0.5209059233449478, 0.5209059233449478, 0.524390243902439, 0.524390243902439, 0.5296167247386759, 0.5296167247386759, 0.5313588850174216, 0.5313588850174216, 0.5365853658536586, 0.5365853658536586, 0.5418118466898955, 0.5418118466898955, 0.5452961672473867, 0.5452961672473867, 0.5470383275261324, 0.5470383275261324, 0.5487804878048781, 0.5487804878048781, 0.554006968641115, 0.554006968641115, 0.5557491289198606, 0.5557491289198606, 0.5592334494773519, 0.5592334494773519, 0.5679442508710801, 0.5679442508710801, 0.5696864111498258, 0.5696864111498258, 0.5818815331010453, 0.5818815331010453, 0.5905923344947736, 0.5905923344947736, 0.5958188153310104, 0.5958188153310104, 0.6010452961672473, 0.6010452961672473, 0.6027874564459931, 0.6027874564459931, 0.6045296167247387, 0.6045296167247387, 0.60801393728223, 0.60801393728223, 0.6097560975609756, 0.6097560975609756, 0.6114982578397212, 0.6114982578397212, 0.6149825783972126, 0.6149825783972126, 0.6167247386759582, 0.6167247386759582, 0.6184668989547039, 0.6184668989547039, 0.6219512195121951, 0.6219512195121951, 0.6236933797909407, 0.6236933797909407, 0.6254355400696864, 0.6254355400696864, 0.627177700348432, 0.627177700348432, 0.6289198606271778, 0.6289198606271778, 0.632404181184669, 0.632404181184669, 0.6341463414634146, 0.6341463414634146, 0.6411149825783972, 0.6411149825783972, 0.6428571428571429, 0.6428571428571429, 0.6445993031358885, 0.6445993031358885, 0.6463414634146342, 0.6463414634146342, 0.6498257839721254, 0.6498257839721254, 0.6550522648083623, 0.6550522648083623, 0.6567944250871081, 0.6567944250871081, 0.6655052264808362, 0.6655052264808362, 0.6689895470383276, 0.6689895470383276, 0.6724738675958188, 0.6724738675958188, 0.6759581881533101, 0.6759581881533101, 0.6777003484320557, 0.6777003484320557, 0.681184668989547, 0.681184668989547, 0.6846689895470384, 0.6846689895470384, 0.686411149825784, 0.686411149825784, 0.6881533101045296, 0.6881533101045296, 0.6933797909407665, 0.6933797909407665, 0.6951219512195121, 0.6951219512195121, 0.6986062717770035, 0.6986062717770035, 0.7003484320557491, 0.7003484320557491, 0.7020905923344948, 0.7020905923344948, 0.7038327526132404, 0.7038327526132404, 0.7090592334494773, 0.7090592334494773, 0.710801393728223, 0.710801393728223, 0.7125435540069687, 0.7125435540069687, 0.7142857142857143, 0.7142857142857143, 0.7160278745644599, 0.7160278745644599, 0.7195121951219512, 0.7195121951219512, 0.7247386759581882, 0.7247386759581882, 0.7264808362369338, 0.7264808362369338, 0.7299651567944251, 0.7299651567944251, 0.7317073170731707, 0.7317073170731707, 0.7334494773519163, 0.7334494773519163, 0.7369337979094077, 0.7369337979094077, 0.7421602787456446, 0.7421602787456446, 0.7508710801393729, 0.7508710801393729, 0.7526132404181185, 0.7526132404181185, 0.7543554006968641, 0.7543554006968641, 0.7578397212543554, 0.7578397212543554, 0.7613240418118467, 0.7613240418118467, 0.764808362369338, 0.764808362369338, 0.7700348432055749, 0.7700348432055749, 0.7717770034843205, 0.7717770034843205, 0.7735191637630662, 0.7735191637630662, 0.7752613240418118, 0.7752613240418118, 0.7787456445993032, 0.7787456445993032, 0.7822299651567944, 0.7822299651567944, 0.7839721254355401, 0.7839721254355401, 0.7926829268292683, 0.7926829268292683, 0.794425087108014, 0.794425087108014, 0.7979094076655052, 0.7979094076655052, 0.8013937282229965, 0.8013937282229965, 0.8048780487804879, 0.8048780487804879, 0.8066202090592335, 0.8066202090592335, 0.8205574912891986, 0.8205574912891986, 0.8222996515679443, 0.8222996515679443, 0.8275261324041812, 0.8275261324041812, 0.8362369337979094, 0.8362369337979094, 0.8397212543554007, 0.8397212543554007, 0.8432055749128919, 0.8432055749128919, 0.8449477351916377, 0.8449477351916377, 0.8466898954703833, 0.8466898954703833, 0.8484320557491289, 0.8484320557491289, 0.8519163763066202, 0.8519163763066202, 0.8536585365853658, 0.8536585365853658, 0.8554006968641115, 0.8554006968641115, 0.8571428571428571, 0.8571428571428571, 0.8623693379790941, 0.8623693379790941, 0.867595818815331, 0.867595818815331, 0.8693379790940766, 0.8693379790940766, 0.8710801393728222, 0.8710801393728222, 0.8745644599303136, 0.8745644599303136, 0.8763066202090593, 0.8763066202090593, 0.8780487804878049, 0.8780487804878049, 0.8797909407665505, 0.8797909407665505, 0.8815331010452961, 0.8815331010452961, 0.8832752613240418, 0.8832752613240418, 0.8850174216027874, 0.8850174216027874, 0.8885017421602788, 0.8885017421602788, 0.8902439024390244, 0.8902439024390244, 0.89198606271777, 0.89198606271777, 0.8937282229965157, 0.8937282229965157, 0.8954703832752613, 0.8954703832752613, 0.8972125435540069, 0.8972125435540069, 0.9024390243902439, 0.9024390243902439, 0.9041811846689896, 0.9041811846689896, 0.9094076655052264, 0.9094076655052264, 0.9146341463414634, 0.9146341463414634, 0.9163763066202091, 0.9163763066202091, 0.9198606271777003, 0.9198606271777003, 0.921602787456446, 0.921602787456446, 0.9250871080139372, 0.9250871080139372, 0.926829268292683, 0.926829268292683, 0.9285714285714286, 0.9285714285714286, 0.9303135888501742, 0.9303135888501742, 0.9355400696864111, 0.9355400696864111, 0.9372822299651568, 0.9372822299651568, 0.9425087108013938, 0.9425087108013938, 0.9442508710801394, 0.9442508710801394, 0.9477351916376306, 0.9477351916376306, 0.9529616724738676, 0.9529616724738676, 0.9547038327526133, 0.9547038327526133, 0.9564459930313589, 0.9564459930313589, 0.9599303135888502, 0.9599303135888502, 0.9616724738675958, 0.9616724738675958, 0.9651567944250871, 0.9651567944250871, 0.9668989547038328, 0.9668989547038328, 0.9703832752613241, 0.9703832752613241, 0.9721254355400697, 0.9721254355400697, 0.9738675958188153, 0.9738675958188153, 0.980836236933798, 0.980836236933798, 0.9825783972125436, 0.9825783972125436, 0.9843205574912892, 0.9843205574912892, 0.9895470383275261, 0.9895470383275261, 0.9947735191637631, 0.9947735191637631, 0.9982578397212544, 0.9982578397212544, 1.0, 1.0], \"type\": \"scatter\", \"uid\": \"3d187698-3396-11e9-8c97-67fc199b385a\"}],\n",
       "            {\"title\": \"Hourly ROC Comparisons\", \"xaxis\": {\"title\": \"False Positive Rate\"}, \"yaxis\": {\"title\": \"Sensitivity\"}},\n",
       "            {\"showLink\": false, \"linkText\": \"Export to plot.ly\"}\n",
       "        ).then(function () {return Plotly.addFrames('e1fa3374-ed34-416d-b913-b1b4427366a0',{});}).then(function(){Plotly.animate('e1fa3374-ed34-416d-b913-b1b4427366a0');})\n",
       "        });</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn.metrics as skm\n",
    "\n",
    "lw = 2\n",
    "trace1 = go.Scatter(x=fprs[23], y=tprs[23], \n",
    "                    mode='lines', \n",
    "                    line=dict(color='darkorange', width=lw),\n",
    "                    name='H23 AUC(area = %0.2f)' % aucs[23])\n",
    "trace2 = go.Scatter(x=[0, 1], y=[0, 1], \n",
    "                    mode='lines', \n",
    "                    line=dict(color='navy', width=lw, dash='dash'),\n",
    "                    showlegend=False)\n",
    "trace3 = go.Scatter(x=fprs[21], y=tprs[21], \n",
    "                    mode='lines', \n",
    "                    line=dict(color='cyan', width=lw),\n",
    "                    name='H21 AUC(area = %0.2f)' % aucs[21])\n",
    "trace4 = go.Scatter(x=fprs[18], y=tprs[18], \n",
    "                    mode='lines', \n",
    "                    line=dict(color='green', width=lw),\n",
    "                    name='H18 AUC(area = %0.2f)' % aucs[18])\n",
    "trace5 = go.Scatter(x=fprs[8], y=tprs[8], \n",
    "                    mode='lines', \n",
    "                    line=dict(color='indigo', width=lw),\n",
    "                    name='H8 AUC(area = %0.2f)' % aucs[8])\n",
    "\n",
    "layout = go.Layout(title='Hourly ROC Comparisons',\n",
    "                   xaxis=dict(title='False Positive Rate'),\n",
    "                   yaxis=dict(title='Sensitivity'))\n",
    "fig = go.Figure(data=[trace1, trace2, trace3, trace4, trace5], layout=layout)\n",
    "offline.iplot(fig, show_link=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f77f54b0f28>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x324 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(figsize=(4,4.5))\n",
    "ax.plot(fpr, tpr, lw=2, label=\"Model\")\n",
    "ax.set_xlim(-0.008, 1.0)\n",
    "ax.set_ylim(0.1, 1.0)\n",
    "ax.set_title(\"Hour-23 ROC Curve\")\n",
    "ax.set_xlabel('1 - Specificity')\n",
    "ax.set_ylabel('Sensitivity')\n",
    "ax.legend(loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the training files into hdf5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "for k in range(0, 24):\n",
    "    print(k)\n",
    "    X_H = list()\n",
    "    Y_H = list()\n",
    "### Baseline\n",
    "    for i in range(0, 13):\n",
    "        dd = \"/mnt/data0/tao/ECG/\" + ECG_meta.Exp[i] + \"/Baseline/\" + ECG_meta.Species[i]\n",
    "        fname = '_H' + str(k) + '_'\n",
    "        file_hour = [x for x in os.listdir(dd) if fname in x]\n",
    "        for item in file_hour:\n",
    "            ff = dd + \"/\" + item\n",
    "            data = pd.read_csv(ff)\n",
    "            X_H.append(list(data.VALUE.values))\n",
    "            Y_H.append(0)\n",
    "### Peak\n",
    "    for i in range(0, 13):\n",
    "        dd = \"/mnt/data0/tao/ECG/\" + ECG_meta.Exp[i] + \"/Peak/\" + ECG_meta.Species[i]\n",
    "        fname = '_H' + str(k) + '_'\n",
    "        file_hour = [x for x in os.listdir(dd) if fname in x]\n",
    "        for item in file_hour:\n",
    "            ff = dd + \"/\" + item\n",
    "            data = pd.read_csv(ff)\n",
    "            X_H.append(list(data.VALUE.values))\n",
    "            Y_H.append(1)    \n",
    "    X_H = np.asarray(X_H)\n",
    "    Y_H = np.asarray(Y_H)\n",
    "    fx_n = '/mnt/data0/tao/ECG/hdf5/ECG_H' + str(k) + '_Dataset.h5'\n",
    "    fy_n = '/mnt/data0/tao/ECG/hdf5/ECG_H' + str(k) + '_Label.h5'\n",
    "    x_n = 'ECG_H' + str(k)\n",
    "    with h5py.File(fx_n, 'w') as hf:\n",
    "        hf.create_dataset(\"x_n\", data=X_H)\n",
    "    with h5py.File(fy_n, 'w') as hf:\n",
    "        hf.create_dataset(\"x_n\", data=Y_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
